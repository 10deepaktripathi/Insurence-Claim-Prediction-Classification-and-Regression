{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.model_selection import cross_val_score,cross_val_predict\n",
    "from sklearn.svm import SVR\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler,MinMaxScaler,OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import math\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#mean_squared_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r\"C:\\Users\\Deepak Tripathi\\Desktop\\assignment machine learning\\part3\\CE802_P3_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>F9</th>\n",
       "      <th>F10</th>\n",
       "      <th>F11</th>\n",
       "      <th>F12</th>\n",
       "      <th>F13</th>\n",
       "      <th>F14</th>\n",
       "      <th>F15</th>\n",
       "      <th>F16</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61.22</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>UK</td>\n",
       "      <td>-14.91</td>\n",
       "      <td>1030.95</td>\n",
       "      <td>614.70</td>\n",
       "      <td>-8.40</td>\n",
       "      <td>11.96</td>\n",
       "      <td>275.99</td>\n",
       "      <td>-333.60</td>\n",
       "      <td>1.86</td>\n",
       "      <td>19.48</td>\n",
       "      <td>6</td>\n",
       "      <td>Very high</td>\n",
       "      <td>7841.50</td>\n",
       "      <td>1605.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86.58</td>\n",
       "      <td>12</td>\n",
       "      <td>20.07</td>\n",
       "      <td>Europe</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1059.87</td>\n",
       "      <td>1354.00</td>\n",
       "      <td>-9.97</td>\n",
       "      <td>3.80</td>\n",
       "      <td>347.10</td>\n",
       "      <td>-356.04</td>\n",
       "      <td>6.39</td>\n",
       "      <td>22.15</td>\n",
       "      <td>2</td>\n",
       "      <td>Very low</td>\n",
       "      <td>25589.98</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75.95</td>\n",
       "      <td>6</td>\n",
       "      <td>45.00</td>\n",
       "      <td>USA</td>\n",
       "      <td>-1.29</td>\n",
       "      <td>1320.03</td>\n",
       "      <td>1477.96</td>\n",
       "      <td>-10.02</td>\n",
       "      <td>20.32</td>\n",
       "      <td>345.69</td>\n",
       "      <td>-353.58</td>\n",
       "      <td>13.14</td>\n",
       "      <td>21.87</td>\n",
       "      <td>8</td>\n",
       "      <td>Low</td>\n",
       "      <td>16849.14</td>\n",
       "      <td>3241.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156.57</td>\n",
       "      <td>12</td>\n",
       "      <td>12.93</td>\n",
       "      <td>USA</td>\n",
       "      <td>-4.83</td>\n",
       "      <td>1696.92</td>\n",
       "      <td>750.14</td>\n",
       "      <td>-10.98</td>\n",
       "      <td>4.78</td>\n",
       "      <td>253.37</td>\n",
       "      <td>-254.37</td>\n",
       "      <td>15.48</td>\n",
       "      <td>28.11</td>\n",
       "      <td>10</td>\n",
       "      <td>Very low</td>\n",
       "      <td>10791.06</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101.27</td>\n",
       "      <td>3</td>\n",
       "      <td>0.51</td>\n",
       "      <td>Rest</td>\n",
       "      <td>-26.28</td>\n",
       "      <td>1451.37</td>\n",
       "      <td>251.06</td>\n",
       "      <td>-9.71</td>\n",
       "      <td>2.06</td>\n",
       "      <td>242.35</td>\n",
       "      <td>-379.77</td>\n",
       "      <td>3.90</td>\n",
       "      <td>23.84</td>\n",
       "      <td>10</td>\n",
       "      <td>Very low</td>\n",
       "      <td>14760.66</td>\n",
       "      <td>336.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       F1  F2     F3      F4     F5       F6       F7     F8     F9     F10  \\\n",
       "0   61.22   6   0.00      UK -14.91  1030.95   614.70  -8.40  11.96  275.99   \n",
       "1   86.58  12  20.07  Europe   0.57  1059.87  1354.00  -9.97   3.80  347.10   \n",
       "2   75.95   6  45.00     USA  -1.29  1320.03  1477.96 -10.02  20.32  345.69   \n",
       "3  156.57  12  12.93     USA  -4.83  1696.92   750.14 -10.98   4.78  253.37   \n",
       "4  101.27   3   0.51    Rest -26.28  1451.37   251.06  -9.71   2.06  242.35   \n",
       "\n",
       "      F11    F12    F13  F14        F15       F16   Target  \n",
       "0 -333.60   1.86  19.48    6  Very high   7841.50  1605.31  \n",
       "1 -356.04   6.39  22.15    2   Very low  25589.98     0.00  \n",
       "2 -353.58  13.14  21.87    8        Low  16849.14  3241.77  \n",
       "3 -254.37  15.48  28.11   10   Very low  10791.06     0.00  \n",
       "4 -379.77   3.90  23.84   10   Very low  14760.66   336.25  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 17)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'High', 'Low', 'Medium', 'Very high', 'Very low'}"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df['F15'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>F9</th>\n",
       "      <th>F10</th>\n",
       "      <th>F11</th>\n",
       "      <th>F12</th>\n",
       "      <th>F13</th>\n",
       "      <th>F14</th>\n",
       "      <th>F15</th>\n",
       "      <th>F16</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61.22</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>UK</td>\n",
       "      <td>-14.91</td>\n",
       "      <td>1030.95</td>\n",
       "      <td>614.70</td>\n",
       "      <td>-8.40</td>\n",
       "      <td>11.96</td>\n",
       "      <td>275.99</td>\n",
       "      <td>-333.60</td>\n",
       "      <td>1.86</td>\n",
       "      <td>19.48</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>7841.50</td>\n",
       "      <td>1605.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86.58</td>\n",
       "      <td>12</td>\n",
       "      <td>20.07</td>\n",
       "      <td>Europe</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1059.87</td>\n",
       "      <td>1354.00</td>\n",
       "      <td>-9.97</td>\n",
       "      <td>3.80</td>\n",
       "      <td>347.10</td>\n",
       "      <td>-356.04</td>\n",
       "      <td>6.39</td>\n",
       "      <td>22.15</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>25589.98</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75.95</td>\n",
       "      <td>6</td>\n",
       "      <td>45.00</td>\n",
       "      <td>USA</td>\n",
       "      <td>-1.29</td>\n",
       "      <td>1320.03</td>\n",
       "      <td>1477.96</td>\n",
       "      <td>-10.02</td>\n",
       "      <td>20.32</td>\n",
       "      <td>345.69</td>\n",
       "      <td>-353.58</td>\n",
       "      <td>13.14</td>\n",
       "      <td>21.87</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>16849.14</td>\n",
       "      <td>3241.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156.57</td>\n",
       "      <td>12</td>\n",
       "      <td>12.93</td>\n",
       "      <td>USA</td>\n",
       "      <td>-4.83</td>\n",
       "      <td>1696.92</td>\n",
       "      <td>750.14</td>\n",
       "      <td>-10.98</td>\n",
       "      <td>4.78</td>\n",
       "      <td>253.37</td>\n",
       "      <td>-254.37</td>\n",
       "      <td>15.48</td>\n",
       "      <td>28.11</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10791.06</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101.27</td>\n",
       "      <td>3</td>\n",
       "      <td>0.51</td>\n",
       "      <td>Rest</td>\n",
       "      <td>-26.28</td>\n",
       "      <td>1451.37</td>\n",
       "      <td>251.06</td>\n",
       "      <td>-9.71</td>\n",
       "      <td>2.06</td>\n",
       "      <td>242.35</td>\n",
       "      <td>-379.77</td>\n",
       "      <td>3.90</td>\n",
       "      <td>23.84</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>14760.66</td>\n",
       "      <td>336.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       F1  F2     F3      F4     F5       F6       F7     F8     F9     F10  \\\n",
       "0   61.22   6   0.00      UK -14.91  1030.95   614.70  -8.40  11.96  275.99   \n",
       "1   86.58  12  20.07  Europe   0.57  1059.87  1354.00  -9.97   3.80  347.10   \n",
       "2   75.95   6  45.00     USA  -1.29  1320.03  1477.96 -10.02  20.32  345.69   \n",
       "3  156.57  12  12.93     USA  -4.83  1696.92   750.14 -10.98   4.78  253.37   \n",
       "4  101.27   3   0.51    Rest -26.28  1451.37   251.06  -9.71   2.06  242.35   \n",
       "\n",
       "      F11    F12    F13  F14  F15       F16   Target  \n",
       "0 -333.60   1.86  19.48    6    4   7841.50  1605.31  \n",
       "1 -356.04   6.39  22.15    2    0  25589.98     0.00  \n",
       "2 -353.58  13.14  21.87    8    1  16849.14  3241.77  \n",
       "3 -254.37  15.48  28.11   10    0  10791.06     0.00  \n",
       "4 -379.77   3.90  23.84   10    0  14760.66   336.25  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['F15']=df['F15'].map({'Very low':0,'Low':1,'Medium':2,'High':3,'Very high':4})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First create a very basic modal without doing much preprocessing and see what result are we getting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1     False\n",
      "F2     False\n",
      "F3     False\n",
      "F4      True\n",
      "F5     False\n",
      "F6     False\n",
      "F7     False\n",
      "F8     False\n",
      "F9     False\n",
      "F10    False\n",
      "F11    False\n",
      "F12    False\n",
      "F13    False\n",
      "F14    False\n",
      "F15    False\n",
      "F16    False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "df1=df.drop(['Target'],axis=1)\n",
    "y=df['Target']\n",
    "\n",
    "\n",
    "cat=df1.dtypes==object\n",
    "print(cat)\n",
    "# print(~cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer(\n",
    "     [(\"ohe\", OneHotEncoder(handle_unknown='ignore'), cat),\n",
    "      (\"norm\", StandardScaler(), ~cat)])\n",
    "\n",
    "#MinMaxScaler\n",
    "#StandardScaler\n",
    "\n",
    "ct.fit(df1)\n",
    "x=ct.transform(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 19)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, X_test, y, y_test = train_test_split(x, y, test_size=0.20, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE= 513.8622752143402\n",
      "R2= 0.7759911413929883\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "      <th>Parameter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LinearRegression</th>\n",
       "      <td>513.862275</td>\n",
       "      <td>0.775991</td>\n",
       "      <td>Default</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        RMSE        R2 Parameter\n",
       "LinearRegression  513.862275  0.775991   Default"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo=pd.DataFrame(columns=['RMSE','R2','Parameter'])\n",
    "pred = cross_val_predict(LinearRegression(), x, y, cv=10)\n",
    "\n",
    "MSE=mean_squared_error(y,pred)\n",
    "RMSE=math.sqrt(MSE)\n",
    "r2=r2_score(y,pred)\n",
    "print('RMSE=',RMSE)\n",
    "print('R2=',r2)\n",
    "print('-'*50)\n",
    "\n",
    "model='LinearRegression'\n",
    "algo.loc[model]=[RMSE,r2,'Default']\n",
    "algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE= 512.7958567286271\n",
      "R2= 0.7769199478766612\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "      <th>Parameter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LinearRegression</th>\n",
       "      <td>513.862275</td>\n",
       "      <td>0.775991</td>\n",
       "      <td>Default</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>512.795857</td>\n",
       "      <td>0.776920</td>\n",
       "      <td>Default</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        RMSE        R2 Parameter\n",
       "LinearRegression  513.862275  0.775991   Default\n",
       "Ridge             512.795857  0.776920   Default"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = cross_val_predict(Ridge(), x, y, cv=10)\n",
    "MSE=mean_squared_error(y,pred)\n",
    "RMSE=math.sqrt(MSE)\n",
    "r2=r2_score(y,pred)\n",
    "print('RMSE=',RMSE)\n",
    "print('R2=',r2)\n",
    "print('-'*50)\n",
    "\n",
    "model='Ridge'\n",
    "algo.loc[model]=[RMSE,r2,'Default']\n",
    "algo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE= 511.877805834089\n",
      "R2= 0.7777179867630559\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "      <th>Parameter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LinearRegression</th>\n",
       "      <td>513.862275</td>\n",
       "      <td>0.775991</td>\n",
       "      <td>Default</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>512.795857</td>\n",
       "      <td>0.776920</td>\n",
       "      <td>Default</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>511.877806</td>\n",
       "      <td>0.777718</td>\n",
       "      <td>Default</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        RMSE        R2 Parameter\n",
       "LinearRegression  513.862275  0.775991   Default\n",
       "Ridge             512.795857  0.776920   Default\n",
       "Lasso             511.877806  0.777718   Default"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = cross_val_predict(Lasso(), x, y, cv=10)\n",
    "MSE=mean_squared_error(y,pred)\n",
    "RMSE=math.sqrt(MSE)\n",
    "r2=r2_score(y,pred)\n",
    "print('RMSE=',RMSE)\n",
    "print('R2=',r2)\n",
    "print('-'*50)\n",
    "\n",
    "model='Lasso'\n",
    "algo.loc[model]=[RMSE,r2,'Default']\n",
    "algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE= 1176.7691285694762\n",
      "R2= -0.17477380842216683\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "      <th>Parameter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LinearRegression</th>\n",
       "      <td>513.862275</td>\n",
       "      <td>0.775991</td>\n",
       "      <td>Default</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>512.795857</td>\n",
       "      <td>0.776920</td>\n",
       "      <td>Default</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>511.877806</td>\n",
       "      <td>0.777718</td>\n",
       "      <td>Default</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVR</th>\n",
       "      <td>1176.769129</td>\n",
       "      <td>-0.174774</td>\n",
       "      <td>Default</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         RMSE        R2 Parameter\n",
       "LinearRegression   513.862275  0.775991   Default\n",
       "Ridge              512.795857  0.776920   Default\n",
       "Lasso              511.877806  0.777718   Default\n",
       "SVR               1176.769129 -0.174774   Default"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = cross_val_predict(SVR(), x, y, cv=10)\n",
    "MSE=mean_squared_error(y,pred)\n",
    "RMSE=math.sqrt(MSE)\n",
    "r2=r2_score(y,pred)\n",
    "print('RMSE=',RMSE)\n",
    "print('R2=',r2)\n",
    "print('-'*50)\n",
    "\n",
    "model='SVR'\n",
    "algo.loc[model]=[RMSE,r2,'Default']\n",
    "algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE= 483.203248813257\n",
      "R2= 0.8019242023798863\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "      <th>Parameter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LinearRegression</th>\n",
       "      <td>513.862275</td>\n",
       "      <td>0.775991</td>\n",
       "      <td>Default</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>512.795857</td>\n",
       "      <td>0.776920</td>\n",
       "      <td>Default</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>511.877806</td>\n",
       "      <td>0.777718</td>\n",
       "      <td>Default</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVR</th>\n",
       "      <td>1176.769129</td>\n",
       "      <td>-0.174774</td>\n",
       "      <td>Default</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostingRegressor</th>\n",
       "      <td>483.203249</td>\n",
       "      <td>0.801924</td>\n",
       "      <td>Default</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  RMSE        R2 Parameter\n",
       "LinearRegression            513.862275  0.775991   Default\n",
       "Ridge                       512.795857  0.776920   Default\n",
       "Lasso                       511.877806  0.777718   Default\n",
       "SVR                        1176.769129 -0.174774   Default\n",
       "GradientBoostingRegressor   483.203249  0.801924   Default"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = cross_val_predict(GradientBoostingRegressor(), x, y, cv=10)\n",
    "MSE=mean_squared_error(y,pred)\n",
    "RMSE=math.sqrt(MSE)\n",
    "r2=r2_score(y,pred)\n",
    "print('RMSE=',RMSE)\n",
    "print('R2=',r2)\n",
    "print('-'*50)\n",
    "\n",
    "model='GradientBoostingRegressor'\n",
    "algo.loc[model]=[RMSE,r2,'Default']\n",
    "algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE= 615.9795363893886\n",
      "R2= 0.6781123669041955\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "      <th>Parameter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LinearRegression</th>\n",
       "      <td>513.862275</td>\n",
       "      <td>0.775991</td>\n",
       "      <td>Default</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>512.795857</td>\n",
       "      <td>0.776920</td>\n",
       "      <td>Default</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>511.877806</td>\n",
       "      <td>0.777718</td>\n",
       "      <td>Default</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVR</th>\n",
       "      <td>1176.769129</td>\n",
       "      <td>-0.174774</td>\n",
       "      <td>Default</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostingRegressor</th>\n",
       "      <td>483.203249</td>\n",
       "      <td>0.801924</td>\n",
       "      <td>Default</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestRegressor</th>\n",
       "      <td>615.979536</td>\n",
       "      <td>0.678112</td>\n",
       "      <td>Default</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  RMSE        R2 Parameter\n",
       "LinearRegression            513.862275  0.775991   Default\n",
       "Ridge                       512.795857  0.776920   Default\n",
       "Lasso                       511.877806  0.777718   Default\n",
       "SVR                        1176.769129 -0.174774   Default\n",
       "GradientBoostingRegressor   483.203249  0.801924   Default\n",
       "RandomForestRegressor       615.979536  0.678112   Default"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = cross_val_predict(RandomForestRegressor(), x, y, cv=10)\n",
    "MSE=mean_squared_error(y,pred)\n",
    "RMSE=math.sqrt(MSE)\n",
    "r2=r2_score(y,pred)\n",
    "print('RMSE=',RMSE)\n",
    "print('R2=',r2)\n",
    "print('-'*50)\n",
    "\n",
    "model='RandomForestRegressor'\n",
    "algo.loc[model]=[RMSE,r2,'Default']\n",
    "algo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit model only on importent features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>F9</th>\n",
       "      <th>F10</th>\n",
       "      <th>F11</th>\n",
       "      <th>F12</th>\n",
       "      <th>F13</th>\n",
       "      <th>F14</th>\n",
       "      <th>F15</th>\n",
       "      <th>F16</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61.22</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>UK</td>\n",
       "      <td>-14.91</td>\n",
       "      <td>1030.95</td>\n",
       "      <td>614.70</td>\n",
       "      <td>-8.40</td>\n",
       "      <td>11.96</td>\n",
       "      <td>275.99</td>\n",
       "      <td>-333.60</td>\n",
       "      <td>1.86</td>\n",
       "      <td>19.48</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7841.50</td>\n",
       "      <td>1605.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86.58</td>\n",
       "      <td>12</td>\n",
       "      <td>20.07</td>\n",
       "      <td>Europe</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1059.87</td>\n",
       "      <td>1354.00</td>\n",
       "      <td>-9.97</td>\n",
       "      <td>3.80</td>\n",
       "      <td>347.10</td>\n",
       "      <td>-356.04</td>\n",
       "      <td>6.39</td>\n",
       "      <td>22.15</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25589.98</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75.95</td>\n",
       "      <td>6</td>\n",
       "      <td>45.00</td>\n",
       "      <td>USA</td>\n",
       "      <td>-1.29</td>\n",
       "      <td>1320.03</td>\n",
       "      <td>1477.96</td>\n",
       "      <td>-10.02</td>\n",
       "      <td>20.32</td>\n",
       "      <td>345.69</td>\n",
       "      <td>-353.58</td>\n",
       "      <td>13.14</td>\n",
       "      <td>21.87</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16849.14</td>\n",
       "      <td>3241.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156.57</td>\n",
       "      <td>12</td>\n",
       "      <td>12.93</td>\n",
       "      <td>USA</td>\n",
       "      <td>-4.83</td>\n",
       "      <td>1696.92</td>\n",
       "      <td>750.14</td>\n",
       "      <td>-10.98</td>\n",
       "      <td>4.78</td>\n",
       "      <td>253.37</td>\n",
       "      <td>-254.37</td>\n",
       "      <td>15.48</td>\n",
       "      <td>28.11</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10791.06</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101.27</td>\n",
       "      <td>3</td>\n",
       "      <td>0.51</td>\n",
       "      <td>Rest</td>\n",
       "      <td>-26.28</td>\n",
       "      <td>1451.37</td>\n",
       "      <td>251.06</td>\n",
       "      <td>-9.71</td>\n",
       "      <td>2.06</td>\n",
       "      <td>242.35</td>\n",
       "      <td>-379.77</td>\n",
       "      <td>3.90</td>\n",
       "      <td>23.84</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14760.66</td>\n",
       "      <td>336.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       F1  F2     F3      F4     F5       F6       F7     F8     F9     F10  \\\n",
       "0   61.22   6   0.00      UK -14.91  1030.95   614.70  -8.40  11.96  275.99   \n",
       "1   86.58  12  20.07  Europe   0.57  1059.87  1354.00  -9.97   3.80  347.10   \n",
       "2   75.95   6  45.00     USA  -1.29  1320.03  1477.96 -10.02  20.32  345.69   \n",
       "3  156.57  12  12.93     USA  -4.83  1696.92   750.14 -10.98   4.78  253.37   \n",
       "4  101.27   3   0.51    Rest -26.28  1451.37   251.06  -9.71   2.06  242.35   \n",
       "\n",
       "      F11    F12    F13  F14  F15       F16   Target  \n",
       "0 -333.60   1.86  19.48    6  4.0   7841.50  1605.31  \n",
       "1 -356.04   6.39  22.15    2  0.0  25589.98     0.00  \n",
       "2 -353.58  13.14  21.87    8  1.0  16849.14  3241.77  \n",
       "3 -254.37  15.48  28.11   10  0.0  10791.06     0.00  \n",
       "4 -379.77   3.90  23.84   10  0.0  14760.66   336.25  "
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(r\"C:\\Users\\Deepak Tripathi\\Desktop\\assignment machine learning\\part3\\CE802_P3_Data.csv\")\n",
    "\n",
    "df['F15']=df['F15'].map({'Very low':0.0,'Low':1.0,'Medium':2.0,'High':3.0,'Very high':4.0})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x279b5cf1f70>"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1224x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cor=df.corr()\n",
    "\n",
    "plt.figure(figsize=(17,9))\n",
    "sns.heatmap(cor,annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>F9</th>\n",
       "      <th>F10</th>\n",
       "      <th>F11</th>\n",
       "      <th>F12</th>\n",
       "      <th>F13</th>\n",
       "      <th>F14</th>\n",
       "      <th>F15</th>\n",
       "      <th>F16</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Target</th>\n",
       "      <td>0.015412</td>\n",
       "      <td>-0.2362</td>\n",
       "      <td>0.027713</td>\n",
       "      <td>0.020443</td>\n",
       "      <td>-0.263022</td>\n",
       "      <td>-0.30089</td>\n",
       "      <td>-0.01133</td>\n",
       "      <td>0.357137</td>\n",
       "      <td>-0.031407</td>\n",
       "      <td>-0.43099</td>\n",
       "      <td>0.346373</td>\n",
       "      <td>0.018677</td>\n",
       "      <td>0.193704</td>\n",
       "      <td>0.237441</td>\n",
       "      <td>0.029194</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              F1      F2        F3        F5        F6       F7       F8  \\\n",
       "Target  0.015412 -0.2362  0.027713  0.020443 -0.263022 -0.30089 -0.01133   \n",
       "\n",
       "              F9       F10      F11       F12       F13       F14       F15  \\\n",
       "Target  0.357137 -0.031407 -0.43099  0.346373  0.018677  0.193704  0.237441   \n",
       "\n",
       "             F16  Target  \n",
       "Target  0.029194     1.0  "
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cor[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x279b7b23b20>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(x='F4',y='Target',data = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x279b821fb80>"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(x='F15',y='Target',data = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F2</th>\n",
       "      <th>F4</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F9</th>\n",
       "      <th>F11</th>\n",
       "      <th>F12</th>\n",
       "      <th>F14</th>\n",
       "      <th>F15</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>UK</td>\n",
       "      <td>1030.95</td>\n",
       "      <td>614.70</td>\n",
       "      <td>11.96</td>\n",
       "      <td>-333.60</td>\n",
       "      <td>1.86</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1605.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>Europe</td>\n",
       "      <td>1059.87</td>\n",
       "      <td>1354.00</td>\n",
       "      <td>3.80</td>\n",
       "      <td>-356.04</td>\n",
       "      <td>6.39</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>USA</td>\n",
       "      <td>1320.03</td>\n",
       "      <td>1477.96</td>\n",
       "      <td>20.32</td>\n",
       "      <td>-353.58</td>\n",
       "      <td>13.14</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3241.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>USA</td>\n",
       "      <td>1696.92</td>\n",
       "      <td>750.14</td>\n",
       "      <td>4.78</td>\n",
       "      <td>-254.37</td>\n",
       "      <td>15.48</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Rest</td>\n",
       "      <td>1451.37</td>\n",
       "      <td>251.06</td>\n",
       "      <td>2.06</td>\n",
       "      <td>-379.77</td>\n",
       "      <td>3.90</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>336.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>9</td>\n",
       "      <td>Rest</td>\n",
       "      <td>1534.28</td>\n",
       "      <td>957.56</td>\n",
       "      <td>2.66</td>\n",
       "      <td>-260.19</td>\n",
       "      <td>6.24</td>\n",
       "      <td>12</td>\n",
       "      <td>4.0</td>\n",
       "      <td>110.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>3</td>\n",
       "      <td>Europe</td>\n",
       "      <td>1902.12</td>\n",
       "      <td>889.62</td>\n",
       "      <td>8.72</td>\n",
       "      <td>-382.59</td>\n",
       "      <td>19.98</td>\n",
       "      <td>12</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2672.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>3</td>\n",
       "      <td>Rest</td>\n",
       "      <td>1720.91</td>\n",
       "      <td>294.80</td>\n",
       "      <td>8.58</td>\n",
       "      <td>-469.68</td>\n",
       "      <td>6.06</td>\n",
       "      <td>16</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1722.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>12</td>\n",
       "      <td>Europe</td>\n",
       "      <td>1213.86</td>\n",
       "      <td>1526.42</td>\n",
       "      <td>3.60</td>\n",
       "      <td>-335.64</td>\n",
       "      <td>20.40</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>323.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>21</td>\n",
       "      <td>USA</td>\n",
       "      <td>1122.09</td>\n",
       "      <td>1321.72</td>\n",
       "      <td>3.74</td>\n",
       "      <td>-419.70</td>\n",
       "      <td>66.48</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3591.35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      F2      F4       F6       F7     F9     F11    F12  F14  F15   Target\n",
       "0      6      UK  1030.95   614.70  11.96 -333.60   1.86    6  4.0  1605.31\n",
       "1     12  Europe  1059.87  1354.00   3.80 -356.04   6.39    2  0.0     0.00\n",
       "2      6     USA  1320.03  1477.96  20.32 -353.58  13.14    8  1.0  3241.77\n",
       "3     12     USA  1696.92   750.14   4.78 -254.37  15.48   10  0.0     0.00\n",
       "4      3    Rest  1451.37   251.06   2.06 -379.77   3.90   10  0.0   336.25\n",
       "...   ..     ...      ...      ...    ...     ...    ...  ...  ...      ...\n",
       "1495   9    Rest  1534.28   957.56   2.66 -260.19   6.24   12  4.0   110.45\n",
       "1496   3  Europe  1902.12   889.62   8.72 -382.59  19.98   12  4.0  2672.58\n",
       "1497   3    Rest  1720.91   294.80   8.58 -469.68   6.06   16  4.0  1722.09\n",
       "1498  12  Europe  1213.86  1526.42   3.60 -335.64  20.40   10  1.0   323.48\n",
       "1499  21     USA  1122.09  1321.72   3.74 -419.70  66.48    2  0.0  3591.35\n",
       "\n",
       "[1500 rows x 10 columns]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_df=df[['F2','F4','F6','F7','F9','F11','F12','F14','F15','Target']]\n",
    "selected_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=selected_df.drop(['Target'],axis=1)\n",
    "y=selected_df['Target']\n",
    "\n",
    "\n",
    "cat=df1.dtypes==object\n",
    "# print(cat)\n",
    "# print(~cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 12)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct = ColumnTransformer(\n",
    "     [(\"ohe\", OneHotEncoder(handle_unknown='ignore'), cat),\n",
    "      (\"norm\", StandardScaler(), ~cat)])\n",
    "\n",
    "\n",
    "ct.fit(df1)\n",
    "x=ct.transform(df1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, X_test, y, y_test = train_test_split(x, y, test_size=0.20, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE= 507.4250409474444\n",
      "R2= 0.7815683767888106\n",
      "--------------------------------------------------\n",
      "RMSE= 506.5322497993542\n",
      "R2= 0.7823363414929355\n",
      "--------------------------------------------------\n",
      "RMSE= 506.565916677344\n",
      "R2= 0.7823074063190436\n",
      "--------------------------------------------------\n",
      "RMSE= 1168.6198686599744\n",
      "R2= -0.15855926310134105\n",
      "--------------------------------------------------\n",
      "RMSE= 482.59058007441774\n",
      "R2= 0.8024261771269001\n",
      "--------------------------------------------------\n",
      "RMSE= 561.8434506111254\n",
      "R2= 0.7322050594734542\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "      <th>Parameter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LinearRegression</th>\n",
       "      <td>507.425041</td>\n",
       "      <td>0.781568</td>\n",
       "      <td>Default</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>506.532250</td>\n",
       "      <td>0.782336</td>\n",
       "      <td>Default</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>506.565917</td>\n",
       "      <td>0.782307</td>\n",
       "      <td>Default</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVR</th>\n",
       "      <td>1168.619869</td>\n",
       "      <td>-0.158559</td>\n",
       "      <td>Default</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostingRegressor</th>\n",
       "      <td>482.590580</td>\n",
       "      <td>0.802426</td>\n",
       "      <td>Default</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestRegressor</th>\n",
       "      <td>561.843451</td>\n",
       "      <td>0.732205</td>\n",
       "      <td>Default</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  RMSE        R2 Parameter\n",
       "LinearRegression            507.425041  0.781568   Default\n",
       "Ridge                       506.532250  0.782336   Default\n",
       "Lasso                       506.565917  0.782307   Default\n",
       "SVR                        1168.619869 -0.158559   Default\n",
       "GradientBoostingRegressor   482.590580  0.802426   Default\n",
       "RandomForestRegressor       561.843451  0.732205   Default"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo=pd.DataFrame(columns=['RMSE','R2','Parameter'])\n",
    "pred = cross_val_predict(LinearRegression(), x, y, cv=10)\n",
    "\n",
    "MSE=mean_squared_error(y,pred)\n",
    "RMSE=math.sqrt(MSE)\n",
    "r2=r2_score(y,pred)\n",
    "print('RMSE=',RMSE)\n",
    "print('R2=',r2)\n",
    "print('-'*50)\n",
    "\n",
    "model='LinearRegression'\n",
    "algo.loc[model]=[RMSE,r2,'Default']\n",
    "algo\n",
    "\n",
    "\n",
    "pred = cross_val_predict(Ridge(), x, y, cv=10)\n",
    "MSE=mean_squared_error(y,pred)\n",
    "RMSE=math.sqrt(MSE)\n",
    "r2=r2_score(y,pred)\n",
    "print('RMSE=',RMSE)\n",
    "print('R2=',r2)\n",
    "print('-'*50)\n",
    "\n",
    "model='Ridge'\n",
    "algo.loc[model]=[RMSE,r2,'Default']\n",
    "algo\n",
    "\n",
    "\n",
    "pred = cross_val_predict(Lasso(), x, y, cv=10)\n",
    "MSE=mean_squared_error(y,pred)\n",
    "RMSE=math.sqrt(MSE)\n",
    "r2=r2_score(y,pred)\n",
    "print('RMSE=',RMSE)\n",
    "print('R2=',r2)\n",
    "print('-'*50)\n",
    "\n",
    "model='Lasso'\n",
    "algo.loc[model]=[RMSE,r2,'Default']\n",
    "algo\n",
    "\n",
    "\n",
    "\n",
    "pred = cross_val_predict(SVR(), x, y, cv=10)\n",
    "MSE=mean_squared_error(y,pred)\n",
    "RMSE=math.sqrt(MSE)\n",
    "r2=r2_score(y,pred)\n",
    "print('RMSE=',RMSE)\n",
    "print('R2=',r2)\n",
    "print('-'*50)\n",
    "\n",
    "model='SVR'\n",
    "algo.loc[model]=[RMSE,r2,'Default']\n",
    "algo\n",
    "\n",
    "\n",
    "pred = cross_val_predict(GradientBoostingRegressor(), x, y, cv=10)\n",
    "MSE=mean_squared_error(y,pred)\n",
    "RMSE=math.sqrt(MSE)\n",
    "r2=r2_score(y,pred)\n",
    "print('RMSE=',RMSE)\n",
    "print('R2=',r2)\n",
    "print('-'*50)\n",
    "\n",
    "model='GradientBoostingRegressor'\n",
    "algo.loc[model]=[RMSE,r2,'Default']\n",
    "algo\n",
    "\n",
    "\n",
    "pred = cross_val_predict(RandomForestRegressor(), x, y, cv=10)\n",
    "MSE=mean_squared_error(y,pred)\n",
    "RMSE=math.sqrt(MSE)\n",
    "r2=r2_score(y,pred)\n",
    "print('RMSE=',RMSE)\n",
    "print('R2=',r2)\n",
    "print('-'*50)\n",
    "\n",
    "model='RandomForestRegressor'\n",
    "algo.loc[model]=[RMSE,r2,'Default']\n",
    "algo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7783655173565555\n",
      "{'alpha': 10.0}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'alpha': np.logspace(-3, 3, 13)}\n",
    "\n",
    "clf = Ridge()\n",
    "grid = GridSearchCV(clf, param_grid, cv=5)\n",
    "grid.fit(x,y)\n",
    "\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE= 506.6380717622033\n",
      "R2= 0.782245385777401\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "      <th>Parameter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>506.638072</td>\n",
       "      <td>0.782245</td>\n",
       "      <td>{'alpha': 10.0}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             RMSE        R2        Parameter\n",
       "Ridge  506.638072  0.782245  {'alpha': 10.0}"
      ]
     },
     "execution_count": 1256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo=pd.DataFrame(columns=['RMSE','R2','Parameter'])\n",
    "pred = cross_val_predict(Ridge(**grid.best_params_), x, y, cv=10)\n",
    "\n",
    "MSE=mean_squared_error(y,pred)\n",
    "RMSE=math.sqrt(MSE)\n",
    "r2=r2_score(y,pred)\n",
    "print('RMSE=',RMSE)\n",
    "print('R2=',r2)\n",
    "print('-'*50)\n",
    "\n",
    "model='Ridge'\n",
    "algo.loc[model]=[RMSE,r2,grid.best_params_]\n",
    "algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7783345240656461\n",
      "{'alpha': 1.0}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'alpha': np.logspace(-3, 3, 13)}\n",
    "\n",
    "clf = Lasso()\n",
    "grid = GridSearchCV(clf, param_grid, cv=5)\n",
    "grid.fit(x,y)\n",
    "\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE= 506.565916677344\n",
      "R2= 0.7823074063190436\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "      <th>Parameter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>506.638072</td>\n",
       "      <td>0.782245</td>\n",
       "      <td>{'alpha': 10.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>506.565917</td>\n",
       "      <td>0.782307</td>\n",
       "      <td>{'alpha': 1.0}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             RMSE        R2        Parameter\n",
       "Ridge  506.638072  0.782245  {'alpha': 10.0}\n",
       "Lasso  506.565917  0.782307   {'alpha': 1.0}"
      ]
     },
     "execution_count": 1258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = cross_val_predict(Lasso(**grid.best_params_), x, y, cv=10)\n",
    "MSE=mean_squared_error(y,pred)\n",
    "RMSE=math.sqrt(MSE)\n",
    "r2=r2_score(y,pred)\n",
    "print('RMSE=',RMSE)\n",
    "print('R2=',r2)\n",
    "print('-'*50)\n",
    "\n",
    "model='Lasso'\n",
    "algo.loc[model]=[RMSE,r2,grid.best_params_]\n",
    "algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7259587021369522\n",
      "{'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "param_grid={'n_estimators':[100,200,300,400,500],\n",
    "            'max_features':[\"auto\", \"sqrt\", \"log2\"],\n",
    "            'max_depth':[2,4,8,10]\n",
    "           }\n",
    "         \n",
    "\n",
    "clf = RandomForestRegressor()\n",
    "grid = GridSearchCV(clf, param_grid, cv=5)\n",
    "grid.fit(x,y)\n",
    "\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE= 561.6576578326631\n",
      "R2= 0.7323821413261016\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "      <th>Parameter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>506.638072</td>\n",
       "      <td>0.782245</td>\n",
       "      <td>{'alpha': 10.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>506.565917</td>\n",
       "      <td>0.782307</td>\n",
       "      <td>{'alpha': 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestRegressor</th>\n",
       "      <td>561.657658</td>\n",
       "      <td>0.732382</td>\n",
       "      <td>{'max_depth': 10, 'max_features': 'sqrt', 'n_e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             RMSE        R2  \\\n",
       "Ridge                  506.638072  0.782245   \n",
       "Lasso                  506.565917  0.782307   \n",
       "RandomForestRegressor  561.657658  0.732382   \n",
       "\n",
       "                                                               Parameter  \n",
       "Ridge                                                    {'alpha': 10.0}  \n",
       "Lasso                                                     {'alpha': 1.0}  \n",
       "RandomForestRegressor  {'max_depth': 10, 'max_features': 'sqrt', 'n_e...  "
      ]
     },
     "execution_count": 1260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = cross_val_predict(RandomForestRegressor(**grid.best_params_), x, y, cv=10)\n",
    "MSE=mean_squared_error(y,pred)\n",
    "RMSE=math.sqrt(MSE)\n",
    "r2=r2_score(y,pred)\n",
    "print('RMSE=',RMSE)\n",
    "print('R2=',r2)\n",
    "print('-'*50)\n",
    "\n",
    "model='RandomForestRegressor'\n",
    "algo.loc[model]=[RMSE,r2,grid.best_params_]\n",
    "algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8529084713001488\n",
      "{'max_depth': 4, 'max_features': 'sqrt', 'n_estimators': 600}\n"
     ]
    }
   ],
   "source": [
    "param_grid={'n_estimators':[100,200,300,400,500,600,700,800],\n",
    "            'max_features':[\"auto\", \"sqrt\", \"log2\"],\n",
    "            'max_depth':[2,4,8,10]\n",
    "           }\n",
    "         \n",
    "\n",
    "clf = GradientBoostingRegressor()\n",
    "grid = GridSearchCV(clf, param_grid, cv=5)\n",
    "grid.fit(x,y)\n",
    "\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE= 410.44184178195997\n",
      "R2= 0.8570859332127738\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "      <th>Parameter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>506.638072</td>\n",
       "      <td>0.782245</td>\n",
       "      <td>{'alpha': 10.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>506.565917</td>\n",
       "      <td>0.782307</td>\n",
       "      <td>{'alpha': 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestRegressor</th>\n",
       "      <td>561.657658</td>\n",
       "      <td>0.732382</td>\n",
       "      <td>{'max_depth': 10, 'max_features': 'sqrt', 'n_e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostingRegressor</th>\n",
       "      <td>410.441842</td>\n",
       "      <td>0.857086</td>\n",
       "      <td>{'max_depth': 4, 'max_features': 'sqrt', 'n_es...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 RMSE        R2  \\\n",
       "Ridge                      506.638072  0.782245   \n",
       "Lasso                      506.565917  0.782307   \n",
       "RandomForestRegressor      561.657658  0.732382   \n",
       "GradientBoostingRegressor  410.441842  0.857086   \n",
       "\n",
       "                                                                   Parameter  \n",
       "Ridge                                                        {'alpha': 10.0}  \n",
       "Lasso                                                         {'alpha': 1.0}  \n",
       "RandomForestRegressor      {'max_depth': 10, 'max_features': 'sqrt', 'n_e...  \n",
       "GradientBoostingRegressor  {'max_depth': 4, 'max_features': 'sqrt', 'n_es...  "
      ]
     },
     "execution_count": 1262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = cross_val_predict(GradientBoostingRegressor(**grid.best_params_), x, y, cv=10)\n",
    "MSE=mean_squared_error(y,pred)\n",
    "RMSE=math.sqrt(MSE)\n",
    "r2=r2_score(y,pred)\n",
    "print('RMSE=',RMSE)\n",
    "print('R2=',r2)\n",
    "print('-'*50)\n",
    "\n",
    "model='GradientBoostingRegressor'\n",
    "algo.loc[model]=[RMSE,r2,grid.best_params_]\n",
    "algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7595451843120288\n",
      "{'C': 200, 'gamma': 1e-07, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "param_grid={'kernel': ('rbf','linear','poly'),'C':[1.5,10,200],'gamma': [1e-7, 1e-4]}\n",
    "\n",
    "clf = SVR()\n",
    "grid = GridSearchCV(clf, param_grid, cv=5)\n",
    "grid.fit(x,y)\n",
    "\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE= 525.6068523658425\n",
      "R2= 0.765634457935142\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "      <th>Parameter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>506.638072</td>\n",
       "      <td>0.782245</td>\n",
       "      <td>{'alpha': 10.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>506.565917</td>\n",
       "      <td>0.782307</td>\n",
       "      <td>{'alpha': 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestRegressor</th>\n",
       "      <td>561.657658</td>\n",
       "      <td>0.732382</td>\n",
       "      <td>{'max_depth': 10, 'max_features': 'sqrt', 'n_e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostingRegressor</th>\n",
       "      <td>410.441842</td>\n",
       "      <td>0.857086</td>\n",
       "      <td>{'max_depth': 4, 'max_features': 'sqrt', 'n_es...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVR</th>\n",
       "      <td>525.606852</td>\n",
       "      <td>0.765634</td>\n",
       "      <td>{'C': 200, 'gamma': 1e-07, 'kernel': 'linear'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 RMSE        R2  \\\n",
       "Ridge                      506.638072  0.782245   \n",
       "Lasso                      506.565917  0.782307   \n",
       "RandomForestRegressor      561.657658  0.732382   \n",
       "GradientBoostingRegressor  410.441842  0.857086   \n",
       "SVR                        525.606852  0.765634   \n",
       "\n",
       "                                                                   Parameter  \n",
       "Ridge                                                        {'alpha': 10.0}  \n",
       "Lasso                                                         {'alpha': 1.0}  \n",
       "RandomForestRegressor      {'max_depth': 10, 'max_features': 'sqrt', 'n_e...  \n",
       "GradientBoostingRegressor  {'max_depth': 4, 'max_features': 'sqrt', 'n_es...  \n",
       "SVR                           {'C': 200, 'gamma': 1e-07, 'kernel': 'linear'}  "
      ]
     },
     "execution_count": 1264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = cross_val_predict(SVR(**grid.best_params_), x, y, cv=10)\n",
    "MSE=mean_squared_error(y,pred)\n",
    "RMSE=math.sqrt(MSE)\n",
    "r2=r2_score(y,pred)\n",
    "print('RMSE=',RMSE)\n",
    "print('R2=',r2)\n",
    "print('-'*50)\n",
    "\n",
    "model='SVR'\n",
    "algo.loc[model]=[RMSE,r2,grid.best_params_]\n",
    "algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try using deep learning model on this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Activation, Dropout,BatchNormalization\n",
    "from keras.optimizers import Adam,SGD\n",
    "from keras.callbacks import TensorBoard,EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(x, y, test_size=0.10, random_state=41)\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.20, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1080, 12)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 12)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from keras.initializers import RandomNormal\n",
    "fan_in=X_train.shape[1]\n",
    "relu_std=math.sqrt(2/fan_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 55)                715       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 56        \n",
      "=================================================================\n",
      "Total params: 771\n",
      "Trainable params: 771\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/500000\n",
      "1/1 [==============================] - 0s 244ms/step - loss: 2035609.3750 - val_loss: 1423302.6250\n",
      "Epoch 2/500000\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 1563228.8750 - val_loss: 743773.8750\n",
      "Epoch 3/500000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 852158.2500 - val_loss: 550418.0000\n",
      "Epoch 4/500000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 629568.8125 - val_loss: 446913.1250\n",
      "Epoch 5/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 520583.2188 - val_loss: 302193.3438\n",
      "Epoch 6/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 348443.0625 - val_loss: 219787.7656\n",
      "Epoch 7/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 236849.1250 - val_loss: 209500.8281\n",
      "Epoch 8/500000\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 197129.1250 - val_loss: 202076.4531\n",
      "Epoch 9/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 173678.6250 - val_loss: 167245.2344\n",
      "Epoch 10/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 140438.9531 - val_loss: 127764.6172\n",
      "Epoch 11/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 122453.3672 - val_loss: 103615.9297\n",
      "Epoch 12/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 125708.2656 - val_loss: 103374.7109\n",
      "Epoch 13/500000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 128616.5000 - val_loss: 94952.5156\n",
      "Epoch 14/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 110220.9062 - val_loss: 89628.0391\n",
      "Epoch 15/500000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 90725.8203 - val_loss: 90495.2734\n",
      "Epoch 16/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 81569.7344 - val_loss: 87226.8359\n",
      "Epoch 17/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 79145.4844 - val_loss: 79355.0000\n",
      "Epoch 18/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 72069.3203 - val_loss: 59586.1875\n",
      "Epoch 19/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 62085.9961 - val_loss: 49313.3398\n",
      "Epoch 20/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 56158.9023 - val_loss: 47652.4180\n",
      "Epoch 21/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 53943.6016 - val_loss: 46384.7109\n",
      "Epoch 22/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 51451.8359 - val_loss: 41061.9336\n",
      "Epoch 23/500000\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 46262.0352 - val_loss: 36782.6016\n",
      "Epoch 24/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 40693.2812 - val_loss: 38263.0273\n",
      "Epoch 25/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 38832.7266 - val_loss: 38893.8242\n",
      "Epoch 26/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 37861.3086 - val_loss: 38864.4844\n",
      "Epoch 27/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 35263.4180 - val_loss: 34744.7227\n",
      "Epoch 28/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 31354.1855 - val_loss: 28779.8105\n",
      "Epoch 29/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 28062.2383 - val_loss: 27033.8789\n",
      "Epoch 30/500000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 26427.2637 - val_loss: 27072.5176\n",
      "Epoch 31/500000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 25648.9160 - val_loss: 27638.8203\n",
      "Epoch 32/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 23850.2539 - val_loss: 26648.0527\n",
      "Epoch 33/500000\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 22125.9531 - val_loss: 25866.0293\n",
      "Epoch 34/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 21527.7969 - val_loss: 25760.3848\n",
      "Epoch 35/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 21191.5449 - val_loss: 24769.3574\n",
      "Epoch 36/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 20572.0566 - val_loss: 22991.8535\n",
      "Epoch 37/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 19275.3633 - val_loss: 20722.1172\n",
      "Epoch 38/500000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 18189.5117 - val_loss: 20349.0234\n",
      "Epoch 39/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 17382.0293 - val_loss: 20782.7461\n",
      "Epoch 40/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 16811.6973 - val_loss: 19338.1621\n",
      "Epoch 41/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 15888.0264 - val_loss: 19270.5293\n",
      "Epoch 42/500000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 15138.4941 - val_loss: 19887.4961\n",
      "Epoch 43/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 14885.5000 - val_loss: 20986.4023\n",
      "Epoch 44/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 15074.9736 - val_loss: 20922.1797\n",
      "Epoch 45/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 14773.0869 - val_loss: 19646.5840\n",
      "Epoch 46/500000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 14192.2295 - val_loss: 18060.3867\n",
      "Epoch 47/500000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 13786.8428 - val_loss: 16950.9863\n",
      "Epoch 48/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 13526.8057 - val_loss: 17165.1504\n",
      "Epoch 49/500000\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 13169.9219 - val_loss: 17331.5723\n",
      "Epoch 50/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 12858.9688 - val_loss: 17965.6875\n",
      "Epoch 51/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 12485.9150 - val_loss: 18694.8867\n",
      "Epoch 52/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 12160.4033 - val_loss: 19100.8613\n",
      "Epoch 53/500000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 11930.4629 - val_loss: 19276.1465\n",
      "Epoch 54/500000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 11764.2920 - val_loss: 19388.0508\n",
      "Epoch 55/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 11570.0771 - val_loss: 19940.9180\n",
      "Epoch 56/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 11388.9922 - val_loss: 19162.3926\n",
      "Epoch 57/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 11102.7041 - val_loss: 18764.3633\n",
      "Epoch 58/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 10963.8945 - val_loss: 18310.7207\n",
      "Epoch 59/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 10829.6982 - val_loss: 18216.4824\n",
      "Epoch 60/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 10649.5205 - val_loss: 18111.1641\n",
      "Epoch 61/500000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 10522.2500 - val_loss: 17531.3027\n",
      "Epoch 62/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 10360.9102 - val_loss: 16779.9180\n",
      "Epoch 63/500000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 10139.3994 - val_loss: 16426.3320\n",
      "Epoch 64/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 9950.1514 - val_loss: 16204.4443\n",
      "Epoch 65/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 9766.1650 - val_loss: 16022.2666\n",
      "Epoch 66/500000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 9567.3037 - val_loss: 15907.8125\n",
      "Epoch 67/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 9365.1318 - val_loss: 15938.5596\n",
      "Epoch 68/500000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 9246.5635 - val_loss: 15826.7373\n",
      "Epoch 69/500000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 9112.2422 - val_loss: 15725.4775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 9020.3066 - val_loss: 15762.0049\n",
      "Epoch 71/500000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 8971.0088 - val_loss: 15907.4561\n",
      "Epoch 72/500000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 8898.2256 - val_loss: 15955.2539\n",
      "Epoch 73/500000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 8818.8613 - val_loss: 15790.1211\n",
      "Epoch 74/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 8753.0830 - val_loss: 15660.2900\n",
      "Epoch 75/500000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 8717.3555 - val_loss: 15663.7002\n",
      "Epoch 76/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 8638.9014 - val_loss: 15694.2871\n",
      "Epoch 77/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 8581.5518 - val_loss: 15336.8711\n",
      "Epoch 78/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8532.1201 - val_loss: 15131.1621\n",
      "Epoch 79/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 8461.0000 - val_loss: 14947.6943\n",
      "Epoch 80/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 8374.6367 - val_loss: 14780.7490\n",
      "Epoch 81/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 8314.3096 - val_loss: 14635.0566\n",
      "Epoch 82/500000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 8255.0146 - val_loss: 14552.4980\n",
      "Epoch 83/500000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8176.6743 - val_loss: 14487.5371\n",
      "Epoch 84/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8120.1558 - val_loss: 14422.3525\n",
      "Epoch 85/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8051.9619 - val_loss: 14349.2041\n",
      "Epoch 86/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 7973.3047 - val_loss: 14326.2100\n",
      "Epoch 87/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 7915.4878 - val_loss: 14331.8887\n",
      "Epoch 88/500000\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 7877.5742 - val_loss: 14316.2480\n",
      "Epoch 89/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 7839.8296 - val_loss: 14214.8574\n",
      "Epoch 90/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 7784.8979 - val_loss: 14108.3584\n",
      "Epoch 91/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 7730.2271 - val_loss: 14081.6836\n",
      "Epoch 92/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 7663.2554 - val_loss: 14141.7607\n",
      "Epoch 93/500000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 7598.8037 - val_loss: 14355.5693\n",
      "Epoch 94/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 7545.1548 - val_loss: 14601.0029\n",
      "Epoch 95/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 7495.4751 - val_loss: 14784.6475\n",
      "Epoch 96/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 7460.8066 - val_loss: 14943.4336\n",
      "Epoch 97/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 7415.4785 - val_loss: 15063.3584\n",
      "Epoch 98/500000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 7360.7275 - val_loss: 15074.2529\n",
      "Epoch 99/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 7316.6074 - val_loss: 14951.7900\n",
      "Epoch 100/500000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 7279.2617 - val_loss: 14790.1855\n",
      "Epoch 101/500000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7231.0249 - val_loss: 14627.5957\n",
      "Epoch 102/500000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 7175.5415 - val_loss: 14471.0576\n",
      "Epoch 103/500000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 7121.1919 - val_loss: 14389.0400\n",
      "Epoch 104/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 7089.6660 - val_loss: 14352.9375\n",
      "Epoch 105/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 7069.0044 - val_loss: 14315.4629\n",
      "Epoch 106/500000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 7045.4526 - val_loss: 14292.7461\n",
      "Epoch 107/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 7013.6084 - val_loss: 14310.2471\n",
      "Epoch 108/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 6968.4805 - val_loss: 14287.2002\n",
      "Epoch 109/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 6939.7900 - val_loss: 14224.5420\n",
      "Epoch 110/500000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 6921.9673 - val_loss: 14147.4209\n",
      "Epoch 111/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 6896.6021 - val_loss: 14054.4004\n",
      "Epoch 112/500000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 6868.4258 - val_loss: 13946.1914\n",
      "Epoch 113/500000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 6836.1792 - val_loss: 13880.4102\n",
      "Epoch 114/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 6806.8574 - val_loss: 13855.7588\n",
      "Epoch 115/500000\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 6787.6001 - val_loss: 13868.7373\n",
      "Epoch 116/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 6765.5557 - val_loss: 13918.7910\n",
      "Epoch 117/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 6742.9922 - val_loss: 13994.4189\n",
      "Epoch 118/500000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 6710.5581 - val_loss: 14088.2451\n",
      "Epoch 119/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 6681.7095 - val_loss: 14178.2607\n",
      "Epoch 120/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 6666.4307 - val_loss: 14222.9600\n",
      "Epoch 121/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 6654.0742 - val_loss: 14201.5635\n",
      "Epoch 122/500000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 6645.4121 - val_loss: 14133.9189\n",
      "Epoch 123/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 6630.5225 - val_loss: 14065.7637\n",
      "Epoch 124/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 6613.8354 - val_loss: 14035.7314\n",
      "Epoch 125/500000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 6598.9639 - val_loss: 14033.0459\n",
      "Epoch 126/500000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 6585.5474 - val_loss: 14018.3252\n",
      "Epoch 127/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 6571.7476 - val_loss: 13994.0771\n",
      "Epoch 128/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 6558.1304 - val_loss: 13983.1650\n",
      "Epoch 129/500000\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 6544.2212 - val_loss: 13985.8994\n",
      "Epoch 130/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 6530.9761 - val_loss: 13982.2520\n",
      "Epoch 131/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 6518.0020 - val_loss: 13957.5625\n",
      "Epoch 132/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 6499.2261 - val_loss: 13928.2646\n",
      "Epoch 133/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 6462.9600 - val_loss: 13939.2646\n",
      "Epoch 134/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 6433.3096 - val_loss: 14023.7275\n",
      "Epoch 135/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 6410.4775 - val_loss: 14162.2979\n",
      "Epoch 136/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 6395.1738 - val_loss: 14313.1670\n",
      "Epoch 137/500000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 6380.0020 - val_loss: 14452.6611\n",
      "Epoch 138/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 6365.0825 - val_loss: 14572.7979\n",
      "Epoch 139/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 6346.4102 - val_loss: 14672.6543\n",
      "Epoch 140/500000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 6318.4023 - val_loss: 14758.6885\n",
      "Epoch 141/500000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 6279.5029 - val_loss: 14833.8301\n",
      "Epoch 142/500000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 6288.4946 - val_loss: 14870.3447\n",
      "Epoch 143/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 6267.4697 - val_loss: 14851.8545\n",
      "Epoch 144/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 6249.6479 - val_loss: 14815.6836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 6240.9141 - val_loss: 14784.0742\n",
      "Epoch 146/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 6222.3525 - val_loss: 14743.5283\n",
      "Epoch 147/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 6208.3398 - val_loss: 14690.1592\n",
      "Epoch 148/500000\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 6194.6333 - val_loss: 14649.7686\n",
      "Epoch 149/500000\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 6178.5918 - val_loss: 14654.2441\n",
      "Epoch 150/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 6138.1582 - val_loss: 14671.0586\n",
      "Epoch 151/500000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 6126.3369 - val_loss: 14643.4570\n",
      "Epoch 152/500000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 6109.6445 - val_loss: 14561.1211\n",
      "Epoch 153/500000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 6092.1279 - val_loss: 14457.6396\n",
      "Epoch 154/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 6087.6973 - val_loss: 14342.7900\n",
      "Epoch 155/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 6079.3813 - val_loss: 14209.8350\n",
      "Epoch 156/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 6065.2422 - val_loss: 14063.2344\n",
      "Epoch 157/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 6052.6909 - val_loss: 13933.1523\n",
      "Epoch 158/500000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 6039.3599 - val_loss: 13854.1475\n",
      "Epoch 159/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 6028.6841 - val_loss: 13819.6543\n",
      "Epoch 160/500000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 6023.9956 - val_loss: 13780.3564\n",
      "Epoch 161/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 6015.4165 - val_loss: 13707.9238\n",
      "Epoch 162/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 6006.5825 - val_loss: 13631.0020\n",
      "Epoch 163/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5995.3931 - val_loss: 13581.5693\n",
      "Epoch 164/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5983.6011 - val_loss: 13566.3838\n",
      "Epoch 165/500000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5972.5283 - val_loss: 13582.0303\n",
      "Epoch 166/500000\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 5962.2583 - val_loss: 13610.7559\n",
      "Epoch 167/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5952.7632 - val_loss: 13636.1260\n",
      "Epoch 168/500000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 5943.4316 - val_loss: 13652.3477\n",
      "Epoch 169/500000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 5933.7822 - val_loss: 13664.0000\n",
      "Epoch 170/500000\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 5925.3296 - val_loss: 13678.8994\n",
      "Epoch 171/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5917.4868 - val_loss: 13696.4863\n",
      "Epoch 172/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5908.0239 - val_loss: 13708.5059\n",
      "Epoch 173/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5898.7656 - val_loss: 13706.2373\n",
      "Epoch 174/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5889.6138 - val_loss: 13694.4355\n",
      "Epoch 175/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5880.2656 - val_loss: 13691.9209\n",
      "Epoch 176/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5870.5391 - val_loss: 13708.6064\n",
      "Epoch 177/500000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 5858.1226 - val_loss: 13736.8438\n",
      "Epoch 178/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5839.3652 - val_loss: 13759.8896\n",
      "Epoch 179/500000\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 5814.1123 - val_loss: 13768.9082\n",
      "Epoch 180/500000\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 5793.4209 - val_loss: 13771.3164\n",
      "Epoch 181/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5780.1064 - val_loss: 13777.8525\n",
      "Epoch 182/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5760.0581 - val_loss: 13789.5615\n",
      "Epoch 183/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 5734.7402 - val_loss: 13799.3145\n",
      "Epoch 184/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5720.0122 - val_loss: 13802.6895\n",
      "Epoch 185/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5711.9053 - val_loss: 13803.3145\n",
      "Epoch 186/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5706.3350 - val_loss: 13808.7754\n",
      "Epoch 187/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5700.4136 - val_loss: 13824.0254\n",
      "Epoch 188/500000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 5692.5405 - val_loss: 13847.4648\n",
      "Epoch 189/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5683.7456 - val_loss: 13871.8145\n",
      "Epoch 190/500000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 5675.1909 - val_loss: 13888.3730\n",
      "Epoch 191/500000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 5668.1104 - val_loss: 13894.3213\n",
      "Epoch 192/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5660.5591 - val_loss: 13895.6621\n",
      "Epoch 193/500000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 5650.8291 - val_loss: 13900.3740\n",
      "Epoch 194/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5641.2056 - val_loss: 13911.3604\n",
      "Epoch 195/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 5633.7485 - val_loss: 13924.0000\n",
      "Epoch 196/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5627.8813 - val_loss: 13929.0205\n",
      "Epoch 197/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5621.9175 - val_loss: 13920.3438\n",
      "Epoch 198/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5615.1465 - val_loss: 13900.5000\n",
      "Epoch 199/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5607.4976 - val_loss: 13878.0811\n",
      "Epoch 200/500000\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 5599.2188 - val_loss: 13859.2070\n",
      "Epoch 201/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5589.7310 - val_loss: 13842.4990\n",
      "Epoch 202/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5579.0342 - val_loss: 13824.4287\n",
      "Epoch 203/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5571.9141 - val_loss: 13804.8750\n",
      "Epoch 204/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5566.2417 - val_loss: 13786.4570\n",
      "Epoch 205/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5559.2520 - val_loss: 13770.1660\n",
      "Epoch 206/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5552.0361 - val_loss: 13752.9893\n",
      "Epoch 207/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5545.4639 - val_loss: 13729.6758\n",
      "Epoch 208/500000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 5539.5767 - val_loss: 13697.6025\n",
      "Epoch 209/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 5533.3896 - val_loss: 13659.5508\n",
      "Epoch 210/500000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 5526.9214 - val_loss: 13620.4980\n",
      "Epoch 211/500000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 5520.0269 - val_loss: 13582.8320\n",
      "Epoch 212/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 5512.6426 - val_loss: 13545.2354\n",
      "Epoch 213/500000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 5505.7700 - val_loss: 13505.0713\n",
      "Epoch 214/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 5499.8965 - val_loss: 13461.9688\n",
      "Epoch 215/500000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 5493.9062 - val_loss: 13419.0830\n",
      "Epoch 216/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5486.9395 - val_loss: 13380.8145\n",
      "Epoch 217/500000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 5479.5361 - val_loss: 13350.3936\n",
      "Epoch 218/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5472.5498 - val_loss: 13329.1670\n",
      "Epoch 219/500000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5466.8657 - val_loss: 13315.9434\n",
      "Epoch 220/500000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 5462.4238 - val_loss: 13307.7314\n",
      "Epoch 221/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5458.5591 - val_loss: 13301.8477\n",
      "Epoch 222/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5454.9556 - val_loss: 13296.6084\n",
      "Epoch 223/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5451.6899 - val_loss: 13290.8320\n",
      "Epoch 224/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5448.3989 - val_loss: 13283.1260\n",
      "Epoch 225/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5444.8228 - val_loss: 13272.0996\n",
      "Epoch 226/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5441.1768 - val_loss: 13257.1621\n",
      "Epoch 227/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5437.5923 - val_loss: 13239.3711\n",
      "Epoch 228/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 5433.9404 - val_loss: 13221.3662\n",
      "Epoch 229/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5430.2373 - val_loss: 13205.4531\n",
      "Epoch 230/500000\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 5426.6035 - val_loss: 13191.6504\n",
      "Epoch 231/500000\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 5422.9917 - val_loss: 13177.9785\n",
      "Epoch 232/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5419.2207 - val_loss: 13162.7539\n",
      "Epoch 233/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5415.2197 - val_loss: 13146.0283\n",
      "Epoch 234/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5411.1050 - val_loss: 13129.6045\n",
      "Epoch 235/500000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 5407.1011 - val_loss: 13115.9434\n",
      "Epoch 236/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5403.2603 - val_loss: 13106.5146\n",
      "Epoch 237/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5399.1890 - val_loss: 13101.0479\n",
      "Epoch 238/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5394.6416 - val_loss: 13098.1836\n",
      "Epoch 239/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5389.5542 - val_loss: 13096.8438\n",
      "Epoch 240/500000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 5383.6797 - val_loss: 13096.8271\n",
      "Epoch 241/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5377.7222 - val_loss: 13098.9043\n",
      "Epoch 242/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5373.5303 - val_loss: 13104.4951\n",
      "Epoch 243/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5370.6465 - val_loss: 13114.3662\n",
      "Epoch 244/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5367.7246 - val_loss: 13127.9473\n",
      "Epoch 245/500000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 5364.1172 - val_loss: 13143.9736\n",
      "Epoch 246/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5359.7900 - val_loss: 13161.5508\n",
      "Epoch 247/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5355.1240 - val_loss: 13180.5439\n",
      "Epoch 248/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5350.7266 - val_loss: 13200.8584\n",
      "Epoch 249/500000\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 5346.7524 - val_loss: 13221.3438\n",
      "Epoch 250/500000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 5343.5020 - val_loss: 13240.4521\n",
      "Epoch 251/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 5340.5947 - val_loss: 13257.8330\n",
      "Epoch 252/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 5337.5786 - val_loss: 13274.1895\n",
      "Epoch 253/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5334.4121 - val_loss: 13289.9355\n",
      "Epoch 254/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5331.1108 - val_loss: 13304.4189\n",
      "Epoch 255/500000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 5327.8174 - val_loss: 13316.5996\n",
      "Epoch 256/500000\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 5324.6860 - val_loss: 13326.0264\n",
      "Epoch 257/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 5321.8213 - val_loss: 13333.0225\n",
      "Epoch 258/500000\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 5319.1665 - val_loss: 13338.5039\n",
      "Epoch 259/500000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 5316.6567 - val_loss: 13343.6777\n",
      "Epoch 260/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5314.1558 - val_loss: 13349.6221\n",
      "Epoch 261/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5311.6094 - val_loss: 13356.8604\n",
      "Epoch 262/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5308.9775 - val_loss: 13365.1348\n",
      "Epoch 263/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5306.1597 - val_loss: 13373.9072\n",
      "Epoch 264/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5303.1094 - val_loss: 13383.0410\n",
      "Epoch 265/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5299.6416 - val_loss: 13392.8701\n",
      "Epoch 266/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5295.3511 - val_loss: 13403.6621\n",
      "Epoch 267/500000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 5289.6523 - val_loss: 13414.9883\n",
      "Epoch 268/500000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 5282.1172 - val_loss: 13426.2549\n",
      "Epoch 269/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5274.6421 - val_loss: 13437.8701\n",
      "Epoch 270/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5269.8940 - val_loss: 13450.4414\n",
      "Epoch 271/500000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 5266.9463 - val_loss: 13464.7275\n",
      "Epoch 272/500000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 5264.6562 - val_loss: 13481.9561\n",
      "Epoch 273/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 5262.5620 - val_loss: 13503.7314\n",
      "Epoch 274/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5260.4131 - val_loss: 13532.3105\n",
      "Epoch 275/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5257.9663 - val_loss: 13572.1855\n",
      "Epoch 276/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5254.8901 - val_loss: 13631.8486\n",
      "Epoch 277/500000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 5251.1382 - val_loss: 13705.4668\n",
      "Epoch 278/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5249.6343 - val_loss: 13746.4590\n",
      "Epoch 279/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5246.4956 - val_loss: 13754.5996\n",
      "Epoch 280/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5244.0854 - val_loss: 13764.1064\n",
      "Epoch 281/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5242.6777 - val_loss: 13786.6631\n",
      "Epoch 282/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5239.7241 - val_loss: 13810.0518\n",
      "Epoch 283/500000\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 5237.8359 - val_loss: 13811.1924\n",
      "Epoch 284/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5235.2939 - val_loss: 13792.8525\n",
      "Epoch 285/500000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 5232.4277 - val_loss: 13777.7646\n",
      "Epoch 286/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5230.2539 - val_loss: 13780.7920\n",
      "Epoch 287/500000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 5226.6592 - val_loss: 13801.2529\n",
      "Epoch 288/500000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 5222.8325 - val_loss: 13825.4814\n",
      "Epoch 289/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5217.8369 - val_loss: 13849.4336\n",
      "Epoch 290/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5212.5400 - val_loss: 13880.1045\n",
      "Epoch 291/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5209.9834 - val_loss: 13915.7959\n",
      "Epoch 292/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5208.0620 - val_loss: 13947.6162\n",
      "Epoch 293/500000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step - loss: 5206.3882 - val_loss: 13965.2207\n",
      "Epoch 294/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5204.7681 - val_loss: 13970.8330\n",
      "Epoch 295/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5202.6743 - val_loss: 13977.0146\n",
      "Epoch 296/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5200.8403 - val_loss: 13990.6504\n",
      "Epoch 297/500000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 5198.6172 - val_loss: 14006.3652\n",
      "Epoch 298/500000\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 5196.4053 - val_loss: 14015.2568\n",
      "Epoch 299/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5194.2295 - val_loss: 14017.1895\n",
      "Epoch 300/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5191.6895 - val_loss: 14018.8643\n",
      "Epoch 301/500000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 5189.3047 - val_loss: 14025.5342\n",
      "Epoch 302/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5186.6274 - val_loss: 14034.5498\n",
      "Epoch 303/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5183.8037 - val_loss: 14039.8506\n",
      "Epoch 304/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5180.9536 - val_loss: 14040.0098\n",
      "Epoch 305/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5177.9009 - val_loss: 14038.1348\n",
      "Epoch 306/500000\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 5174.9175 - val_loss: 14037.6572\n",
      "Epoch 307/500000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 5171.7676 - val_loss: 14038.2354\n",
      "Epoch 308/500000\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 5168.5806 - val_loss: 14037.0518\n",
      "Epoch 309/500000\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 5165.2881 - val_loss: 14033.4248\n",
      "Epoch 310/500000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 5161.6392 - val_loss: 14029.5977\n",
      "Epoch 311/500000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 5158.0635 - val_loss: 14027.2178\n",
      "Epoch 312/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5154.9629 - val_loss: 14024.8691\n",
      "Epoch 313/500000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 5152.5107 - val_loss: 14020.0176\n",
      "Epoch 314/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5150.4233 - val_loss: 14012.7188\n",
      "Epoch 315/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5148.3481 - val_loss: 14005.8438\n",
      "Epoch 316/500000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 5146.2637 - val_loss: 14001.2061\n",
      "Epoch 317/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5144.0967 - val_loss: 13997.1885\n",
      "Epoch 318/500000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 5141.9463 - val_loss: 13991.3701\n",
      "Epoch 319/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5139.8452 - val_loss: 13984.4023\n",
      "Epoch 320/500000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 5137.7593 - val_loss: 13979.6094\n",
      "Epoch 321/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 5135.6953 - val_loss: 13978.8916\n",
      "Epoch 322/500000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 5133.5278 - val_loss: 13980.5312\n",
      "Epoch 323/500000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 5131.1934 - val_loss: 13981.4707\n",
      "Epoch 324/500000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 5128.5972 - val_loss: 13980.4590\n",
      "Epoch 325/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5125.8042 - val_loss: 13977.4561\n",
      "Epoch 326/500000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 5123.4448 - val_loss: 13971.1982\n",
      "Epoch 327/500000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 5121.6113 - val_loss: 13960.0361\n",
      "Epoch 328/500000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 5119.2515 - val_loss: 13946.0273\n",
      "Epoch 329/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 5116.8354 - val_loss: 13934.9209\n",
      "Epoch 330/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5114.6128 - val_loss: 13930.5537\n",
      "Epoch 331/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 5112.5098 - val_loss: 13931.7988\n",
      "Epoch 332/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5110.6094 - val_loss: 13934.6064\n",
      "Epoch 333/500000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 5108.5400 - val_loss: 13936.1104\n",
      "Epoch 334/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5106.4258 - val_loss: 13936.7949\n",
      "Epoch 335/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5104.6846 - val_loss: 13938.6270\n",
      "Epoch 336/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5102.8359 - val_loss: 13941.7754\n",
      "Epoch 337/500000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 5100.7456 - val_loss: 13944.6836\n",
      "Epoch 338/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5098.9224 - val_loss: 13946.8311\n",
      "Epoch 339/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5097.4233 - val_loss: 13949.8936\n",
      "Epoch 340/500000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 5096.0112 - val_loss: 13955.6885\n",
      "Epoch 341/500000\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 5094.3975 - val_loss: 13963.2100\n",
      "Epoch 342/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5092.4268 - val_loss: 13969.2920\n",
      "Epoch 343/500000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 5090.2617 - val_loss: 13971.8945\n",
      "Epoch 344/500000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 5088.0498 - val_loss: 13972.0928\n",
      "Epoch 345/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 5085.8149 - val_loss: 13973.0498\n",
      "Epoch 346/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5083.4761 - val_loss: 13977.7061\n",
      "Epoch 347/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5080.9688 - val_loss: 13986.6660\n",
      "Epoch 348/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5078.5859 - val_loss: 13998.1182\n",
      "Epoch 349/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5076.6475 - val_loss: 14009.8662\n",
      "Epoch 350/500000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 5075.0303 - val_loss: 14021.0938\n",
      "Epoch 351/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5073.5562 - val_loss: 14032.5586\n",
      "Epoch 352/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5072.1562 - val_loss: 14045.2334\n",
      "Epoch 353/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5070.7729 - val_loss: 14058.8682\n",
      "Epoch 354/500000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 5069.3892 - val_loss: 14072.1787\n",
      "Epoch 355/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5067.9761 - val_loss: 14084.0137\n",
      "Epoch 356/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 5066.4653 - val_loss: 14094.1543\n",
      "Epoch 357/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5064.8965 - val_loss: 14103.0889\n",
      "Epoch 358/500000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5063.3657 - val_loss: 14111.2246\n",
      "Epoch 359/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5061.8794 - val_loss: 14118.4561\n",
      "Epoch 360/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5060.4189 - val_loss: 14124.5205\n",
      "Epoch 361/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5058.9526 - val_loss: 14129.6250\n",
      "Epoch 362/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5057.4443 - val_loss: 14134.7920\n",
      "Epoch 363/500000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 5055.9092 - val_loss: 14141.3291\n",
      "Epoch 364/500000\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 5054.3604 - val_loss: 14149.8242\n",
      "Epoch 365/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5052.7935 - val_loss: 14159.4775\n",
      "Epoch 366/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5051.2344 - val_loss: 14168.7227\n",
      "Epoch 367/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5049.6758 - val_loss: 14176.4004\n",
      "Epoch 368/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5048.0615 - val_loss: 14182.5625\n",
      "Epoch 369/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 5046.3472 - val_loss: 14188.1367\n",
      "Epoch 370/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5044.4863 - val_loss: 14194.0312\n",
      "Epoch 371/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5042.4390 - val_loss: 14200.5146\n",
      "Epoch 372/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5040.2007 - val_loss: 14207.4688\n",
      "Epoch 373/500000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 5037.8345 - val_loss: 14214.9004\n",
      "Epoch 374/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5035.5181 - val_loss: 14223.0879\n",
      "Epoch 375/500000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 5033.4634 - val_loss: 14232.1396\n",
      "Epoch 376/500000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5031.7695 - val_loss: 14241.6816\n",
      "Epoch 377/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5030.3628 - val_loss: 14251.0186\n",
      "Epoch 378/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5029.1250 - val_loss: 14259.6602\n",
      "Epoch 379/500000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 5027.9297 - val_loss: 14267.5781\n",
      "Epoch 380/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5026.6948 - val_loss: 14275.0586\n",
      "Epoch 381/500000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 5025.3721 - val_loss: 14282.2549\n",
      "Epoch 382/500000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 5023.9531 - val_loss: 14288.9746\n",
      "Epoch 383/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5022.4541 - val_loss: 14294.8838\n",
      "Epoch 384/500000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 5020.9087 - val_loss: 14299.9082\n",
      "Epoch 385/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5019.3628 - val_loss: 14304.2773\n",
      "Epoch 386/500000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5017.8555 - val_loss: 14308.2334\n",
      "Epoch 387/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 5016.4097 - val_loss: 14311.7002\n",
      "Epoch 388/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5015.0186 - val_loss: 14314.2139\n",
      "Epoch 389/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5013.6562 - val_loss: 14315.3789\n",
      "Epoch 390/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5012.3213 - val_loss: 14315.1963\n",
      "Epoch 391/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5011.0249 - val_loss: 14314.0469\n",
      "Epoch 392/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5009.7788 - val_loss: 14312.3770\n",
      "Epoch 393/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5008.5576 - val_loss: 14310.4238\n",
      "Epoch 394/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5007.3276 - val_loss: 14308.1143\n",
      "Epoch 395/500000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 5006.0557 - val_loss: 14305.3213\n",
      "Epoch 396/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5004.7280 - val_loss: 14301.9756\n",
      "Epoch 397/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5003.3369 - val_loss: 14298.1211\n",
      "Epoch 398/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5001.8706 - val_loss: 14293.7754\n",
      "Epoch 399/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5000.2832 - val_loss: 14288.9443\n",
      "Epoch 400/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 4998.5244 - val_loss: 14283.6680\n",
      "Epoch 401/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 4996.5420 - val_loss: 14278.1914\n",
      "Epoch 402/500000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 4994.3789 - val_loss: 14272.8477\n",
      "Epoch 403/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 4992.2241 - val_loss: 14267.9746\n",
      "Epoch 404/500000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 4990.3315 - val_loss: 14263.7871\n",
      "Epoch 405/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4988.8027 - val_loss: 14260.3457\n",
      "Epoch 406/500000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 4987.5435 - val_loss: 14257.6514\n",
      "Epoch 407/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4986.4185 - val_loss: 14255.7129\n",
      "Epoch 408/500000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 4985.3315 - val_loss: 14254.4756\n",
      "Epoch 409/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4984.2354 - val_loss: 14253.8877\n",
      "Epoch 410/500000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 4983.1172 - val_loss: 14253.7529\n",
      "Epoch 411/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 4981.9780 - val_loss: 14253.9316\n",
      "Epoch 412/500000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 4980.8213 - val_loss: 14254.3701\n",
      "Epoch 413/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4979.6533 - val_loss: 14255.1768\n",
      "Epoch 414/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 4978.4722 - val_loss: 14256.4463\n",
      "Epoch 415/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4977.2764 - val_loss: 14258.2568\n",
      "Epoch 416/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4976.0552 - val_loss: 14260.5898\n",
      "Epoch 417/500000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 4974.8037 - val_loss: 14263.3584\n",
      "Epoch 418/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4973.5107 - val_loss: 14266.4414\n",
      "Epoch 419/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4972.1733 - val_loss: 14269.6279\n",
      "Epoch 420/500000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 4970.7915 - val_loss: 14272.6396\n",
      "Epoch 421/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4969.3818 - val_loss: 14275.1025\n",
      "Epoch 422/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4967.9546 - val_loss: 14276.6875\n",
      "Epoch 423/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4966.5132 - val_loss: 14277.0527\n",
      "Epoch 424/500000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 4965.0298 - val_loss: 14275.9619\n",
      "Epoch 425/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 4963.4419 - val_loss: 14273.1270\n",
      "Epoch 426/500000\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 4961.6323 - val_loss: 14268.2168\n",
      "Epoch 427/500000\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 4959.4185 - val_loss: 14260.9414\n",
      "Epoch 428/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4956.5532 - val_loss: 14251.3877\n",
      "Epoch 429/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 4952.8325 - val_loss: 14240.8691\n",
      "Epoch 430/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 4948.4233 - val_loss: 14232.9727\n",
      "Epoch 431/500000\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 4944.8521 - val_loss: 14230.7500\n",
      "Epoch 432/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4943.4185 - val_loss: 14232.8838\n",
      "Epoch 433/500000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4942.6816 - val_loss: 14237.4287\n",
      "Epoch 434/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 4941.9800 - val_loss: 14243.5527\n",
      "Epoch 435/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4941.2388 - val_loss: 14251.1729\n",
      "Epoch 436/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4940.2471 - val_loss: 14260.4697\n",
      "Epoch 437/500000\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 4938.8672 - val_loss: 14271.6348\n",
      "Epoch 438/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 4937.2344 - val_loss: 14284.6348\n",
      "Epoch 439/500000\n",
      "1/1 [==============================] - ETA: 0s - loss: 4935.66 - 0s 33ms/step - loss: 4935.6685 - val_loss: 14299.2686\n",
      "Epoch 440/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4934.3071 - val_loss: 14315.3486\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "model_mlp = Sequential()\n",
    "model_mlp.add(Dense(55, input_dim=X_train.shape[1], activation='sigmoid',kernel_initializer='random_uniform'))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.5))\n",
    "model_mlp.add(Dense(1, activation='linear'))\n",
    "opt = SGD(learning_rate=0.01,momentum=0.9)\n",
    "model_mlp.compile(optimizer=opt, loss='mse')\n",
    "model_mlp.summary()\n",
    "early_stopping_cb = EarlyStopping(patience=200,restore_best_weights=True)\n",
    "run_logdir=r\"C:\\Users\\Deepak Tripathi\\Desktop\\rossman\\log\"\n",
    "history = model_mlp.fit(np.array(X_train), y_train, batch_size=len(X_train),validation_data=(X_val, y_val) ,epochs=500000, verbose=1,callbacks=[early_stopping_cb])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1271,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE= 111.78662428383754\n",
      "R2= 0.9883113680708007\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "      <th>Parameter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>506.638072</td>\n",
       "      <td>0.782245</td>\n",
       "      <td>{'alpha': 10.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>506.565917</td>\n",
       "      <td>0.782307</td>\n",
       "      <td>{'alpha': 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestRegressor</th>\n",
       "      <td>561.657658</td>\n",
       "      <td>0.732382</td>\n",
       "      <td>{'max_depth': 10, 'max_features': 'sqrt', 'n_e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostingRegressor</th>\n",
       "      <td>410.441842</td>\n",
       "      <td>0.857086</td>\n",
       "      <td>{'max_depth': 4, 'max_features': 'sqrt', 'n_es...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVR</th>\n",
       "      <td>525.606852</td>\n",
       "      <td>0.765634</td>\n",
       "      <td>{'C': 200, 'gamma': 1e-07, 'kernel': 'linear'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP</th>\n",
       "      <td>111.786624</td>\n",
       "      <td>0.988311</td>\n",
       "      <td>{'No of Nuerons': 40, 'acivation': ['sigmoid',...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 RMSE        R2  \\\n",
       "Ridge                      506.638072  0.782245   \n",
       "Lasso                      506.565917  0.782307   \n",
       "RandomForestRegressor      561.657658  0.732382   \n",
       "GradientBoostingRegressor  410.441842  0.857086   \n",
       "SVR                        525.606852  0.765634   \n",
       "MLP                        111.786624  0.988311   \n",
       "\n",
       "                                                                   Parameter  \n",
       "Ridge                                                        {'alpha': 10.0}  \n",
       "Lasso                                                         {'alpha': 1.0}  \n",
       "RandomForestRegressor      {'max_depth': 10, 'max_features': 'sqrt', 'n_e...  \n",
       "GradientBoostingRegressor  {'max_depth': 4, 'max_features': 'sqrt', 'n_es...  \n",
       "SVR                           {'C': 200, 'gamma': 1e-07, 'kernel': 'linear'}  \n",
       "MLP                        {'No of Nuerons': 40, 'acivation': ['sigmoid',...  "
      ]
     },
     "execution_count": 1271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model_mlp.predict(X_val)\n",
    "MSE=mean_squared_error(y_val,pred)\n",
    "RMSE=math.sqrt(MSE)\n",
    "r2=r2_score(y_val,pred)\n",
    "print('RMSE=',RMSE)\n",
    "print('R2=',r2)\n",
    "print('-'*50)\n",
    "\n",
    "model='MLP'\n",
    "algo.loc[model]=[RMSE,r2,{'No of Nuerons':40,'acivation':['sigmoid','linear']}]\n",
    "algo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP is giving extremely good result on valdation datset. Lets check its performence on Test datset as well, which we have not used so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1272,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred=model_mlp(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE= 120.98627755732583\n",
      "R2= 0.9882578997916465\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "      <th>Parameter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>506.638072</td>\n",
       "      <td>0.782245</td>\n",
       "      <td>{'alpha': 10.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>506.565917</td>\n",
       "      <td>0.782307</td>\n",
       "      <td>{'alpha': 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestRegressor</th>\n",
       "      <td>561.657658</td>\n",
       "      <td>0.732382</td>\n",
       "      <td>{'max_depth': 10, 'max_features': 'sqrt', 'n_e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostingRegressor</th>\n",
       "      <td>410.441842</td>\n",
       "      <td>0.857086</td>\n",
       "      <td>{'max_depth': 4, 'max_features': 'sqrt', 'n_es...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVR</th>\n",
       "      <td>525.606852</td>\n",
       "      <td>0.765634</td>\n",
       "      <td>{'C': 200, 'gamma': 1e-07, 'kernel': 'linear'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP</th>\n",
       "      <td>120.986278</td>\n",
       "      <td>0.988258</td>\n",
       "      <td>{'No of Nuerons': 40, 'acivation': ['sigmoid',...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 RMSE        R2  \\\n",
       "Ridge                      506.638072  0.782245   \n",
       "Lasso                      506.565917  0.782307   \n",
       "RandomForestRegressor      561.657658  0.732382   \n",
       "GradientBoostingRegressor  410.441842  0.857086   \n",
       "SVR                        525.606852  0.765634   \n",
       "MLP                        120.986278  0.988258   \n",
       "\n",
       "                                                                   Parameter  \n",
       "Ridge                                                        {'alpha': 10.0}  \n",
       "Lasso                                                         {'alpha': 1.0}  \n",
       "RandomForestRegressor      {'max_depth': 10, 'max_features': 'sqrt', 'n_e...  \n",
       "GradientBoostingRegressor  {'max_depth': 4, 'max_features': 'sqrt', 'n_es...  \n",
       "SVR                           {'C': 200, 'gamma': 1e-07, 'kernel': 'linear'}  \n",
       "MLP                        {'No of Nuerons': 40, 'acivation': ['sigmoid',...  "
      ]
     },
     "execution_count": 1273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE=mean_squared_error(y_test,test_pred)\n",
    "RMSE=math.sqrt(MSE)\n",
    "r2=r2_score(y_test,test_pred)\n",
    "print('RMSE=',RMSE)\n",
    "print('R2=',r2)\n",
    "print('-'*50)\n",
    "\n",
    "model='MLP'\n",
    "algo.loc[model]=[RMSE,r2,{'No of Nuerons':40,'acivation':['sigmoid','linear']}]\n",
    "algo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So MLP is giving good result on Test Dataset as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try using Relu activation function in first layer and see we can improve the performence even further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_58\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_111 (Dense)            (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 13\n",
      "Trainable params: 13\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/500000\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 2101190.0000 - val_loss: 1870551.5000\n",
      "Epoch 2/500000\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2065878.7500 - val_loss: 1767503.7500\n",
      "Epoch 3/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1955502.6250 - val_loss: 1610195.5000\n",
      "Epoch 4/500000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 1786383.1250 - val_loss: 1417160.8750\n",
      "Epoch 5/500000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1577900.1250 - val_loss: 1207098.2500\n",
      "Epoch 6/500000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1349656.8750 - val_loss: 997189.5000\n",
      "Epoch 7/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1119709.8750 - val_loss: 801787.3125\n",
      "Epoch 8/500000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 903222.2500 - val_loss: 631619.1875\n",
      "Epoch 9/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 711638.3125 - val_loss: 493472.5000\n",
      "Epoch 10/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 552353.8125 - val_loss: 390299.1250\n",
      "Epoch 11/500000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 428814.1562 - val_loss: 321646.5000\n",
      "Epoch 12/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 340952.2500 - val_loss: 284321.0938\n",
      "Epoch 13/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 285855.5938 - val_loss: 273171.2188\n",
      "Epoch 14/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 258562.2188 - val_loss: 281896.4688\n",
      "Epoch 15/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 252887.8125 - val_loss: 303808.9375\n",
      "Epoch 16/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 262199.2812 - val_loss: 332478.4062\n",
      "Epoch 17/500000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 280083.3125 - val_loss: 362238.1562\n",
      "Epoch 18/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 300855.8125 - val_loss: 388530.3750\n",
      "Epoch 19/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 319919.4062 - val_loss: 408085.3438\n",
      "Epoch 20/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 333939.2812 - val_loss: 418946.5938\n",
      "Epoch 21/500000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 340880.2500 - val_loss: 420385.2812\n",
      "Epoch 22/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 339917.3750 - val_loss: 412684.9688\n",
      "Epoch 23/500000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 331251.7500 - val_loss: 396923.6875\n",
      "Epoch 24/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 315883.7500 - val_loss: 374770.5000\n",
      "Epoch 25/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 295345.2500 - val_loss: 348176.2500\n",
      "Epoch 26/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 271445.2500 - val_loss: 319129.0000\n",
      "Epoch 27/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 246043.4844 - val_loss: 289477.3438\n",
      "Epoch 28/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 220840.4688 - val_loss: 260829.3125\n",
      "Epoch 29/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 197235.7656 - val_loss: 234418.6094\n",
      "Epoch 30/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 176258.4219 - val_loss: 211155.3125\n",
      "Epoch 31/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 158620.6250 - val_loss: 191464.6406\n",
      "Epoch 32/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 144680.0781 - val_loss: 175468.2188\n",
      "Epoch 33/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 134420.5000 - val_loss: 163135.2344\n",
      "Epoch 34/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 127519.1641 - val_loss: 154001.1250\n",
      "Epoch 35/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 123532.2266 - val_loss: 147627.7500\n",
      "Epoch 36/500000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 121867.5000 - val_loss: 143337.6250\n",
      "Epoch 37/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 121883.1641 - val_loss: 140633.4219\n",
      "Epoch 38/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 122937.5781 - val_loss: 139005.7188\n",
      "Epoch 39/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 124420.4688 - val_loss: 137970.3125\n",
      "Epoch 40/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 125873.3438 - val_loss: 137134.9531\n",
      "Epoch 41/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 126965.5078 - val_loss: 136273.2188\n",
      "Epoch 42/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 127477.3438 - val_loss: 135182.1719\n",
      "Epoch 43/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 127271.3281 - val_loss: 133810.2500\n",
      "Epoch 44/500000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 126326.7500 - val_loss: 132161.5312\n",
      "Epoch 45/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 124726.1094 - val_loss: 130287.8906\n",
      "Epoch 46/500000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 122578.9141 - val_loss: 128280.0391\n",
      "Epoch 47/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 120038.8359 - val_loss: 126245.3828\n",
      "Epoch 48/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 117267.6016 - val_loss: 124289.2891\n",
      "Epoch 49/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 114426.0469 - val_loss: 122511.8672\n",
      "Epoch 50/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 111657.3828 - val_loss: 120987.5391\n",
      "Epoch 51/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 109082.7812 - val_loss: 119763.4141\n",
      "Epoch 52/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 106785.6953 - val_loss: 118862.7578\n",
      "Epoch 53/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 104825.7188 - val_loss: 118276.6328\n",
      "Epoch 54/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 103220.6250 - val_loss: 117979.2188\n",
      "Epoch 55/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 101974.0391 - val_loss: 117925.9609\n",
      "Epoch 56/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 101056.7344 - val_loss: 118057.6328\n",
      "Epoch 57/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 100423.5703 - val_loss: 118309.4766\n",
      "Epoch 58/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 100019.9922 - val_loss: 118616.1094\n",
      "Epoch 59/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 99786.5547 - val_loss: 118917.4688\n",
      "Epoch 60/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 99665.1875 - val_loss: 119159.6172\n",
      "Epoch 61/500000\n",
      "1/1 [==============================] - ETA: 0s - loss: 99603.609 - 0s 40ms/step - loss: 99603.6094 - val_loss: 119304.3047\n",
      "Epoch 62/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 99555.3828 - val_loss: 119323.7344\n",
      "Epoch 63/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 99485.6953 - val_loss: 119201.0312\n",
      "Epoch 64/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 99372.2031 - val_loss: 118932.4609\n",
      "Epoch 65/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 99202.9922 - val_loss: 118523.5312\n",
      "Epoch 66/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 98975.5781 - val_loss: 117984.5938\n",
      "Epoch 67/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 98695.2344 - val_loss: 117340.2422\n",
      "Epoch 68/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 98371.0859 - val_loss: 116613.1797\n",
      "Epoch 69/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 98006.8672 - val_loss: 115830.7266\n",
      "Epoch 70/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 97627.1094 - val_loss: 115018.8906\n",
      "Epoch 71/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 97246.2031 - val_loss: 114202.6719\n",
      "Epoch 72/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 96881.1250 - val_loss: 113403.6953\n",
      "Epoch 73/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 96539.3594 - val_loss: 112640.0469\n",
      "Epoch 74/500000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 96231.7422 - val_loss: 111924.4141\n",
      "Epoch 75/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 95955.1016 - val_loss: 111261.1641\n",
      "Epoch 76/500000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 95712.8906 - val_loss: 110654.1797\n",
      "Epoch 77/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 95503.7969 - val_loss: 110108.9297\n",
      "Epoch 78/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 95325.5859 - val_loss: 109625.6406\n",
      "Epoch 79/500000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 95180.5625 - val_loss: 109202.2109\n",
      "Epoch 80/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 95063.1719 - val_loss: 108834.8906\n",
      "Epoch 81/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 94967.1953 - val_loss: 108518.9688\n",
      "Epoch 82/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 94886.3594 - val_loss: 108249.3516\n",
      "Epoch 83/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 94809.2656 - val_loss: 108021.1641\n",
      "Epoch 84/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 94729.6953 - val_loss: 107823.7031\n",
      "Epoch 85/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 94650.1562 - val_loss: 107654.9844\n",
      "Epoch 86/500000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 94566.1406 - val_loss: 107514.3359\n",
      "Epoch 87/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 94476.9922 - val_loss: 107398.2422\n",
      "Epoch 88/500000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 94384.0156 - val_loss: 107304.1797\n",
      "Epoch 89/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 94286.0547 - val_loss: 107229.0234\n",
      "Epoch 90/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 94185.5859 - val_loss: 107170.7109\n",
      "Epoch 91/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 94083.8984 - val_loss: 107127.7812\n",
      "Epoch 92/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 93982.2500 - val_loss: 107098.5078\n",
      "Epoch 93/500000\n",
      "1/1 [==============================] - ETA: 0s - loss: 93881.960 - 0s 32ms/step - loss: 93881.9609 - val_loss: 107080.9219\n",
      "Epoch 94/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 93784.2656 - val_loss: 107073.2734\n",
      "Epoch 95/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 93690.2578 - val_loss: 107073.7812\n",
      "Epoch 96/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 93600.7578 - val_loss: 107080.5859\n",
      "Epoch 97/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 93516.3203 - val_loss: 107091.7578\n",
      "Epoch 98/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 93437.2578 - val_loss: 107105.3984\n",
      "Epoch 99/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 93363.6094 - val_loss: 107117.4609\n",
      "Epoch 100/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 93295.1875 - val_loss: 107127.1172\n",
      "Epoch 101/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 93231.6484 - val_loss: 107134.0781\n",
      "Epoch 102/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 93172.5391 - val_loss: 107136.9688\n",
      "Epoch 103/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 93117.3516 - val_loss: 107134.7109\n",
      "Epoch 104/500000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 93065.5078 - val_loss: 107126.4531\n",
      "Epoch 105/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 93016.5156 - val_loss: 107111.6562\n",
      "Epoch 106/500000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 92969.9453 - val_loss: 107090.0547\n",
      "Epoch 107/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 92925.3828 - val_loss: 107061.6641\n",
      "Epoch 108/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 92882.5312 - val_loss: 107026.7266\n",
      "Epoch 109/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 92840.2344 - val_loss: 106986.1250\n",
      "Epoch 110/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 92797.8672 - val_loss: 106940.4688\n",
      "Epoch 111/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 92756.4609 - val_loss: 106890.4688\n",
      "Epoch 112/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 92716.1484 - val_loss: 106836.3906\n",
      "Epoch 113/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 92676.9688 - val_loss: 106779.7500\n",
      "Epoch 114/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 92639.0703 - val_loss: 106721.4844\n",
      "Epoch 115/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 92602.6094 - val_loss: 106662.5000\n",
      "Epoch 116/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 92567.6562 - val_loss: 106603.6250\n",
      "Epoch 117/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 92534.2500 - val_loss: 106545.2969\n",
      "Epoch 118/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 92502.1172 - val_loss: 106484.7188\n",
      "Epoch 119/500000\n",
      "1/1 [==============================] - ETA: 0s - loss: 92471.484 - 0s 40ms/step - loss: 92471.4844 - val_loss: 106423.0078\n",
      "Epoch 120/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 92440.8203 - val_loss: 106362.2578\n",
      "Epoch 121/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 92411.6250 - val_loss: 106303.1484\n",
      "Epoch 122/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 92383.9141 - val_loss: 106246.2422\n",
      "Epoch 123/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 92357.4766 - val_loss: 106190.7109\n",
      "Epoch 124/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 92330.9609 - val_loss: 106137.0391\n",
      "Epoch 125/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 92305.6484 - val_loss: 106085.6250\n",
      "Epoch 126/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 92281.4141 - val_loss: 106033.6797\n",
      "Epoch 127/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 92257.1094 - val_loss: 105983.6406\n",
      "Epoch 128/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 92233.0078 - val_loss: 105932.9922\n",
      "Epoch 129/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 92208.4844 - val_loss: 105885.4141\n",
      "Epoch 130/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 92184.3984 - val_loss: 105841.0859\n",
      "Epoch 131/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 92160.9531 - val_loss: 105800.0547\n",
      "Epoch 132/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 92138.2031 - val_loss: 105762.2500\n",
      "Epoch 133/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 92116.1641 - val_loss: 105727.5859\n",
      "Epoch 134/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 92093.6328 - val_loss: 105695.9766\n",
      "Epoch 135/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 92071.5859 - val_loss: 105667.2031\n",
      "Epoch 136/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 92050.1875 - val_loss: 105641.1250\n",
      "Epoch 137/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 92029.4609 - val_loss: 105617.5391\n",
      "Epoch 138/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 92009.4219 - val_loss: 105596.1953\n",
      "Epoch 139/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91989.1875 - val_loss: 105576.3438\n",
      "Epoch 140/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91968.2422 - val_loss: 105557.8594\n",
      "Epoch 141/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91947.7969 - val_loss: 105540.5391\n",
      "Epoch 142/500000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step - loss: 91927.8438 - val_loss: 105524.3203\n",
      "Epoch 143/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91908.4375 - val_loss: 105508.6016\n",
      "Epoch 144/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91889.5391 - val_loss: 105490.6953\n",
      "Epoch 145/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91871.1719 - val_loss: 105473.6719\n",
      "Epoch 146/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91853.3359 - val_loss: 105457.4453\n",
      "Epoch 147/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91836.0469 - val_loss: 105441.9609\n",
      "Epoch 148/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91819.2656 - val_loss: 105426.8359\n",
      "Epoch 149/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91803.0312 - val_loss: 105409.8828\n",
      "Epoch 150/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91787.3047 - val_loss: 105393.6250\n",
      "Epoch 151/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91772.1094 - val_loss: 105378.0156\n",
      "Epoch 152/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91757.4141 - val_loss: 105363.0234\n",
      "Epoch 153/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91743.2266 - val_loss: 105348.5781\n",
      "Epoch 154/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91729.5234 - val_loss: 105334.7266\n",
      "Epoch 155/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91716.3125 - val_loss: 105321.3672\n",
      "Epoch 156/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91703.5859 - val_loss: 105308.5391\n",
      "Epoch 157/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91691.3359 - val_loss: 105296.1719\n",
      "Epoch 158/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91679.5391 - val_loss: 105284.2734\n",
      "Epoch 159/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91668.1875 - val_loss: 105272.7969\n",
      "Epoch 160/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91657.2656 - val_loss: 105261.7422\n",
      "Epoch 161/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91646.7891 - val_loss: 105251.0547\n",
      "Epoch 162/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91636.7266 - val_loss: 105240.7578\n",
      "Epoch 163/500000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 91627.0625 - val_loss: 105230.6875\n",
      "Epoch 164/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91617.0703 - val_loss: 105217.7656\n",
      "Epoch 165/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91607.2422 - val_loss: 105204.0391\n",
      "Epoch 166/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91597.7031 - val_loss: 105189.6016\n",
      "Epoch 167/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91588.4453 - val_loss: 105174.6484\n",
      "Epoch 168/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91579.4688 - val_loss: 105159.2891\n",
      "Epoch 169/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91570.7891 - val_loss: 105143.6875\n",
      "Epoch 170/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91562.4062 - val_loss: 105127.9844\n",
      "Epoch 171/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91554.3438 - val_loss: 105112.2969\n",
      "Epoch 172/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91546.5781 - val_loss: 105096.7344\n",
      "Epoch 173/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91538.8438 - val_loss: 105080.5312\n",
      "Epoch 174/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91530.6641 - val_loss: 105063.8594\n",
      "Epoch 175/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91522.6641 - val_loss: 105046.9609\n",
      "Epoch 176/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91514.8828 - val_loss: 105029.9844\n",
      "Epoch 177/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91507.3281 - val_loss: 105013.1250\n",
      "Epoch 178/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91500.0391 - val_loss: 104996.5703\n",
      "Epoch 179/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91492.7969 - val_loss: 104978.5391\n",
      "Epoch 180/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91485.3203 - val_loss: 104959.4219\n",
      "Epoch 181/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 91477.9844 - val_loss: 104939.4922\n",
      "Epoch 182/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91470.8047 - val_loss: 104919.0938\n",
      "Epoch 183/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91463.8203 - val_loss: 104898.5000\n",
      "Epoch 184/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91457.0234 - val_loss: 104877.9531\n",
      "Epoch 185/500000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 91450.4141 - val_loss: 104857.6953\n",
      "Epoch 186/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91444.0547 - val_loss: 104837.9453\n",
      "Epoch 187/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91437.8984 - val_loss: 104818.8359\n",
      "Epoch 188/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91431.9531 - val_loss: 104800.5391\n",
      "Epoch 189/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91426.2656 - val_loss: 104783.1484\n",
      "Epoch 190/500000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 91420.7656 - val_loss: 104766.7656\n",
      "Epoch 191/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91415.5000 - val_loss: 104751.4219\n",
      "Epoch 192/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91410.4219 - val_loss: 104737.2188\n",
      "Epoch 193/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91405.5547 - val_loss: 104724.1016\n",
      "Epoch 194/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91400.8828 - val_loss: 104712.1250\n",
      "Epoch 195/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91396.4062 - val_loss: 104701.2266\n",
      "Epoch 196/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91392.0781 - val_loss: 104691.4141\n",
      "Epoch 197/500000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 91387.9453 - val_loss: 104682.6328\n",
      "Epoch 198/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91383.3125 - val_loss: 104674.8984\n",
      "Epoch 199/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91378.7109 - val_loss: 104668.1953\n",
      "Epoch 200/500000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 91374.1484 - val_loss: 104662.4453\n",
      "Epoch 201/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91369.6797 - val_loss: 104657.5938\n",
      "Epoch 202/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91365.2969 - val_loss: 104653.5547\n",
      "Epoch 203/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91360.9922 - val_loss: 104650.3047\n",
      "Epoch 204/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91356.7969 - val_loss: 104647.7812\n",
      "Epoch 205/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91352.7422 - val_loss: 104645.8750\n",
      "Epoch 206/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91348.8047 - val_loss: 104644.6016\n",
      "Epoch 207/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91345.0078 - val_loss: 104643.0703\n",
      "Epoch 208/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91341.3359 - val_loss: 104641.6953\n",
      "Epoch 209/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91337.7969 - val_loss: 104640.7656\n",
      "Epoch 210/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91334.4141 - val_loss: 104639.8984\n",
      "Epoch 211/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91331.1562 - val_loss: 104638.7578\n",
      "Epoch 212/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91328.0469 - val_loss: 104637.9297\n",
      "Epoch 213/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91325.0469 - val_loss: 104637.3125\n",
      "Epoch 214/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91322.1562 - val_loss: 104636.9062\n",
      "Epoch 215/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91319.4219 - val_loss: 104636.6484\n",
      "Epoch 216/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91316.7812 - val_loss: 104636.4688\n",
      "Epoch 217/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91314.2500 - val_loss: 104636.3594\n",
      "Epoch 218/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91311.8281 - val_loss: 104636.2031\n",
      "Epoch 219/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91309.4844 - val_loss: 104636.0469\n",
      "Epoch 220/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91307.2422 - val_loss: 104635.2656\n",
      "Epoch 221/500000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 91305.0938 - val_loss: 104633.9531\n",
      "Epoch 222/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91303.0156 - val_loss: 104632.5234\n",
      "Epoch 223/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91301.0391 - val_loss: 104630.9922\n",
      "Epoch 224/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91299.1250 - val_loss: 104629.3125\n",
      "Epoch 225/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91297.2734 - val_loss: 104627.4766\n",
      "Epoch 226/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91295.5078 - val_loss: 104625.4609\n",
      "Epoch 227/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91293.8047 - val_loss: 104623.2734\n",
      "Epoch 228/500000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 91292.1562 - val_loss: 104620.9297\n",
      "Epoch 229/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91290.5859 - val_loss: 104618.3906\n",
      "Epoch 230/500000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 91288.8125 - val_loss: 104614.6875\n",
      "Epoch 231/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91286.9688 - val_loss: 104609.8750\n",
      "Epoch 232/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91285.1328 - val_loss: 104604.1406\n",
      "Epoch 233/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91283.3047 - val_loss: 104597.5625\n",
      "Epoch 234/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91281.5156 - val_loss: 104590.3359\n",
      "Epoch 235/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91279.7344 - val_loss: 104582.5703\n",
      "Epoch 236/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91278.0078 - val_loss: 104574.3906\n",
      "Epoch 237/500000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 91276.3281 - val_loss: 104565.9219\n",
      "Epoch 238/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91274.6719 - val_loss: 104557.2656\n",
      "Epoch 239/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91273.0703 - val_loss: 104548.5547\n",
      "Epoch 240/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91271.5391 - val_loss: 104539.8516\n",
      "Epoch 241/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91270.0469 - val_loss: 104531.2500\n",
      "Epoch 242/500000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 91268.6172 - val_loss: 104522.8359\n",
      "Epoch 243/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91267.2266 - val_loss: 104514.6641\n",
      "Epoch 244/500000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 91265.8984 - val_loss: 104506.8281\n",
      "Epoch 245/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91264.6328 - val_loss: 104499.2969\n",
      "Epoch 246/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91263.4062 - val_loss: 104492.1484\n",
      "Epoch 247/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91262.2266 - val_loss: 104485.4062\n",
      "Epoch 248/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91261.0938 - val_loss: 104479.0938\n",
      "Epoch 249/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91260.0156 - val_loss: 104473.2109\n",
      "Epoch 250/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91258.9688 - val_loss: 104467.7344\n",
      "Epoch 251/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91257.9844 - val_loss: 104462.7031\n",
      "Epoch 252/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91257.0156 - val_loss: 104458.0703\n",
      "Epoch 253/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91256.0938 - val_loss: 104453.8516\n",
      "Epoch 254/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91255.1953 - val_loss: 104450.0156\n",
      "Epoch 255/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91254.3359 - val_loss: 104446.5391\n",
      "Epoch 256/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91253.5078 - val_loss: 104443.4062\n",
      "Epoch 257/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91252.7109 - val_loss: 104440.5703\n",
      "Epoch 258/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91251.9453 - val_loss: 104438.0469\n",
      "Epoch 259/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91251.2109 - val_loss: 104435.7656\n",
      "Epoch 260/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91250.5000 - val_loss: 104433.7266\n",
      "Epoch 261/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91249.8203 - val_loss: 104431.9141\n",
      "Epoch 262/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91249.1641 - val_loss: 104430.2578\n",
      "Epoch 263/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91248.5234 - val_loss: 104428.7812\n",
      "Epoch 264/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91247.9219 - val_loss: 104427.4375\n",
      "Epoch 265/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91247.3359 - val_loss: 104426.2109\n",
      "Epoch 266/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91246.7734 - val_loss: 104425.0781\n",
      "Epoch 267/500000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 91246.2266 - val_loss: 104424.0391\n",
      "Epoch 268/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91245.7109 - val_loss: 104423.0703\n",
      "Epoch 269/500000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 91245.2266 - val_loss: 104422.1250\n",
      "Epoch 270/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91244.7344 - val_loss: 104421.2422\n",
      "Epoch 271/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91244.2891 - val_loss: 104420.3984\n",
      "Epoch 272/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91243.8438 - val_loss: 104419.5547\n",
      "Epoch 273/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91243.4297 - val_loss: 104418.7266\n",
      "Epoch 274/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91243.0234 - val_loss: 104417.9297\n",
      "Epoch 275/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91242.6250 - val_loss: 104417.1172\n",
      "Epoch 276/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91242.2656 - val_loss: 104416.3203\n",
      "Epoch 277/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91241.8984 - val_loss: 104415.5312\n",
      "Epoch 278/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91241.5625 - val_loss: 104414.7031\n",
      "Epoch 279/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91241.2266 - val_loss: 104413.9219\n",
      "Epoch 280/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91240.9062 - val_loss: 104413.1094\n",
      "Epoch 281/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91240.6094 - val_loss: 104412.2812\n",
      "Epoch 282/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91240.3281 - val_loss: 104411.4766\n",
      "Epoch 283/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91240.0391 - val_loss: 104410.6641\n",
      "Epoch 284/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91239.7734 - val_loss: 104409.8516\n",
      "Epoch 285/500000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 91239.5312 - val_loss: 104409.0312\n",
      "Epoch 286/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91239.2812 - val_loss: 104408.2422\n",
      "Epoch 287/500000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 91239.0547 - val_loss: 104407.4297\n",
      "Epoch 288/500000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step - loss: 91238.8203 - val_loss: 104406.5625\n",
      "Epoch 289/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91238.6016 - val_loss: 104405.4922\n",
      "Epoch 290/500000\n",
      "1/1 [==============================] - ETA: 0s - loss: 91238.398 - 0s 32ms/step - loss: 91238.3984 - val_loss: 104404.4297\n",
      "Epoch 291/500000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 91238.2031 - val_loss: 104403.3750\n",
      "Epoch 292/500000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 91238.0078 - val_loss: 104402.3438\n",
      "Epoch 293/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91237.8203 - val_loss: 104401.3281\n",
      "Epoch 294/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91237.6562 - val_loss: 104400.3203\n",
      "Epoch 295/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91237.4766 - val_loss: 104399.3281\n",
      "Epoch 296/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91237.3359 - val_loss: 104398.3281\n",
      "Epoch 297/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91237.1641 - val_loss: 104397.3672\n",
      "Epoch 298/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91237.0234 - val_loss: 104396.4062\n",
      "Epoch 299/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91236.8828 - val_loss: 104395.4844\n",
      "Epoch 300/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91236.7422 - val_loss: 104394.5391\n",
      "Epoch 301/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91236.6094 - val_loss: 104393.6328\n",
      "Epoch 302/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91236.4766 - val_loss: 104392.7188\n",
      "Epoch 303/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91236.3750 - val_loss: 104391.8438\n",
      "Epoch 304/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91236.2578 - val_loss: 104390.9531\n",
      "Epoch 305/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91236.1484 - val_loss: 104390.0859\n",
      "Epoch 306/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91236.0391 - val_loss: 104389.2266\n",
      "Epoch 307/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91235.9297 - val_loss: 104388.3984\n",
      "Epoch 308/500000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 91235.8438 - val_loss: 104387.5547\n",
      "Epoch 309/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91235.7422 - val_loss: 104386.7266\n",
      "Epoch 310/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91235.6562 - val_loss: 104385.9375\n",
      "Epoch 311/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91235.5703 - val_loss: 104385.1406\n",
      "Epoch 312/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91235.4922 - val_loss: 104384.3672\n",
      "Epoch 313/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91235.4141 - val_loss: 104383.5938\n",
      "Epoch 314/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91235.3359 - val_loss: 104382.8438\n",
      "Epoch 315/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91235.2578 - val_loss: 104382.1016\n",
      "Epoch 316/500000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 91235.1953 - val_loss: 104381.3984\n",
      "Epoch 317/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91235.1328 - val_loss: 104380.6953\n",
      "Epoch 318/500000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 91235.0703 - val_loss: 104380.0078\n",
      "Epoch 319/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91235.0156 - val_loss: 104379.3359\n",
      "Epoch 320/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91234.9453 - val_loss: 104378.6484\n",
      "Epoch 321/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91234.8906 - val_loss: 104378.0391\n",
      "Epoch 322/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91234.8359 - val_loss: 104377.4141\n",
      "Epoch 323/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91234.7891 - val_loss: 104376.8125\n",
      "Epoch 324/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91234.7344 - val_loss: 104376.2500\n",
      "Epoch 325/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91234.6875 - val_loss: 104375.6641\n",
      "Epoch 326/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91234.6406 - val_loss: 104375.1094\n",
      "Epoch 327/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 91234.6094 - val_loss: 104374.5859\n",
      "Epoch 328/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91234.5703 - val_loss: 104374.0703\n",
      "Epoch 329/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91234.5234 - val_loss: 104373.5859\n",
      "Epoch 330/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91234.4844 - val_loss: 104373.0938\n",
      "Epoch 331/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91234.4453 - val_loss: 104372.6328\n",
      "Epoch 332/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91234.4141 - val_loss: 104372.1875\n",
      "Epoch 333/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91234.3828 - val_loss: 104371.7500\n",
      "Epoch 334/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91234.3438 - val_loss: 104371.3438\n",
      "Epoch 335/500000\n",
      "1/1 [==============================] - ETA: 0s - loss: 91234.320 - 0s 32ms/step - loss: 91234.3203 - val_loss: 104370.9219\n",
      "Epoch 336/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91234.2891 - val_loss: 104370.5547\n",
      "Epoch 337/500000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 91234.2578 - val_loss: 104370.1797\n",
      "Epoch 338/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91234.2266 - val_loss: 104369.8125\n",
      "Epoch 339/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91234.2109 - val_loss: 104369.4453\n",
      "Epoch 340/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91234.1797 - val_loss: 104369.1016\n",
      "Epoch 341/500000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 91234.1484 - val_loss: 104368.7656\n",
      "Epoch 342/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91234.1328 - val_loss: 104368.4453\n",
      "Epoch 343/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91234.1172 - val_loss: 104368.1250\n",
      "Epoch 344/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91234.0781 - val_loss: 104367.8438\n",
      "Epoch 345/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91234.0781 - val_loss: 104367.5391\n",
      "Epoch 346/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91234.0547 - val_loss: 104367.2500\n",
      "Epoch 347/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91234.0312 - val_loss: 104366.9531\n",
      "Epoch 348/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91234.0156 - val_loss: 104366.6953\n",
      "Epoch 349/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91234.0078 - val_loss: 104366.4062\n",
      "Epoch 350/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.9844 - val_loss: 104366.1406\n",
      "Epoch 351/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.9688 - val_loss: 104365.8906\n",
      "Epoch 352/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.9531 - val_loss: 104365.6250\n",
      "Epoch 353/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.9375 - val_loss: 104365.3828\n",
      "Epoch 354/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.9219 - val_loss: 104365.1484\n",
      "Epoch 355/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.9141 - val_loss: 104364.8984\n",
      "Epoch 356/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.8906 - val_loss: 104364.6484\n",
      "Epoch 357/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.8828 - val_loss: 104364.4375\n",
      "Epoch 358/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.8828 - val_loss: 104364.2031\n",
      "Epoch 359/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.8594 - val_loss: 104363.9609\n",
      "Epoch 360/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.8359 - val_loss: 104363.7578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 361/500000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 91233.8438 - val_loss: 104363.5234\n",
      "Epoch 362/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.8281 - val_loss: 104363.3125\n",
      "Epoch 363/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.8203 - val_loss: 104363.1016\n",
      "Epoch 364/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.8125 - val_loss: 104362.8984\n",
      "Epoch 365/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.8047 - val_loss: 104362.6875\n",
      "Epoch 366/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.7891 - val_loss: 104362.4922\n",
      "Epoch 367/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.7891 - val_loss: 104362.2969\n",
      "Epoch 368/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.7656 - val_loss: 104362.1016\n",
      "Epoch 369/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.7812 - val_loss: 104361.9141\n",
      "Epoch 370/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.7656 - val_loss: 104361.7266\n",
      "Epoch 371/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.7578 - val_loss: 104361.5469\n",
      "Epoch 372/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.7500 - val_loss: 104361.3672\n",
      "Epoch 373/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.7422 - val_loss: 104361.1875\n",
      "Epoch 374/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.7422 - val_loss: 104361.0156\n",
      "Epoch 375/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.7344 - val_loss: 104360.8438\n",
      "Epoch 376/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.7266 - val_loss: 104360.6875\n",
      "Epoch 377/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.7188 - val_loss: 104360.5234\n",
      "Epoch 378/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.7109 - val_loss: 104360.3438\n",
      "Epoch 379/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.7109 - val_loss: 104360.2031\n",
      "Epoch 380/500000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 91233.7031 - val_loss: 104360.0625\n",
      "Epoch 381/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.7031 - val_loss: 104359.8984\n",
      "Epoch 382/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6953 - val_loss: 104359.7656\n",
      "Epoch 383/500000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 91233.6953 - val_loss: 104359.6094\n",
      "Epoch 384/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6953 - val_loss: 104359.4766\n",
      "Epoch 385/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6875 - val_loss: 104359.3516\n",
      "Epoch 386/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6875 - val_loss: 104359.2109\n",
      "Epoch 387/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6797 - val_loss: 104359.0859\n",
      "Epoch 388/500000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 91233.6797 - val_loss: 104358.9531\n",
      "Epoch 389/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6719 - val_loss: 104358.8516\n",
      "Epoch 390/500000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 91233.6797 - val_loss: 104358.7109\n",
      "Epoch 391/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6719 - val_loss: 104358.6016\n",
      "Epoch 392/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6641 - val_loss: 104358.4922\n",
      "Epoch 393/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 91233.6641 - val_loss: 104358.3750\n",
      "Epoch 394/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 91233.6562 - val_loss: 104358.2656\n",
      "Epoch 395/500000\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 91233.6562 - val_loss: 104358.1562\n",
      "Epoch 396/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6484 - val_loss: 104358.0625\n",
      "Epoch 397/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6562 - val_loss: 104357.9453\n",
      "Epoch 398/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6484 - val_loss: 104357.8594\n",
      "Epoch 399/500000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 91233.6641 - val_loss: 104357.7656\n",
      "Epoch 400/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6562 - val_loss: 104357.6641\n",
      "Epoch 401/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6484 - val_loss: 104357.5859\n",
      "Epoch 402/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6562 - val_loss: 104357.4844\n",
      "Epoch 403/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6562 - val_loss: 104357.3984\n",
      "Epoch 404/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6406 - val_loss: 104357.3047\n",
      "Epoch 405/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6484 - val_loss: 104357.2188\n",
      "Epoch 406/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6406 - val_loss: 104357.1406\n",
      "Epoch 407/500000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 91233.6484 - val_loss: 104357.0703\n",
      "Epoch 408/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6484 - val_loss: 104356.9766\n",
      "Epoch 409/500000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 91233.6328 - val_loss: 104356.8906\n",
      "Epoch 410/500000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 91233.6328 - val_loss: 104356.8281\n",
      "Epoch 411/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6406 - val_loss: 104356.7500\n",
      "Epoch 412/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6406 - val_loss: 104356.6719\n",
      "Epoch 413/500000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 91233.6328 - val_loss: 104356.6016\n",
      "Epoch 414/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6328 - val_loss: 104356.5234\n",
      "Epoch 415/500000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 91233.6328 - val_loss: 104356.4688\n",
      "Epoch 416/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6250 - val_loss: 104356.4062\n",
      "Epoch 417/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6328 - val_loss: 104356.3359\n",
      "Epoch 418/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6328 - val_loss: 104356.2656\n",
      "Epoch 419/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6172 - val_loss: 104356.1953\n",
      "Epoch 420/500000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 91233.6172 - val_loss: 104356.1484\n",
      "Epoch 421/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6250 - val_loss: 104356.1016\n",
      "Epoch 422/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6328 - val_loss: 104356.0391\n",
      "Epoch 423/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6250 - val_loss: 104355.9844\n",
      "Epoch 424/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6328 - val_loss: 104355.9297\n",
      "Epoch 425/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6250 - val_loss: 104355.8750\n",
      "Epoch 426/500000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 91233.6328 - val_loss: 104355.8047\n",
      "Epoch 427/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6250 - val_loss: 104355.7656\n",
      "Epoch 428/500000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 91233.6250 - val_loss: 104355.7031\n",
      "Epoch 429/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6250 - val_loss: 104355.6641\n",
      "Epoch 430/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6172 - val_loss: 104355.6094\n",
      "Epoch 431/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6172 - val_loss: 104355.5547\n",
      "Epoch 432/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6250 - val_loss: 104355.5234\n",
      "Epoch 433/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6250 - val_loss: 104355.4609\n",
      "Epoch 434/500000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104355.4141\n",
      "Epoch 435/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104355.3750\n",
      "Epoch 436/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6172 - val_loss: 104355.3516\n",
      "Epoch 437/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6172 - val_loss: 104355.2969\n",
      "Epoch 438/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6172 - val_loss: 104355.2500\n",
      "Epoch 439/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6094 - val_loss: 104355.2109\n",
      "Epoch 440/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6172 - val_loss: 104355.1641\n",
      "Epoch 441/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104355.1094\n",
      "Epoch 442/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104355.0859\n",
      "Epoch 443/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104355.0391\n",
      "Epoch 444/500000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 91233.6094 - val_loss: 104355.0078\n",
      "Epoch 445/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6172 - val_loss: 104354.9766\n",
      "Epoch 446/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6172 - val_loss: 104354.9531\n",
      "Epoch 447/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6172 - val_loss: 104354.8984\n",
      "Epoch 448/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104354.8828\n",
      "Epoch 449/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104354.8359\n",
      "Epoch 450/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6094 - val_loss: 104354.8125\n",
      "Epoch 451/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6172 - val_loss: 104354.7891\n",
      "Epoch 452/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6094 - val_loss: 104354.7578\n",
      "Epoch 453/500000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 91233.6094 - val_loss: 104354.7188\n",
      "Epoch 454/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6016 - val_loss: 104354.6875\n",
      "Epoch 455/500000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 91233.6172 - val_loss: 104354.6641\n",
      "Epoch 456/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6172 - val_loss: 104354.6328\n",
      "Epoch 457/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6172 - val_loss: 104354.5938\n",
      "Epoch 458/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6016 - val_loss: 104354.5859\n",
      "Epoch 459/500000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 91233.6172 - val_loss: 104354.5625\n",
      "Epoch 460/500000\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 91233.6016 - val_loss: 104354.5312\n",
      "Epoch 461/500000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 91233.6094 - val_loss: 104354.5078\n",
      "Epoch 462/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6172 - val_loss: 104354.4688\n",
      "Epoch 463/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104354.4531\n",
      "Epoch 464/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104354.4141\n",
      "Epoch 465/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104354.3906\n",
      "Epoch 466/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6172 - val_loss: 104354.3828\n",
      "Epoch 467/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6094 - val_loss: 104354.3359\n",
      "Epoch 468/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6016 - val_loss: 104354.3281\n",
      "Epoch 469/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6172 - val_loss: 104354.3047\n",
      "Epoch 470/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6172 - val_loss: 104354.2734\n",
      "Epoch 471/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6250 - val_loss: 104354.2656\n",
      "Epoch 472/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104354.2578\n",
      "Epoch 473/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6094 - val_loss: 104354.2344\n",
      "Epoch 474/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104354.1953\n",
      "Epoch 475/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6094 - val_loss: 104354.1953\n",
      "Epoch 476/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 91233.6094 - val_loss: 104354.1797\n",
      "Epoch 477/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104354.1328\n",
      "Epoch 478/500000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 91233.6016 - val_loss: 104354.1328\n",
      "Epoch 479/500000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 91233.6094 - val_loss: 104354.1094\n",
      "Epoch 480/500000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 91233.6016 - val_loss: 104354.1016\n",
      "Epoch 481/500000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 91233.6094 - val_loss: 104354.0859\n",
      "Epoch 482/500000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 91233.6094 - val_loss: 104354.0547\n",
      "Epoch 483/500000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 91233.6016 - val_loss: 104354.0469\n",
      "Epoch 484/500000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 91233.6094 - val_loss: 104354.0391\n",
      "Epoch 485/500000\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 91233.6094 - val_loss: 104354.0156\n",
      "Epoch 486/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6172 - val_loss: 104354.0000\n",
      "Epoch 487/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.9844\n",
      "Epoch 488/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.9844\n",
      "Epoch 489/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6016 - val_loss: 104353.9609\n",
      "Epoch 490/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6016 - val_loss: 104353.9609\n",
      "Epoch 491/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.9297\n",
      "Epoch 492/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6172 - val_loss: 104353.9141\n",
      "Epoch 493/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6016 - val_loss: 104353.8984\n",
      "Epoch 494/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.8984\n",
      "Epoch 495/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.8828\n",
      "Epoch 496/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.8672\n",
      "Epoch 497/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6016 - val_loss: 104353.8516\n",
      "Epoch 498/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6016 - val_loss: 104353.8438\n",
      "Epoch 499/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6172 - val_loss: 104353.8203\n",
      "Epoch 500/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6016 - val_loss: 104353.8203\n",
      "Epoch 501/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6172 - val_loss: 104353.7969\n",
      "Epoch 502/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.8047\n",
      "Epoch 503/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.5938 - val_loss: 104353.7891\n",
      "Epoch 504/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6172 - val_loss: 104353.7812\n",
      "Epoch 505/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.5938 - val_loss: 104353.7656\n",
      "Epoch 506/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6016 - val_loss: 104353.7656\n",
      "Epoch 507/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.7656\n",
      "Epoch 508/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.7500\n",
      "Epoch 509/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6172 - val_loss: 104353.7344\n",
      "Epoch 510/500000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 91233.6016 - val_loss: 104353.7500\n",
      "Epoch 511/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.7344\n",
      "Epoch 512/500000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 91233.6094 - val_loss: 104353.7344\n",
      "Epoch 513/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6172 - val_loss: 104353.7188\n",
      "Epoch 514/500000\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 91233.6094 - val_loss: 104353.7031\n",
      "Epoch 515/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.6953\n",
      "Epoch 516/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6016 - val_loss: 104353.6875\n",
      "Epoch 517/500000\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 91233.6172 - val_loss: 104353.6641\n",
      "Epoch 518/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6172 - val_loss: 104353.6641\n",
      "Epoch 519/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6172 - val_loss: 104353.6406\n",
      "Epoch 520/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.6406\n",
      "Epoch 521/500000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 91233.6094 - val_loss: 104353.6250\n",
      "Epoch 522/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.6250\n",
      "Epoch 523/500000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 91233.6172 - val_loss: 104353.6016\n",
      "Epoch 524/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.6016\n",
      "Epoch 525/500000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 91233.6094 - val_loss: 104353.5938\n",
      "Epoch 526/500000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 91233.6016 - val_loss: 104353.5859\n",
      "Epoch 527/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6016 - val_loss: 104353.5938\n",
      "Epoch 528/500000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 91233.6094 - val_loss: 104353.5625\n",
      "Epoch 529/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.5547\n",
      "Epoch 530/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.5547\n",
      "Epoch 531/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.5469\n",
      "Epoch 532/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6172 - val_loss: 104353.5547\n",
      "Epoch 533/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6172 - val_loss: 104353.5391\n",
      "Epoch 534/500000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 91233.6016 - val_loss: 104353.5391\n",
      "Epoch 535/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.5391\n",
      "Epoch 536/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.5234\n",
      "Epoch 537/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6016 - val_loss: 104353.5234\n",
      "Epoch 538/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.5234\n",
      "Epoch 539/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6016 - val_loss: 104353.5000\n",
      "Epoch 540/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.5000\n",
      "Epoch 541/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.5000\n",
      "Epoch 542/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.5000\n",
      "Epoch 543/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.5000\n",
      "Epoch 544/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.5000\n",
      "Epoch 545/500000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 91233.6094 - val_loss: 104353.5000\n",
      "Epoch 546/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6016 - val_loss: 104353.5000\n",
      "Epoch 547/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6094 - val_loss: 104353.5000\n",
      "Epoch 548/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.5000\n",
      "Epoch 549/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6172 - val_loss: 104353.5000\n",
      "Epoch 550/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6016 - val_loss: 104353.5000\n",
      "Epoch 551/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.5000\n",
      "Epoch 552/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.4922\n",
      "Epoch 553/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.4922\n",
      "Epoch 554/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6172 - val_loss: 104353.4844\n",
      "Epoch 555/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.5000\n",
      "Epoch 556/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.4844\n",
      "Epoch 557/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6016 - val_loss: 104353.4844\n",
      "Epoch 558/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6094 - val_loss: 104353.4688\n",
      "Epoch 559/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6172 - val_loss: 104353.4688\n",
      "Epoch 560/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.4688\n",
      "Epoch 561/500000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 91233.6094 - val_loss: 104353.4688\n",
      "Epoch 562/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6172 - val_loss: 104353.4609\n",
      "Epoch 563/500000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 91233.6094 - val_loss: 104353.4453\n",
      "Epoch 564/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6094 - val_loss: 104353.4375\n",
      "Epoch 565/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.5938 - val_loss: 104353.4297\n",
      "Epoch 566/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.4297\n",
      "Epoch 567/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6016 - val_loss: 104353.4141\n",
      "Epoch 568/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6016 - val_loss: 104353.4297\n",
      "Epoch 569/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 91233.6094 - val_loss: 104353.4141\n",
      "Epoch 570/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6016 - val_loss: 104353.4141\n",
      "Epoch 571/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.4141\n",
      "Epoch 572/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6016 - val_loss: 104353.4141\n",
      "Epoch 573/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6094 - val_loss: 104353.4141\n",
      "Epoch 574/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6172 - val_loss: 104353.4062\n",
      "Epoch 575/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6094 - val_loss: 104353.3984\n",
      "Epoch 576/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6016 - val_loss: 104353.4062\n",
      "Epoch 577/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.3984\n",
      "Epoch 578/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.3984\n",
      "Epoch 579/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6172 - val_loss: 104353.3828\n",
      "Epoch 580/500000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.3984\n",
      "Epoch 581/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6016 - val_loss: 104353.3984\n",
      "Epoch 582/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.3750\n",
      "Epoch 583/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.3750\n",
      "Epoch 584/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6016 - val_loss: 104353.3828\n",
      "Epoch 585/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6016 - val_loss: 104353.3750\n",
      "Epoch 586/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.3750\n",
      "Epoch 587/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.3750\n",
      "Epoch 588/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.3672\n",
      "Epoch 589/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.3750\n",
      "Epoch 590/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6016 - val_loss: 104353.3594\n",
      "Epoch 591/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6172 - val_loss: 104353.3594\n",
      "Epoch 592/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6172 - val_loss: 104353.3594\n",
      "Epoch 593/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6172 - val_loss: 104353.3594\n",
      "Epoch 594/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.3594\n",
      "Epoch 595/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6016 - val_loss: 104353.3594\n",
      "Epoch 596/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6016 - val_loss: 104353.3594\n",
      "Epoch 597/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6016 - val_loss: 104353.3750\n",
      "Epoch 598/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.5938 - val_loss: 104353.3750\n",
      "Epoch 599/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.3594\n",
      "Epoch 600/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.3594\n",
      "Epoch 601/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6016 - val_loss: 104353.3594\n",
      "Epoch 602/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.5938 - val_loss: 104353.3594\n",
      "Epoch 603/500000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 91233.6172 - val_loss: 104353.3438\n",
      "Epoch 604/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6016 - val_loss: 104353.3516\n",
      "Epoch 605/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6016 - val_loss: 104353.3438\n",
      "Epoch 606/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.3438\n",
      "Epoch 607/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6250 - val_loss: 104353.3438\n",
      "Epoch 608/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6172 - val_loss: 104353.3438\n",
      "Epoch 609/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6172 - val_loss: 104353.3359\n",
      "Epoch 610/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6172 - val_loss: 104353.3125\n",
      "Epoch 611/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.3281\n",
      "Epoch 612/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.3281\n",
      "Epoch 613/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.3281\n",
      "Epoch 614/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.3281\n",
      "Epoch 615/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6016 - val_loss: 104353.3281\n",
      "Epoch 616/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6094 - val_loss: 104353.3281\n",
      "Epoch 617/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.3281\n",
      "Epoch 618/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6172 - val_loss: 104353.3281\n",
      "Epoch 619/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6172 - val_loss: 104353.3281\n",
      "Epoch 620/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.3125\n",
      "Epoch 621/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.3281\n",
      "Epoch 622/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.3281\n",
      "Epoch 623/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6016 - val_loss: 104353.3281\n",
      "Epoch 624/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6172 - val_loss: 104353.3281\n",
      "Epoch 625/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.3125\n",
      "Epoch 626/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.3047\n",
      "Epoch 627/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6016 - val_loss: 104353.3047\n",
      "Epoch 628/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6016 - val_loss: 104353.3047\n",
      "Epoch 629/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6016 - val_loss: 104353.3047\n",
      "Epoch 630/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.3047\n",
      "Epoch 631/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6016 - val_loss: 104353.3047\n",
      "Epoch 632/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.3047\n",
      "Epoch 633/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.3281\n",
      "Epoch 634/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.3125\n",
      "Epoch 635/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6172 - val_loss: 104353.3125\n",
      "Epoch 636/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.3047\n",
      "Epoch 637/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6094 - val_loss: 104353.3047\n",
      "Epoch 638/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.3047\n",
      "Epoch 639/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.3047\n",
      "Epoch 640/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.3047\n",
      "Epoch 641/500000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 91233.6094 - val_loss: 104353.3047\n",
      "Epoch 642/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.3047\n",
      "Epoch 643/500000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 91233.6094 - val_loss: 104353.3047\n",
      "Epoch 644/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6172 - val_loss: 104353.3047\n",
      "Epoch 645/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6172 - val_loss: 104353.3047\n",
      "Epoch 646/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.3047\n",
      "Epoch 647/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.3047\n",
      "Epoch 648/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.3047\n",
      "Epoch 649/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.3047\n",
      "Epoch 650/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6172 - val_loss: 104353.3047\n",
      "Epoch 651/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.3047\n",
      "Epoch 652/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.3047\n",
      "Epoch 653/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6172 - val_loss: 104353.3047\n",
      "Epoch 654/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.3047\n",
      "Epoch 655/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.3047\n",
      "Epoch 656/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6016 - val_loss: 104353.2969\n",
      "Epoch 657/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6172 - val_loss: 104353.2969\n",
      "Epoch 658/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 659/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6016 - val_loss: 104353.3047\n",
      "Epoch 660/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6016 - val_loss: 104353.2969\n",
      "Epoch 661/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.2969\n",
      "Epoch 662/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6016 - val_loss: 104353.2969\n",
      "Epoch 663/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6016 - val_loss: 104353.2969\n",
      "Epoch 664/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6016 - val_loss: 104353.2891\n",
      "Epoch 665/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 666/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 667/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6172 - val_loss: 104353.2969\n",
      "Epoch 668/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6016 - val_loss: 104353.2891\n",
      "Epoch 669/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 670/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6016 - val_loss: 104353.2891\n",
      "Epoch 671/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6172 - val_loss: 104353.2891\n",
      "Epoch 672/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6094 - val_loss: 104353.2969\n",
      "Epoch 673/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6094 - val_loss: 104353.2969\n",
      "Epoch 674/500000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 675/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6172 - val_loss: 104353.2891\n",
      "Epoch 676/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 677/500000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 678/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 679/500000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 680/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 681/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 682/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 683/500000\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 91233.6016 - val_loss: 104353.2891\n",
      "Epoch 684/500000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 91233.6094 - val_loss: 104353.2734\n",
      "Epoch 685/500000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 91233.6094 - val_loss: 104353.2734\n",
      "Epoch 686/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.2734\n",
      "Epoch 687/500000\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 91233.6094 - val_loss: 104353.2812\n",
      "Epoch 688/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.2812\n",
      "Epoch 689/500000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 91233.6016 - val_loss: 104353.2812\n",
      "Epoch 690/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6094 - val_loss: 104353.2812\n",
      "Epoch 691/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.2734\n",
      "Epoch 692/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6094 - val_loss: 104353.2734\n",
      "Epoch 693/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6016 - val_loss: 104353.2891\n",
      "Epoch 694/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6172 - val_loss: 104353.2891\n",
      "Epoch 695/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6172 - val_loss: 104353.2891\n",
      "Epoch 696/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 697/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6172 - val_loss: 104353.2891\n",
      "Epoch 698/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 699/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6172 - val_loss: 104353.2891\n",
      "Epoch 700/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6172 - val_loss: 104353.2891\n",
      "Epoch 701/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6172 - val_loss: 104353.2891\n",
      "Epoch 702/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 703/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6172 - val_loss: 104353.2891\n",
      "Epoch 704/500000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 705/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6016 - val_loss: 104353.2891\n",
      "Epoch 706/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 707/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 708/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 709/500000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 91233.6172 - val_loss: 104353.2891\n",
      "Epoch 710/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 711/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6172 - val_loss: 104353.2891\n",
      "Epoch 712/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 713/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 714/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6016 - val_loss: 104353.2891\n",
      "Epoch 715/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6172 - val_loss: 104353.2891\n",
      "Epoch 716/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6016 - val_loss: 104353.2891\n",
      "Epoch 717/500000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 91233.6172 - val_loss: 104353.2891\n",
      "Epoch 718/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6016 - val_loss: 104353.2891\n",
      "Epoch 719/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 720/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 721/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 722/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 723/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6016 - val_loss: 104353.2891\n",
      "Epoch 724/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 725/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 726/500000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6172 - val_loss: 104353.2891\n",
      "Epoch 727/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6172 - val_loss: 104353.2891\n",
      "Epoch 728/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 729/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 730/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6172 - val_loss: 104353.2891\n",
      "Epoch 731/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6172 - val_loss: 104353.2891\n",
      "Epoch 732/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 733/500000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 91233.6016 - val_loss: 104353.2891\n",
      "Epoch 734/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 735/500000\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 736/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6172 - val_loss: 104353.2891\n",
      "Epoch 737/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6172 - val_loss: 104353.2891\n",
      "Epoch 738/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 739/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6250 - val_loss: 104353.2891\n",
      "Epoch 740/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6172 - val_loss: 104353.2891\n",
      "Epoch 741/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 742/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 743/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 744/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6172 - val_loss: 104353.2891\n",
      "Epoch 745/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 746/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 747/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 748/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 749/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 750/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 751/500000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 752/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6016 - val_loss: 104353.2891\n",
      "Epoch 753/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.2969\n",
      "Epoch 754/500000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 91233.6094 - val_loss: 104353.2969\n",
      "Epoch 755/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6172 - val_loss: 104353.2969\n",
      "Epoch 756/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6172 - val_loss: 104353.2969\n",
      "Epoch 757/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6172 - val_loss: 104353.2969\n",
      "Epoch 758/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.2969\n",
      "Epoch 759/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.2969\n",
      "Epoch 760/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6016 - val_loss: 104353.2969\n",
      "Epoch 761/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.2969\n",
      "Epoch 762/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.2969\n",
      "Epoch 763/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.2969\n",
      "Epoch 764/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.2969\n",
      "Epoch 765/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6016 - val_loss: 104353.2969\n",
      "Epoch 766/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.2969\n",
      "Epoch 767/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.2969\n",
      "Epoch 768/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.2969\n",
      "Epoch 769/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6016 - val_loss: 104353.2969\n",
      "Epoch 770/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6016 - val_loss: 104353.2969\n",
      "Epoch 771/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 772/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 773/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6016 - val_loss: 104353.2891\n",
      "Epoch 774/500000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 775/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 776/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 777/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 778/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 779/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 780/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6016 - val_loss: 104353.2891\n",
      "Epoch 781/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6016 - val_loss: 104353.2891\n",
      "Epoch 782/500000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 783/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6094 - val_loss: 104353.2891\n",
      "Epoch 784/500000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 91233.6172 - val_loss: 104353.2891\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=X_train.shape[1], activation='relu',kernel_initializer=RandomNormal(mean=0.0, stddev=relu_std, seed=None)))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dense(1, activation='linear'))\n",
    "opt = SGD(learning_rate=0.01,momentum=0.9)\n",
    "model.compile(optimizer=opt, loss='mse')\n",
    "model.summary()\n",
    "early_stopping_cb = EarlyStopping(patience=100,restore_best_weights=True)\n",
    "run_logdir=r\"C:\\Users\\Deepak Tripathi\\Desktop\\rossman\\log\"\n",
    "history = model.fit(np.array(X_train), y_train, batch_size=len(X_train),validation_data=(X_val, y_val) ,epochs=500000, verbose=1,callbacks=[early_stopping_cb])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE= 104353.28429197156\n",
      "R2= 0.9089319566401841\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X_val)\n",
    "MSE=mean_squared_error(y_val,pred)\n",
    "r2=r2_score(y_val,pred)\n",
    "print('MSE=',MSE)\n",
    "print('R2=',r2)\n",
    "print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# performence has decreased with Relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# so best model for regression is MLP with 55 nuerons in first layer with sigmoid activation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classfication for Claim or No claim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create a model to classify if a person should be given insurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1274,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r\"C:\\Users\\Deepak Tripathi\\Desktop\\assignment machine learning\\part3\\CE802_P3_Data.csv\")\n",
    "\n",
    "\n",
    "df['F15']=df['F15'].map({'Very low':0.0,'Low':1.0,'Medium':2.0,'High':3.0,'Very high':4.0})\n",
    "\n",
    "selected_df=df[['F2','F4','F6','F7','F9','F11','F12','F14','F15','Target']]\n",
    "\n",
    "df=selected_df.drop(['Target'],axis=1)\n",
    "y=selected_df['Target']\n",
    "\n",
    "\n",
    "cat=df.dtypes==object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1275,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer(\n",
    "     [(\"ohe\", OneHotEncoder(handle_unknown='ignore'), cat),\n",
    "      (\"norm\", StandardScaler(), ~cat)])\n",
    "\n",
    "ct.fit(df)\n",
    "x=ct.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 12)"
      ]
     },
     "execution_count": 1276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1277,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1278,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, X_test, y, y_test = train_test_split(x, y, test_size=0.20, random_state=41)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create a new target for classification. This target will be false if claim value is equal to 0 otherwise True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1279,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_c=y!=0\n",
    "y_test_c=y_test!=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier= 0.8479769971864618\n",
      "LogisticRegression= 0.9922575780448272\n",
      "GradientBoostingClassifier= 0.9279454246144377\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(DecisionTreeClassifier(), x, y_c, cv=10,scoring='f1') # df1 dataset contains standard scaled features\n",
    "print('DecisionTreeClassifier=', np.mean(scores))\n",
    "\n",
    "scores = cross_val_score(LogisticRegression(), x, y_c, cv=10, scoring='f1')\n",
    "print('LogisticRegression=',np.mean(scores))\n",
    "\n",
    "scores = cross_val_score(GradientBoostingClassifier(), x, y_c, cv=10, scoring='f1')\n",
    "print('GradientBoostingClassifier=',np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use logistic regression to predict if a person will claim or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 1281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic=LogisticRegression()\n",
    "logistic.fit(x,y_c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now again train MLP on same data in which logistic regression has been trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1282,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(x, y, test_size=0.10, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_77\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_148 (Dense)            (None, 55)                715       \n",
      "_________________________________________________________________\n",
      "dense_149 (Dense)            (None, 1)                 56        \n",
      "=================================================================\n",
      "Total params: 771\n",
      "Trainable params: 771\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/500000\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 2037210.5000 - val_loss: 1468720.5000\n",
      "Epoch 2/500000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1612183.7500 - val_loss: 744681.3750\n",
      "Epoch 3/500000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 854368.5625 - val_loss: 520993.1875\n",
      "Epoch 4/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 607519.1875 - val_loss: 453741.8125\n",
      "Epoch 5/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 529547.8125 - val_loss: 352524.3125\n",
      "Epoch 6/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 390475.2500 - val_loss: 264007.4062\n",
      "Epoch 7/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 275063.6562 - val_loss: 265387.0000\n",
      "Epoch 8/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 234773.3750 - val_loss: 273899.2188\n",
      "Epoch 9/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 216652.9531 - val_loss: 234132.5938\n",
      "Epoch 10/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 179494.6719 - val_loss: 168244.8281\n",
      "Epoch 11/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 149381.8594 - val_loss: 129466.3281\n",
      "Epoch 12/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 141828.6250 - val_loss: 110340.4609\n",
      "Epoch 13/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 139589.1406 - val_loss: 96294.5469\n",
      "Epoch 14/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 116695.0391 - val_loss: 74013.7031\n",
      "Epoch 15/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 88952.3594 - val_loss: 73948.2500\n",
      "Epoch 16/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 75625.3125 - val_loss: 82090.3672\n",
      "Epoch 17/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 76023.9766 - val_loss: 75917.8359\n",
      "Epoch 18/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 69834.9453 - val_loss: 61657.8047\n",
      "Epoch 19/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 60708.5312 - val_loss: 54890.8711\n",
      "Epoch 20/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 57679.6680 - val_loss: 53850.8047\n",
      "Epoch 21/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 58629.5156 - val_loss: 53738.7148\n",
      "Epoch 22/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 55838.9062 - val_loss: 52640.0117\n",
      "Epoch 23/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 50133.3789 - val_loss: 53711.4336\n",
      "Epoch 24/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 45285.9844 - val_loss: 52702.7852\n",
      "Epoch 25/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 43586.1211 - val_loss: 50876.6250\n",
      "Epoch 26/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 42923.4688 - val_loss: 48235.7734\n",
      "Epoch 27/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 38083.7852 - val_loss: 44770.7500\n",
      "Epoch 28/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 34097.5859 - val_loss: 43703.7695\n",
      "Epoch 29/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 32910.5859 - val_loss: 40647.6875\n",
      "Epoch 30/500000\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 31958.8184 - val_loss: 36276.9336\n",
      "Epoch 31/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 29472.1484 - val_loss: 36287.6250\n",
      "Epoch 32/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 27309.5293 - val_loss: 39157.7383\n",
      "Epoch 33/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 26467.3027 - val_loss: 40740.3047\n",
      "Epoch 34/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 26493.6074 - val_loss: 39656.2148\n",
      "Epoch 35/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 24958.4336 - val_loss: 36052.3281\n",
      "Epoch 36/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 23331.5371 - val_loss: 31067.3008\n",
      "Epoch 37/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 22545.7676 - val_loss: 29438.3672\n",
      "Epoch 38/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 22178.9824 - val_loss: 28276.0996\n",
      "Epoch 39/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 21383.9375 - val_loss: 26926.7812\n",
      "Epoch 40/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 20136.6777 - val_loss: 27534.6289\n",
      "Epoch 41/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 19087.6699 - val_loss: 27971.8633\n",
      "Epoch 42/500000\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 18611.8301 - val_loss: 27839.0176\n",
      "Epoch 43/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 18197.2012 - val_loss: 26878.2051\n",
      "Epoch 44/500000\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 17274.9004 - val_loss: 25152.4551\n",
      "Epoch 45/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 16399.7539 - val_loss: 24730.1367\n",
      "Epoch 46/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 15791.7227 - val_loss: 24130.3301\n",
      "Epoch 47/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 15351.9688 - val_loss: 23493.5977\n",
      "Epoch 48/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 14834.6973 - val_loss: 24088.8867\n",
      "Epoch 49/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 14248.6875 - val_loss: 24944.3008\n",
      "Epoch 50/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 13914.2061 - val_loss: 24796.7949\n",
      "Epoch 51/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 13613.6689 - val_loss: 23713.7207\n",
      "Epoch 52/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 13394.4072 - val_loss: 22802.7617\n",
      "Epoch 53/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 13045.1104 - val_loss: 21859.6875\n",
      "Epoch 54/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 12574.1650 - val_loss: 21860.5039\n",
      "Epoch 55/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 12339.7812 - val_loss: 22551.8418\n",
      "Epoch 56/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 12007.8955 - val_loss: 22513.1035\n",
      "Epoch 57/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 11745.3018 - val_loss: 22423.4258\n",
      "Epoch 58/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 11548.6182 - val_loss: 22432.2168\n",
      "Epoch 59/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 11216.7139 - val_loss: 22607.0703\n",
      "Epoch 60/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 10962.2832 - val_loss: 22249.1582\n",
      "Epoch 61/500000\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 10735.8125 - val_loss: 20730.8496\n",
      "Epoch 62/500000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 10587.0645 - val_loss: 19641.6953\n",
      "Epoch 63/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 10500.1143 - val_loss: 19146.8789\n",
      "Epoch 64/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 10452.9541 - val_loss: 18985.5527\n",
      "Epoch 65/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 10350.5537 - val_loss: 19032.6836\n",
      "Epoch 66/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 10226.8145 - val_loss: 19157.2383\n",
      "Epoch 67/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 10067.5850 - val_loss: 19115.9004\n",
      "Epoch 68/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 9950.2969 - val_loss: 18802.0664\n",
      "Epoch 69/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 9863.5391 - val_loss: 18504.2734\n",
      "Epoch 70/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 9798.5088 - val_loss: 18483.5527\n",
      "Epoch 71/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 9700.8330 - val_loss: 18636.6484\n",
      "Epoch 72/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 9546.4355 - val_loss: 18704.3926\n",
      "Epoch 73/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 9319.1318 - val_loss: 18789.0410\n",
      "Epoch 74/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 9155.2510 - val_loss: 18914.9336\n",
      "Epoch 75/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 9006.8164 - val_loss: 18881.1289\n",
      "Epoch 76/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 8954.9111 - val_loss: 18686.0898\n",
      "Epoch 77/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 8895.8506 - val_loss: 18481.5430\n",
      "Epoch 78/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 8839.1689 - val_loss: 18698.3535\n",
      "Epoch 79/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 8744.0332 - val_loss: 18972.2383\n",
      "Epoch 80/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 8668.4912 - val_loss: 19077.2852\n",
      "Epoch 81/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 8625.1230 - val_loss: 19122.2188\n",
      "Epoch 82/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 8591.1826 - val_loss: 19109.6680\n",
      "Epoch 83/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 8529.1514 - val_loss: 19146.6582\n",
      "Epoch 84/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 8436.1221 - val_loss: 19389.8555\n",
      "Epoch 85/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 8352.1660 - val_loss: 19600.5332\n",
      "Epoch 86/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 8318.6689 - val_loss: 19683.2324\n",
      "Epoch 87/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 8285.7881 - val_loss: 19759.6836\n",
      "Epoch 88/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 8222.1631 - val_loss: 19817.0918\n",
      "Epoch 89/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 8149.4482 - val_loss: 19843.7988\n",
      "Epoch 90/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 8102.4863 - val_loss: 19872.2793\n",
      "Epoch 91/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 8066.3286 - val_loss: 19908.2559\n",
      "Epoch 92/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 8024.3887 - val_loss: 19950.6621\n",
      "Epoch 93/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 7993.8696 - val_loss: 20022.8555\n",
      "Epoch 94/500000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 7958.7075 - val_loss: 20123.6992\n",
      "Epoch 95/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 7910.9204 - val_loss: 20200.0508\n",
      "Epoch 96/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 7879.9233 - val_loss: 20235.2676\n",
      "Epoch 97/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 7847.5518 - val_loss: 20263.1172\n",
      "Epoch 98/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 7799.3657 - val_loss: 20304.0586\n",
      "Epoch 99/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 7753.6909 - val_loss: 20357.2930\n",
      "Epoch 100/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 7715.8223 - val_loss: 20365.1992\n",
      "Epoch 101/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 7679.3232 - val_loss: 20324.1289\n",
      "Epoch 102/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 7624.0601 - val_loss: 20289.0527\n",
      "Epoch 103/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 7508.5640 - val_loss: 20337.5625\n",
      "Epoch 104/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 7469.9570 - val_loss: 20420.8965\n",
      "Epoch 105/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 7441.4214 - val_loss: 20480.2461\n",
      "Epoch 106/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 7349.5859 - val_loss: 20502.9746\n",
      "Epoch 107/500000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 7281.7231 - val_loss: 20479.9082\n",
      "Epoch 108/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 7238.5845 - val_loss: 20372.3574\n",
      "Epoch 109/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 7205.3647 - val_loss: 20157.6465\n",
      "Epoch 110/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 7181.0503 - val_loss: 19852.0586\n",
      "Epoch 111/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 7152.4648 - val_loss: 19508.2598\n",
      "Epoch 112/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 7119.4810 - val_loss: 19139.1289\n",
      "Epoch 113/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 7073.1562 - val_loss: 18888.8867\n",
      "Epoch 114/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 7013.3936 - val_loss: 18802.9082\n",
      "Epoch 115/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 6965.2446 - val_loss: 18827.9199\n",
      "Epoch 116/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 6908.0298 - val_loss: 18920.5273\n",
      "Epoch 117/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 6879.3301 - val_loss: 19036.4961\n",
      "Epoch 118/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 6854.0601 - val_loss: 19132.7305\n",
      "Epoch 119/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 6815.2227 - val_loss: 19179.8535\n",
      "Epoch 120/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 6793.2065 - val_loss: 19170.6836\n",
      "Epoch 121/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 6773.4604 - val_loss: 19108.2148\n",
      "Epoch 122/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 6748.9038 - val_loss: 19005.6914\n",
      "Epoch 123/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 6714.0879 - val_loss: 18890.7793\n",
      "Epoch 124/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 6685.7417 - val_loss: 18775.2559\n",
      "Epoch 125/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 6657.4785 - val_loss: 18672.5254\n",
      "Epoch 126/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 6629.1909 - val_loss: 18585.3750\n",
      "Epoch 127/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 6594.6011 - val_loss: 18504.1035\n",
      "Epoch 128/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 6568.4277 - val_loss: 18426.5664\n",
      "Epoch 129/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 6547.4126 - val_loss: 18366.1992\n",
      "Epoch 130/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 6533.9194 - val_loss: 18339.2617\n",
      "Epoch 131/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 6529.8291 - val_loss: 18338.1602\n",
      "Epoch 132/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 6517.1157 - val_loss: 18335.1797\n",
      "Epoch 133/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 6500.1138 - val_loss: 18305.7012\n",
      "Epoch 134/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 6486.8579 - val_loss: 18249.4746\n",
      "Epoch 135/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 6474.5444 - val_loss: 18187.3145\n",
      "Epoch 136/500000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 6466.3521 - val_loss: 18140.5137\n",
      "Epoch 137/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 6456.9619 - val_loss: 18119.9883\n",
      "Epoch 138/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 6445.8608 - val_loss: 18129.0020\n",
      "Epoch 139/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 6434.3184 - val_loss: 18161.1504\n",
      "Epoch 140/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 6423.7417 - val_loss: 18202.2422\n",
      "Epoch 141/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 6413.4351 - val_loss: 18238.8828\n",
      "Epoch 142/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 6400.0874 - val_loss: 18269.4570\n",
      "Epoch 143/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 6385.2900 - val_loss: 18301.5449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 6372.7832 - val_loss: 18339.0000\n",
      "Epoch 145/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 6364.4751 - val_loss: 18386.8984\n",
      "Epoch 146/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 6357.3979 - val_loss: 18421.8848\n",
      "Epoch 147/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 6344.1870 - val_loss: 18418.6582\n",
      "Epoch 148/500000\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 6326.2451 - val_loss: 18379.1992\n",
      "Epoch 149/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 6310.0137 - val_loss: 18314.9043\n",
      "Epoch 150/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 6300.6094 - val_loss: 18245.8066\n",
      "Epoch 151/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 6292.8394 - val_loss: 18208.8457\n",
      "Epoch 152/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 6282.3428 - val_loss: 18252.6211\n",
      "Epoch 153/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 6269.0229 - val_loss: 18394.2148\n",
      "Epoch 154/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 6252.7725 - val_loss: 18539.2383\n",
      "Epoch 155/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 6239.6211 - val_loss: 18607.3359\n",
      "Epoch 156/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 6233.8350 - val_loss: 18627.4395\n",
      "Epoch 157/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 6228.6572 - val_loss: 18628.2207\n",
      "Epoch 158/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 6220.2891 - val_loss: 18607.4180\n",
      "Epoch 159/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 6210.9404 - val_loss: 18552.3613\n",
      "Epoch 160/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 6202.4829 - val_loss: 18450.3301\n",
      "Epoch 161/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 6192.9844 - val_loss: 18316.9746\n",
      "Epoch 162/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 6180.9365 - val_loss: 18210.7578\n",
      "Epoch 163/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 6173.0928 - val_loss: 18109.4961\n",
      "Epoch 164/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 6168.6890 - val_loss: 17991.0625\n",
      "Epoch 165/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 6160.7026 - val_loss: 17851.1133\n",
      "Epoch 166/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 6151.3999 - val_loss: 17716.2637\n",
      "Epoch 167/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 6143.9575 - val_loss: 17605.3223\n",
      "Epoch 168/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 6135.4165 - val_loss: 17512.0605\n",
      "Epoch 169/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 6123.3750 - val_loss: 17439.2715\n",
      "Epoch 170/500000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 6109.2871 - val_loss: 17394.9258\n",
      "Epoch 171/500000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 6095.1699 - val_loss: 17378.1211\n",
      "Epoch 172/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 6076.2305 - val_loss: 17356.8711\n",
      "Epoch 173/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 6056.5000 - val_loss: 17289.9473\n",
      "Epoch 174/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 6047.1328 - val_loss: 17160.7324\n",
      "Epoch 175/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 6037.4111 - val_loss: 16989.8008\n",
      "Epoch 176/500000\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 6027.0137 - val_loss: 16835.3379\n",
      "Epoch 177/500000\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 6020.1172 - val_loss: 16721.4746\n",
      "Epoch 178/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 6012.9629 - val_loss: 16645.1270\n",
      "Epoch 179/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 6003.6328 - val_loss: 16599.9023\n",
      "Epoch 180/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5991.7041 - val_loss: 16592.9551\n",
      "Epoch 181/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5980.5044 - val_loss: 16609.9785\n",
      "Epoch 182/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5970.4502 - val_loss: 16613.0918\n",
      "Epoch 183/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5953.3369 - val_loss: 16594.8066\n",
      "Epoch 184/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5936.6011 - val_loss: 16574.5879\n",
      "Epoch 185/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5927.9204 - val_loss: 16550.0059\n",
      "Epoch 186/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5918.8325 - val_loss: 16518.3809\n",
      "Epoch 187/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5905.7129 - val_loss: 16482.9707\n",
      "Epoch 188/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5891.5391 - val_loss: 16465.2910\n",
      "Epoch 189/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5878.9111 - val_loss: 16464.7051\n",
      "Epoch 190/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5865.0430 - val_loss: 16416.9395\n",
      "Epoch 191/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5852.6958 - val_loss: 16323.2900\n",
      "Epoch 192/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5841.3130 - val_loss: 16246.3857\n",
      "Epoch 193/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5828.4946 - val_loss: 16208.2910\n",
      "Epoch 194/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5814.2393 - val_loss: 16182.4199\n",
      "Epoch 195/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5805.3701 - val_loss: 16145.8330\n",
      "Epoch 196/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5800.5361 - val_loss: 16107.1514\n",
      "Epoch 197/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5797.0693 - val_loss: 16082.3066\n",
      "Epoch 198/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5793.7461 - val_loss: 16064.1094\n",
      "Epoch 199/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5790.5879 - val_loss: 16041.0371\n",
      "Epoch 200/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5786.8389 - val_loss: 16023.7354\n",
      "Epoch 201/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5782.7837 - val_loss: 16023.1143\n",
      "Epoch 202/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5777.7192 - val_loss: 16026.8857\n",
      "Epoch 203/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5772.4326 - val_loss: 16014.7148\n",
      "Epoch 204/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5766.4224 - val_loss: 15984.6650\n",
      "Epoch 205/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5758.6865 - val_loss: 15949.5479\n",
      "Epoch 206/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 5749.8120 - val_loss: 15910.0801\n",
      "Epoch 207/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5740.3179 - val_loss: 15860.1338\n",
      "Epoch 208/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5734.3989 - val_loss: 15807.2041\n",
      "Epoch 209/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5730.5044 - val_loss: 15763.0020\n",
      "Epoch 210/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 5725.3296 - val_loss: 15723.2227\n",
      "Epoch 211/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5718.4111 - val_loss: 15675.3350\n",
      "Epoch 212/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5712.4048 - val_loss: 15617.2510\n",
      "Epoch 213/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 5708.8262 - val_loss: 15555.9082\n",
      "Epoch 214/500000\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 5705.6943 - val_loss: 15494.0957\n",
      "Epoch 215/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 5701.7583 - val_loss: 15431.3770\n",
      "Epoch 216/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 5697.4214 - val_loss: 15373.0225\n",
      "Epoch 217/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 5693.1826 - val_loss: 15326.9854\n",
      "Epoch 218/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 5688.6343 - val_loss: 15292.7217\n",
      "Epoch 219/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 5683.9092 - val_loss: 15260.5459\n",
      "Epoch 220/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5679.0317 - val_loss: 15221.1025\n",
      "Epoch 221/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 5671.3794 - val_loss: 15170.6318\n",
      "Epoch 222/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5653.9009 - val_loss: 15120.2529\n",
      "Epoch 223/500000\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 5642.1235 - val_loss: 15083.6338\n",
      "Epoch 224/500000\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 5645.1665 - val_loss: 15055.3809\n",
      "Epoch 225/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5638.2256 - val_loss: 15044.5771\n",
      "Epoch 226/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5625.4409 - val_loss: 15057.2607\n",
      "Epoch 227/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 5616.1382 - val_loss: 15086.4873\n",
      "Epoch 228/500000\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 5609.6519 - val_loss: 15118.5400\n",
      "Epoch 229/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5605.9834 - val_loss: 15140.2598\n",
      "Epoch 230/500000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 5602.5674 - val_loss: 15146.7607\n",
      "Epoch 231/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 5597.7598 - val_loss: 15139.9775\n",
      "Epoch 232/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 5592.7900 - val_loss: 15126.1592\n",
      "Epoch 233/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5587.9102 - val_loss: 15114.0918\n",
      "Epoch 234/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5582.5317 - val_loss: 15109.7725\n",
      "Epoch 235/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5577.3213 - val_loss: 15112.5742\n",
      "Epoch 236/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5573.1489 - val_loss: 15118.1748\n",
      "Epoch 237/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5569.7324 - val_loss: 15122.6582\n",
      "Epoch 238/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5566.2222 - val_loss: 15123.0752\n",
      "Epoch 239/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5561.8862 - val_loss: 15118.3711\n",
      "Epoch 240/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5556.3481 - val_loss: 15110.9600\n",
      "Epoch 241/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5549.1113 - val_loss: 15106.5088\n",
      "Epoch 242/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5540.5547 - val_loss: 15109.8086\n",
      "Epoch 243/500000\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 5536.1235 - val_loss: 15120.7373\n",
      "Epoch 244/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5534.5396 - val_loss: 15136.1035\n",
      "Epoch 245/500000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 5533.0293 - val_loss: 15152.1143\n",
      "Epoch 246/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5530.8027 - val_loss: 15165.4463\n",
      "Epoch 247/500000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 5527.5767 - val_loss: 15174.2207\n",
      "Epoch 248/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 5522.9233 - val_loss: 15179.9336\n",
      "Epoch 249/500000\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 5515.8706 - val_loss: 15185.8193\n",
      "Epoch 250/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5507.6973 - val_loss: 15185.5459\n",
      "Epoch 251/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5504.6128 - val_loss: 15188.9727\n",
      "Epoch 252/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 5503.2212 - val_loss: 15212.5605\n",
      "Epoch 253/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5501.7339 - val_loss: 15258.0938\n",
      "Epoch 254/500000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 5499.4424 - val_loss: 15315.1689\n",
      "Epoch 255/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5496.6226 - val_loss: 15363.5059\n",
      "Epoch 256/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 5493.8955 - val_loss: 15387.0625\n",
      "Epoch 257/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 5490.8677 - val_loss: 15384.3994\n",
      "Epoch 258/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5487.1694 - val_loss: 15361.7490\n",
      "Epoch 259/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5483.2319 - val_loss: 15326.7168\n",
      "Epoch 260/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5479.3008 - val_loss: 15286.2920\n",
      "Epoch 261/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5474.6294 - val_loss: 15247.2910\n",
      "Epoch 262/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5466.8257 - val_loss: 15217.5664\n",
      "Epoch 263/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5457.4775 - val_loss: 15204.6914\n",
      "Epoch 264/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5452.8823 - val_loss: 15205.2207\n",
      "Epoch 265/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5446.6680 - val_loss: 15210.2246\n",
      "Epoch 266/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5442.1621 - val_loss: 15212.7461\n",
      "Epoch 267/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5435.5918 - val_loss: 15213.4697\n",
      "Epoch 268/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 5435.2070 - val_loss: 15218.2939\n",
      "Epoch 269/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5429.7676 - val_loss: 15226.0449\n",
      "Epoch 270/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 5428.1714 - val_loss: 15225.8545\n",
      "Epoch 271/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5424.8975 - val_loss: 15211.1094\n",
      "Epoch 272/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5417.6919 - val_loss: 15188.0039\n",
      "Epoch 273/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5411.6489 - val_loss: 15162.0762\n",
      "Epoch 274/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5405.7993 - val_loss: 15125.6348\n",
      "Epoch 275/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5402.1294 - val_loss: 15073.9131\n",
      "Epoch 276/500000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 5400.0093 - val_loss: 15016.0039\n",
      "Epoch 277/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5396.3442 - val_loss: 14962.6504\n",
      "Epoch 278/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5392.4443 - val_loss: 14916.9443\n",
      "Epoch 279/500000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 5389.4600 - val_loss: 14881.6143\n",
      "Epoch 280/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5385.0698 - val_loss: 14860.9873\n",
      "Epoch 281/500000\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 5381.7344 - val_loss: 14852.3760\n",
      "Epoch 282/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 5378.3945 - val_loss: 14844.9238\n",
      "Epoch 283/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5373.0181 - val_loss: 14832.8096\n",
      "Epoch 284/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5357.0332 - val_loss: 14819.7246\n",
      "Epoch 285/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5349.5464 - val_loss: 14804.2002\n",
      "Epoch 286/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5349.8105 - val_loss: 14782.2920\n",
      "Epoch 287/500000\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 5349.1372 - val_loss: 14751.9990\n",
      "Epoch 288/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5347.6587 - val_loss: 14718.3389\n",
      "Epoch 289/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 5345.6543 - val_loss: 14686.3477\n",
      "Epoch 290/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5343.4258 - val_loss: 14655.8311\n",
      "Epoch 291/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5341.0747 - val_loss: 14625.9229\n",
      "Epoch 292/500000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step - loss: 5338.5444 - val_loss: 14599.3711\n",
      "Epoch 293/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 5335.9585 - val_loss: 14578.3496\n",
      "Epoch 294/500000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 5332.4580 - val_loss: 14560.7373\n",
      "Epoch 295/500000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 5328.3052 - val_loss: 14543.8809\n",
      "Epoch 296/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 5324.7061 - val_loss: 14526.6104\n",
      "Epoch 297/500000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 5322.2241 - val_loss: 14506.8604\n",
      "Epoch 298/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 5320.7100 - val_loss: 14480.8213\n",
      "Epoch 299/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5319.1489 - val_loss: 14447.4336\n",
      "Epoch 300/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5317.1387 - val_loss: 14410.9355\n",
      "Epoch 301/500000\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 5314.9888 - val_loss: 14377.2598\n",
      "Epoch 302/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5312.5835 - val_loss: 14349.6709\n",
      "Epoch 303/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5310.0947 - val_loss: 14329.1162\n",
      "Epoch 304/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5307.6748 - val_loss: 14316.1816\n",
      "Epoch 305/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5305.1455 - val_loss: 14310.3398\n",
      "Epoch 306/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5302.2622 - val_loss: 14309.2471\n",
      "Epoch 307/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5298.6094 - val_loss: 14310.5293\n",
      "Epoch 308/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5293.5415 - val_loss: 14313.2295\n",
      "Epoch 309/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5286.6460 - val_loss: 14318.9463\n",
      "Epoch 310/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5279.8882 - val_loss: 14332.2881\n",
      "Epoch 311/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5276.6929 - val_loss: 14352.6689\n",
      "Epoch 312/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5275.7861 - val_loss: 14367.5947\n",
      "Epoch 313/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5274.0928 - val_loss: 14363.9902\n",
      "Epoch 314/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5270.4268 - val_loss: 14340.1357\n",
      "Epoch 315/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5265.5356 - val_loss: 14305.5518\n",
      "Epoch 316/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 5260.4390 - val_loss: 14272.7646\n",
      "Epoch 317/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5254.9575 - val_loss: 14247.8271\n",
      "Epoch 318/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5248.2065 - val_loss: 14230.3066\n",
      "Epoch 319/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5244.6245 - val_loss: 14218.6113\n",
      "Epoch 320/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5244.3149 - val_loss: 14211.5723\n",
      "Epoch 321/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5240.9116 - val_loss: 14207.3721\n",
      "Epoch 322/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5235.9404 - val_loss: 14204.6709\n",
      "Epoch 323/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5232.3647 - val_loss: 14203.2920\n",
      "Epoch 324/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5229.1978 - val_loss: 14202.6963\n",
      "Epoch 325/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5224.9473 - val_loss: 14202.5039\n",
      "Epoch 326/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5217.3149 - val_loss: 14209.9287\n",
      "Epoch 327/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5208.9111 - val_loss: 14226.8848\n",
      "Epoch 328/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 5203.7700 - val_loss: 14234.1279\n",
      "Epoch 329/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5197.8979 - val_loss: 14224.4463\n",
      "Epoch 330/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5191.7725 - val_loss: 14206.2441\n",
      "Epoch 331/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5188.2798 - val_loss: 14191.3643\n",
      "Epoch 332/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 5188.7119 - val_loss: 14186.1533\n",
      "Epoch 333/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5189.1855 - val_loss: 14188.1904\n",
      "Epoch 334/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 5187.5361 - val_loss: 14188.5947\n",
      "Epoch 335/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5184.8882 - val_loss: 14176.3418\n",
      "Epoch 336/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5182.6611 - val_loss: 14146.6289\n",
      "Epoch 337/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5180.5854 - val_loss: 14107.1670\n",
      "Epoch 338/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5178.0796 - val_loss: 14072.3125\n",
      "Epoch 339/500000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 5175.2583 - val_loss: 14050.9658\n",
      "Epoch 340/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 5172.0918 - val_loss: 14044.5352\n",
      "Epoch 341/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 5167.9185 - val_loss: 14050.6152\n",
      "Epoch 342/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5163.2368 - val_loss: 14065.1631\n",
      "Epoch 343/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5160.1650 - val_loss: 14085.7090\n",
      "Epoch 344/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 5158.5044 - val_loss: 14116.0957\n",
      "Epoch 345/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 5157.5093 - val_loss: 14163.5479\n",
      "Epoch 346/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5156.7202 - val_loss: 14220.3037\n",
      "Epoch 347/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5155.6641 - val_loss: 14259.5098\n",
      "Epoch 348/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 5154.0713 - val_loss: 14266.1768\n",
      "Epoch 349/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5152.0371 - val_loss: 14245.3633\n",
      "Epoch 350/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5149.9141 - val_loss: 14208.5479\n",
      "Epoch 351/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5147.9834 - val_loss: 14165.9980\n",
      "Epoch 352/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5146.2788 - val_loss: 14124.6455\n",
      "Epoch 353/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5144.7056 - val_loss: 14087.4043\n",
      "Epoch 354/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5143.2036 - val_loss: 14054.0020\n",
      "Epoch 355/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5141.7676 - val_loss: 14023.0537\n",
      "Epoch 356/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5140.3687 - val_loss: 13994.1230\n",
      "Epoch 357/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5138.8613 - val_loss: 13968.6387\n",
      "Epoch 358/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 5137.0850 - val_loss: 13949.0381\n",
      "Epoch 359/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5135.0039 - val_loss: 13937.4082\n",
      "Epoch 360/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5132.7319 - val_loss: 13934.4229\n",
      "Epoch 361/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5130.3525 - val_loss: 13939.2295\n",
      "Epoch 362/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5127.7646 - val_loss: 13950.0684\n",
      "Epoch 363/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5124.7695 - val_loss: 13965.1250\n",
      "Epoch 364/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5121.5664 - val_loss: 13983.2559\n",
      "Epoch 365/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5118.3975 - val_loss: 14004.1465\n",
      "Epoch 366/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 5113.9629 - val_loss: 14028.2705\n",
      "Epoch 367/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5103.8662 - val_loss: 14057.5381\n",
      "Epoch 368/500000\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 5085.9595 - val_loss: 14088.7666\n",
      "Epoch 369/500000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 5081.7725 - val_loss: 14113.4980\n",
      "Epoch 370/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5083.1353 - val_loss: 14128.4521\n",
      "Epoch 371/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5083.1748 - val_loss: 14134.0498\n",
      "Epoch 372/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5081.6582 - val_loss: 14131.4150\n",
      "Epoch 373/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 5078.7407 - val_loss: 14122.2832\n",
      "Epoch 374/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5074.9194 - val_loss: 14110.9746\n",
      "Epoch 375/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5070.4507 - val_loss: 14101.9375\n",
      "Epoch 376/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5065.3267 - val_loss: 14097.2100\n",
      "Epoch 377/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5061.1675 - val_loss: 14097.1123\n",
      "Epoch 378/500000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 5058.5117 - val_loss: 14100.7656\n",
      "Epoch 379/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5057.3706 - val_loss: 14105.8604\n",
      "Epoch 380/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5057.5952 - val_loss: 14109.3877\n",
      "Epoch 381/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5057.8257 - val_loss: 14109.4277\n",
      "Epoch 382/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5057.2490 - val_loss: 14106.0850\n",
      "Epoch 383/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5055.8760 - val_loss: 14100.4980\n",
      "Epoch 384/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5054.1914 - val_loss: 14093.1982\n",
      "Epoch 385/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5052.7085 - val_loss: 14083.8926\n",
      "Epoch 386/500000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 5051.3599 - val_loss: 14073.4336\n",
      "Epoch 387/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5049.7158 - val_loss: 14064.6562\n",
      "Epoch 388/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5047.5347 - val_loss: 14060.8789\n",
      "Epoch 389/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5045.0312 - val_loss: 14063.8721\n",
      "Epoch 390/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5042.8022 - val_loss: 14072.8564\n",
      "Epoch 391/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5041.1729 - val_loss: 14085.0459\n",
      "Epoch 392/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5039.9512 - val_loss: 14097.0312\n",
      "Epoch 393/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5038.9316 - val_loss: 14106.1660\n",
      "Epoch 394/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5038.0742 - val_loss: 14111.1768\n",
      "Epoch 395/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5037.3452 - val_loss: 14111.9043\n",
      "Epoch 396/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5036.6211 - val_loss: 14109.1289\n",
      "Epoch 397/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5035.7827 - val_loss: 14104.3525\n",
      "Epoch 398/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 5034.8442 - val_loss: 14099.3564\n",
      "Epoch 399/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5033.9243 - val_loss: 14095.3896\n",
      "Epoch 400/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5033.0869 - val_loss: 14092.9980\n",
      "Epoch 401/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5032.2798 - val_loss: 14092.2314\n",
      "Epoch 402/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5031.4243 - val_loss: 14093.0254\n",
      "Epoch 403/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5030.4956 - val_loss: 14095.3086\n",
      "Epoch 404/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5029.5225 - val_loss: 14098.7920\n",
      "Epoch 405/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5028.5498 - val_loss: 14102.8271\n",
      "Epoch 406/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5027.6016 - val_loss: 14106.2949\n",
      "Epoch 407/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5026.6401 - val_loss: 14108.0615\n",
      "Epoch 408/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5025.5923 - val_loss: 14107.3867\n",
      "Epoch 409/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 5024.4067 - val_loss: 14104.2881\n",
      "Epoch 410/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5023.1104 - val_loss: 14099.3682\n",
      "Epoch 411/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5021.7568 - val_loss: 14093.7686\n",
      "Epoch 412/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5020.3955 - val_loss: 14088.9287\n",
      "Epoch 413/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 5019.0610 - val_loss: 14086.0205\n",
      "Epoch 414/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5017.7725 - val_loss: 14085.5732\n",
      "Epoch 415/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5016.5322 - val_loss: 14087.3027\n",
      "Epoch 416/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5015.3711 - val_loss: 14090.1699\n",
      "Epoch 417/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5014.3306 - val_loss: 14092.9248\n",
      "Epoch 418/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5013.4219 - val_loss: 14094.4512\n",
      "Epoch 419/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5012.6123 - val_loss: 14094.1406\n",
      "Epoch 420/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5011.8696 - val_loss: 14092.1094\n",
      "Epoch 421/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5011.1699 - val_loss: 14088.9893\n",
      "Epoch 422/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5010.4980 - val_loss: 14085.6270\n",
      "Epoch 423/500000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 5009.8252 - val_loss: 14082.7773\n",
      "Epoch 424/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5009.1313 - val_loss: 14080.8018\n",
      "Epoch 425/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5008.3936 - val_loss: 14079.5918\n",
      "Epoch 426/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5007.5957 - val_loss: 14078.6221\n",
      "Epoch 427/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5006.7026 - val_loss: 14077.1504\n",
      "Epoch 428/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5005.6611 - val_loss: 14074.4287\n",
      "Epoch 429/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5004.3940 - val_loss: 14070.0186\n",
      "Epoch 430/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 5002.9131 - val_loss: 14064.0723\n",
      "Epoch 431/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5001.4443 - val_loss: 14057.3418\n",
      "Epoch 432/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5000.2520 - val_loss: 14050.7461\n",
      "Epoch 433/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4999.3477 - val_loss: 14045.0898\n",
      "Epoch 434/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4998.6094 - val_loss: 14040.9600\n",
      "Epoch 435/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4997.9316 - val_loss: 14038.6455\n",
      "Epoch 436/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 4997.2578 - val_loss: 14037.9941\n",
      "Epoch 437/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4996.5405 - val_loss: 14038.6514\n",
      "Epoch 438/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4995.7178 - val_loss: 14040.0664\n",
      "Epoch 439/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4994.6729 - val_loss: 14041.8242\n",
      "Epoch 440/500000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step - loss: 4993.1094 - val_loss: 14043.6914\n",
      "Epoch 441/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4990.1055 - val_loss: 14045.8066\n",
      "Epoch 442/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4983.2314 - val_loss: 14048.8760\n",
      "Epoch 443/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4975.5547 - val_loss: 14053.8350\n",
      "Epoch 444/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4974.5918 - val_loss: 14060.0225\n",
      "Epoch 445/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4974.9224 - val_loss: 14065.7607\n",
      "Epoch 446/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4974.5298 - val_loss: 14069.5771\n",
      "Epoch 447/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4973.2490 - val_loss: 14071.1650\n",
      "Epoch 448/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4971.6045 - val_loss: 14071.4668\n",
      "Epoch 449/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4970.2075 - val_loss: 14072.1025\n",
      "Epoch 450/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4969.2632 - val_loss: 14074.3545\n",
      "Epoch 451/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4968.5601 - val_loss: 14078.4980\n",
      "Epoch 452/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4967.8477 - val_loss: 14083.7188\n",
      "Epoch 453/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4967.0020 - val_loss: 14088.7686\n",
      "Epoch 454/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4965.9639 - val_loss: 14092.6025\n",
      "Epoch 455/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 4964.7554 - val_loss: 14094.7539\n",
      "Epoch 456/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4963.5151 - val_loss: 14095.2646\n",
      "Epoch 457/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4962.4165 - val_loss: 14094.5439\n",
      "Epoch 458/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 4961.5044 - val_loss: 14092.9111\n",
      "Epoch 459/500000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 4960.7134 - val_loss: 14090.6602\n",
      "Epoch 460/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4959.9644 - val_loss: 14088.0361\n",
      "Epoch 461/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4959.2012 - val_loss: 14085.3008\n",
      "Epoch 462/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4958.4062 - val_loss: 14082.8857\n",
      "Epoch 463/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4957.5708 - val_loss: 14081.1758\n",
      "Epoch 464/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4956.7163 - val_loss: 14080.3496\n",
      "Epoch 465/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4955.8838 - val_loss: 14080.2656\n",
      "Epoch 466/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4955.1147 - val_loss: 14080.5273\n",
      "Epoch 467/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4954.4224 - val_loss: 14080.7959\n",
      "Epoch 468/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4953.7656 - val_loss: 14080.8730\n",
      "Epoch 469/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 4953.0732 - val_loss: 14080.8105\n",
      "Epoch 470/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4952.3193 - val_loss: 14080.7480\n",
      "Epoch 471/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4951.5308 - val_loss: 14080.7920\n",
      "Epoch 472/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4950.7539 - val_loss: 14080.8457\n",
      "Epoch 473/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 4950.0239 - val_loss: 14080.7705\n",
      "Epoch 474/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4949.3403 - val_loss: 14080.4600\n",
      "Epoch 475/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4948.6904 - val_loss: 14079.9531\n",
      "Epoch 476/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4948.0562 - val_loss: 14079.5127\n",
      "Epoch 477/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4947.4238 - val_loss: 14079.4111\n",
      "Epoch 478/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 4946.7915 - val_loss: 14079.9307\n",
      "Epoch 479/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4946.1768 - val_loss: 14081.0811\n",
      "Epoch 480/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4945.6035 - val_loss: 14082.6396\n",
      "Epoch 481/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 4945.0859 - val_loss: 14084.2646\n",
      "Epoch 482/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4944.6191 - val_loss: 14085.6025\n",
      "Epoch 483/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4944.1758 - val_loss: 14086.5049\n",
      "Epoch 484/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 4943.7407 - val_loss: 14087.0371\n",
      "Epoch 485/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 4943.2983 - val_loss: 14087.4814\n",
      "Epoch 486/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4942.8452 - val_loss: 14088.1758\n",
      "Epoch 487/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4942.3882 - val_loss: 14089.4502\n",
      "Epoch 488/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4941.9238 - val_loss: 14091.5332\n",
      "Epoch 489/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4941.4629 - val_loss: 14094.6006\n",
      "Epoch 490/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 4941.0073 - val_loss: 14098.6689\n",
      "Epoch 491/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 4940.5605 - val_loss: 14103.7002\n",
      "Epoch 492/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 4940.1167 - val_loss: 14109.5303\n",
      "Epoch 493/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4939.6777 - val_loss: 14116.0039\n",
      "Epoch 494/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4939.2417 - val_loss: 14122.8623\n",
      "Epoch 495/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4938.8052 - val_loss: 14129.9502\n",
      "Epoch 496/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 4938.3730 - val_loss: 14137.2178\n",
      "Epoch 497/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4937.9443 - val_loss: 14144.6885\n",
      "Epoch 498/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4937.5176 - val_loss: 14152.4473\n",
      "Epoch 499/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4937.0908 - val_loss: 14160.5352\n",
      "Epoch 500/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4936.6685 - val_loss: 14168.8789\n",
      "Epoch 501/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4936.2461 - val_loss: 14177.2959\n",
      "Epoch 502/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 4935.8257 - val_loss: 14185.5645\n",
      "Epoch 503/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4935.4067 - val_loss: 14193.4775\n",
      "Epoch 504/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4934.9888 - val_loss: 14200.8730\n",
      "Epoch 505/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4934.5718 - val_loss: 14207.7197\n",
      "Epoch 506/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4934.1562 - val_loss: 14214.0254\n",
      "Epoch 507/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4933.7407 - val_loss: 14219.8193\n",
      "Epoch 508/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 4933.3232 - val_loss: 14225.1240\n",
      "Epoch 509/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4932.9038 - val_loss: 14229.9375\n",
      "Epoch 510/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4932.4814 - val_loss: 14234.3320\n",
      "Epoch 511/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4932.0571 - val_loss: 14238.3535\n",
      "Epoch 512/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 4931.6260 - val_loss: 14241.9990\n",
      "Epoch 513/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4931.1919 - val_loss: 14245.3271\n",
      "Epoch 514/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4930.7510 - val_loss: 14248.3789\n",
      "Epoch 515/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 4930.3013 - val_loss: 14251.1807\n",
      "Epoch 516/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4929.8428 - val_loss: 14253.7246\n",
      "Epoch 517/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4929.3721 - val_loss: 14256.1123\n",
      "Epoch 518/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4928.8857 - val_loss: 14258.3740\n",
      "Epoch 519/500000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 4928.3813 - val_loss: 14260.5820\n",
      "Epoch 520/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 4927.8574 - val_loss: 14262.7793\n",
      "Epoch 521/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 4927.3169 - val_loss: 14264.9932\n",
      "Epoch 522/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4926.7622 - val_loss: 14267.2451\n",
      "Epoch 523/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4926.2026 - val_loss: 14269.4746\n",
      "Epoch 524/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4925.6421 - val_loss: 14271.7207\n",
      "Epoch 525/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4925.0869 - val_loss: 14273.9502\n",
      "Epoch 526/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4924.5342 - val_loss: 14276.1934\n",
      "Epoch 527/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4923.9692 - val_loss: 14278.4189\n",
      "Epoch 528/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4923.3809 - val_loss: 14280.5371\n",
      "Epoch 529/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4922.7402 - val_loss: 14282.5439\n",
      "Epoch 530/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4922.0181 - val_loss: 14284.2910\n",
      "Epoch 531/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4921.1870 - val_loss: 14285.7080\n",
      "Epoch 532/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4920.2388 - val_loss: 14286.7080\n",
      "Epoch 533/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4919.2007 - val_loss: 14287.3330\n",
      "Epoch 534/500000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 4917.9961 - val_loss: 14287.7666\n",
      "Epoch 535/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4916.2114 - val_loss: 14288.3936\n",
      "Epoch 536/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 4913.5571 - val_loss: 14289.6592\n",
      "Epoch 537/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 4910.9985 - val_loss: 14291.9590\n",
      "Epoch 538/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4909.7734 - val_loss: 14295.5107\n",
      "Epoch 539/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4909.5205 - val_loss: 14300.2207\n",
      "Epoch 540/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4909.4614 - val_loss: 14305.7227\n",
      "Epoch 541/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4909.2344 - val_loss: 14311.3730\n",
      "Epoch 542/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 4908.8101 - val_loss: 14316.4395\n",
      "Epoch 543/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4908.2559 - val_loss: 14320.4043\n",
      "Epoch 544/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4907.5972 - val_loss: 14322.9248\n",
      "Epoch 545/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4906.8667 - val_loss: 14323.9980\n",
      "Epoch 546/500000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 4906.1592 - val_loss: 14323.8438\n",
      "Epoch 547/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4905.5713 - val_loss: 14322.9121\n",
      "Epoch 548/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 4905.1118 - val_loss: 14321.5977\n",
      "Epoch 549/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 4904.7168 - val_loss: 14320.2549\n",
      "Epoch 550/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4904.3325 - val_loss: 14319.0840\n",
      "Epoch 551/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4903.9565 - val_loss: 14318.1846\n",
      "Epoch 552/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4903.6074 - val_loss: 14317.4805\n",
      "Epoch 553/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4903.2891 - val_loss: 14316.9072\n",
      "Epoch 554/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4902.9668 - val_loss: 14316.3945\n",
      "Epoch 555/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4902.6279 - val_loss: 14315.9482\n",
      "Epoch 556/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4902.2852 - val_loss: 14315.5596\n",
      "Epoch 557/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4901.9526 - val_loss: 14315.3037\n",
      "Epoch 558/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4901.6196 - val_loss: 14315.1758\n",
      "Epoch 559/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4901.2681 - val_loss: 14315.1729\n",
      "Epoch 560/500000\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 4900.8975 - val_loss: 14315.2764\n",
      "Epoch 561/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4900.5215 - val_loss: 14315.4727\n",
      "Epoch 562/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4900.1533 - val_loss: 14315.6914\n",
      "Epoch 563/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4899.7861 - val_loss: 14315.9668\n",
      "Epoch 564/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4899.4141 - val_loss: 14316.3350\n",
      "Epoch 565/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4899.0332 - val_loss: 14316.8623\n",
      "Epoch 566/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 4898.6494 - val_loss: 14317.6270\n",
      "Epoch 567/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4898.2632 - val_loss: 14318.7227\n",
      "Epoch 568/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4897.8647 - val_loss: 14320.2871\n",
      "Epoch 569/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4897.4473 - val_loss: 14322.4502\n",
      "Epoch 570/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4897.0112 - val_loss: 14325.4307\n",
      "Epoch 571/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4896.5508 - val_loss: 14329.5898\n",
      "Epoch 572/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 4896.0557 - val_loss: 14335.5127\n",
      "Epoch 573/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4895.5024 - val_loss: 14344.1699\n",
      "Epoch 574/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4894.8672 - val_loss: 14357.0264\n",
      "Epoch 575/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4894.1387 - val_loss: 14375.6660\n",
      "Epoch 576/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4893.3589 - val_loss: 14399.8145\n",
      "Epoch 577/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4892.6777 - val_loss: 14424.4375\n",
      "Epoch 578/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4892.2251 - val_loss: 14442.3477\n",
      "Epoch 579/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4891.9033 - val_loss: 14450.0371\n",
      "Epoch 580/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4891.5361 - val_loss: 14447.7188\n",
      "Epoch 581/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4891.0474 - val_loss: 14436.7939\n",
      "Epoch 582/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4890.4307 - val_loss: 14419.1572\n",
      "Epoch 583/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4889.7134 - val_loss: 14397.8105\n",
      "Epoch 584/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4888.9604 - val_loss: 14376.7588\n",
      "Epoch 585/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4888.2490 - val_loss: 14359.5420\n",
      "Epoch 586/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4887.6265 - val_loss: 14347.8145\n",
      "Epoch 587/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4887.0908 - val_loss: 14341.5586\n",
      "Epoch 588/500000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step - loss: 4886.6221 - val_loss: 14339.9102\n",
      "Epoch 589/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4886.2056 - val_loss: 14341.7314\n",
      "Epoch 590/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4885.8350 - val_loss: 14345.8965\n",
      "Epoch 591/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4885.4976 - val_loss: 14351.2627\n",
      "Epoch 592/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4885.1782 - val_loss: 14356.7354\n",
      "Epoch 593/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4884.8623 - val_loss: 14361.3369\n",
      "Epoch 594/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4884.5439 - val_loss: 14364.4355\n",
      "Epoch 595/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4884.2251 - val_loss: 14365.7998\n",
      "Epoch 596/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4883.9175 - val_loss: 14365.6523\n",
      "Epoch 597/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 4883.6221 - val_loss: 14364.5459\n",
      "Epoch 598/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4883.3379 - val_loss: 14363.0693\n",
      "Epoch 599/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4883.0576 - val_loss: 14361.7529\n",
      "Epoch 600/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4882.7739 - val_loss: 14360.9238\n",
      "Epoch 601/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4882.4854 - val_loss: 14360.7354\n",
      "Epoch 602/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4882.1914 - val_loss: 14361.1416\n",
      "Epoch 603/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4881.9009 - val_loss: 14361.8877\n",
      "Epoch 604/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 4881.6206 - val_loss: 14362.6807\n",
      "Epoch 605/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4881.3457 - val_loss: 14363.2295\n",
      "Epoch 606/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4881.0767 - val_loss: 14363.3418\n",
      "Epoch 607/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4880.8091 - val_loss: 14362.9951\n",
      "Epoch 608/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4880.5405 - val_loss: 14362.4121\n",
      "Epoch 609/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4880.2729 - val_loss: 14361.8145\n",
      "Epoch 610/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4880.0083 - val_loss: 14361.3701\n",
      "Epoch 611/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4879.7456 - val_loss: 14361.1670\n",
      "Epoch 612/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4879.4883 - val_loss: 14361.2334\n",
      "Epoch 613/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 4879.2329 - val_loss: 14361.5059\n",
      "Epoch 614/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4878.9766 - val_loss: 14361.9697\n",
      "Epoch 615/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4878.7207 - val_loss: 14362.5508\n",
      "Epoch 616/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4878.4663 - val_loss: 14363.1504\n",
      "Epoch 617/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4878.2119 - val_loss: 14363.6123\n",
      "Epoch 618/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4877.9609 - val_loss: 14363.8252\n",
      "Epoch 619/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4877.7100 - val_loss: 14363.7402\n",
      "Epoch 620/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4877.4619 - val_loss: 14363.3848\n",
      "Epoch 621/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4877.2139 - val_loss: 14362.8447\n",
      "Epoch 622/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4876.9688 - val_loss: 14362.2656\n",
      "Epoch 623/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4876.7251 - val_loss: 14361.7998\n",
      "Epoch 624/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4876.4824 - val_loss: 14361.5713\n",
      "Epoch 625/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4876.2417 - val_loss: 14361.6475\n",
      "Epoch 626/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4876.0005 - val_loss: 14362.0205\n",
      "Epoch 627/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 4875.7617 - val_loss: 14362.6934\n",
      "Epoch 628/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4875.5220 - val_loss: 14363.5332\n",
      "Epoch 629/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4875.2842 - val_loss: 14364.3662\n",
      "Epoch 630/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4875.0498 - val_loss: 14365.0830\n",
      "Epoch 631/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4874.8184 - val_loss: 14365.5439\n",
      "Epoch 632/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4874.5869 - val_loss: 14365.7129\n",
      "Epoch 633/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4874.3574 - val_loss: 14365.6494\n",
      "Epoch 634/500000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 4874.1294 - val_loss: 14365.4150\n",
      "Epoch 635/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4873.9038 - val_loss: 14365.1211\n",
      "Epoch 636/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4873.6797 - val_loss: 14364.9023\n",
      "Epoch 637/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4873.4575 - val_loss: 14364.8145\n",
      "Epoch 638/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4873.2363 - val_loss: 14364.9072\n",
      "Epoch 639/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4873.0195 - val_loss: 14365.1650\n",
      "Epoch 640/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4872.8013 - val_loss: 14365.5771\n",
      "Epoch 641/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4872.5854 - val_loss: 14366.0771\n",
      "Epoch 642/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4872.3706 - val_loss: 14366.6602\n",
      "Epoch 643/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4872.1562 - val_loss: 14367.3096\n",
      "Epoch 644/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4871.9438 - val_loss: 14367.9727\n",
      "Epoch 645/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4871.7324 - val_loss: 14368.6416\n",
      "Epoch 646/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4871.5205 - val_loss: 14369.2803\n",
      "Epoch 647/500000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 4871.3091 - val_loss: 14369.9102\n",
      "Epoch 648/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4871.1011 - val_loss: 14370.4746\n",
      "Epoch 649/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4870.8926 - val_loss: 14371.0439\n",
      "Epoch 650/500000\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 4870.6831 - val_loss: 14371.6455\n",
      "Epoch 651/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4870.4756 - val_loss: 14372.3086\n",
      "Epoch 652/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4870.2695 - val_loss: 14373.0127\n",
      "Epoch 653/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4870.0649 - val_loss: 14373.7773\n",
      "Epoch 654/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4869.8594 - val_loss: 14374.5850\n",
      "Epoch 655/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4869.6548 - val_loss: 14375.4580\n",
      "Epoch 656/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4869.4502 - val_loss: 14376.3545\n",
      "Epoch 657/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4869.2461 - val_loss: 14377.2773\n",
      "Epoch 658/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4869.0449 - val_loss: 14378.2227\n",
      "Epoch 659/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4868.8418 - val_loss: 14379.2061\n",
      "Epoch 660/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4868.6401 - val_loss: 14380.1982\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "model_mlp = Sequential()\n",
    "model_mlp.add(Dense(55, input_dim=X_train.shape[1], activation='sigmoid',kernel_initializer='random_uniform'))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.5))\n",
    "model_mlp.add(Dense(1, activation='linear'))\n",
    "opt = SGD(learning_rate=0.01,momentum=0.9)\n",
    "model_mlp.compile(optimizer=opt, loss='mse')\n",
    "model_mlp.summary()\n",
    "early_stopping_cb = EarlyStopping(patience=300,restore_best_weights=True)\n",
    "run_logdir=r\"C:\\Users\\Deepak Tripathi\\Desktop\\rossman\\log\"\n",
    "history = model_mlp.fit(np.array(X_train), y_train, batch_size=len(X_train),validation_data=(X_val, y_val) ,epochs=500000, verbose=1,callbacks=[early_stopping_cb])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE= 18621.85790795697\n",
      "R2= 0.9850618587628973\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "      <th>Parameter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>506.638072</td>\n",
       "      <td>0.782245</td>\n",
       "      <td>{'alpha': 10.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>506.565917</td>\n",
       "      <td>0.782307</td>\n",
       "      <td>{'alpha': 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestRegressor</th>\n",
       "      <td>561.657658</td>\n",
       "      <td>0.732382</td>\n",
       "      <td>{'max_depth': 10, 'max_features': 'sqrt', 'n_e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostingRegressor</th>\n",
       "      <td>410.441842</td>\n",
       "      <td>0.857086</td>\n",
       "      <td>{'max_depth': 4, 'max_features': 'sqrt', 'n_es...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVR</th>\n",
       "      <td>525.606852</td>\n",
       "      <td>0.765634</td>\n",
       "      <td>{'C': 200, 'gamma': 1e-07, 'kernel': 'linear'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP</th>\n",
       "      <td>136.461928</td>\n",
       "      <td>0.985062</td>\n",
       "      <td>{'No of Nuerons': 55, 'acivation': ['sigmoid',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic_and_MLP</th>\n",
       "      <td>136.461928</td>\n",
       "      <td>0.985062</td>\n",
       "      <td>{'No of Nuerons': 55, 'acivation': ['sigmoid',...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 RMSE        R2  \\\n",
       "Ridge                      506.638072  0.782245   \n",
       "Lasso                      506.565917  0.782307   \n",
       "RandomForestRegressor      561.657658  0.732382   \n",
       "GradientBoostingRegressor  410.441842  0.857086   \n",
       "SVR                        525.606852  0.765634   \n",
       "MLP                        136.461928  0.985062   \n",
       "Logistic_and_MLP           136.461928  0.985062   \n",
       "\n",
       "                                                                   Parameter  \n",
       "Ridge                                                        {'alpha': 10.0}  \n",
       "Lasso                                                         {'alpha': 1.0}  \n",
       "RandomForestRegressor      {'max_depth': 10, 'max_features': 'sqrt', 'n_e...  \n",
       "GradientBoostingRegressor  {'max_depth': 4, 'max_features': 'sqrt', 'n_es...  \n",
       "SVR                           {'C': 200, 'gamma': 1e-07, 'kernel': 'linear'}  \n",
       "MLP                        {'No of Nuerons': 55, 'acivation': ['sigmoid',...  \n",
       "Logistic_and_MLP           {'No of Nuerons': 55, 'acivation': ['sigmoid',...  "
      ]
     },
     "execution_count": 1297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_pred = model_mlp.predict(X_test)\n",
    "MSE=mean_squared_error(y_test,mlp_pred)\n",
    "RMSE=math.sqrt(MSE)\n",
    "r2=r2_score(y_test,mlp_pred)\n",
    "print('MSE=',MSE)\n",
    "print('R2=',r2)\n",
    "print('-'*50)\n",
    "\n",
    "model='MLP'\n",
    "algo.loc[model]=[RMSE,r2,{'No of Nuerons':55,'acivation':['sigmoid','linear']}]\n",
    "algo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now on test split, use logistic regression first to classify if a person will claim or not.  if he claims, only then predict how much will he claim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if performence improves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1298,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_pred=logistic.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1299,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_pred = model_mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make value equal to 0 where a person will not claim according to logisitc regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1300,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_pred[logistic_pred==False]=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE= 17911.996944755516\n",
      "R2= 0.9856312972893548\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "      <th>Parameter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>506.638072</td>\n",
       "      <td>0.782245</td>\n",
       "      <td>{'alpha': 10.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>506.565917</td>\n",
       "      <td>0.782307</td>\n",
       "      <td>{'alpha': 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestRegressor</th>\n",
       "      <td>561.657658</td>\n",
       "      <td>0.732382</td>\n",
       "      <td>{'max_depth': 10, 'max_features': 'sqrt', 'n_e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostingRegressor</th>\n",
       "      <td>410.441842</td>\n",
       "      <td>0.857086</td>\n",
       "      <td>{'max_depth': 4, 'max_features': 'sqrt', 'n_es...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVR</th>\n",
       "      <td>525.606852</td>\n",
       "      <td>0.765634</td>\n",
       "      <td>{'C': 200, 'gamma': 1e-07, 'kernel': 'linear'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP</th>\n",
       "      <td>136.461928</td>\n",
       "      <td>0.985062</td>\n",
       "      <td>{'No of Nuerons': 55, 'acivation': ['sigmoid',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic_and_MLP</th>\n",
       "      <td>133.835709</td>\n",
       "      <td>0.985631</td>\n",
       "      <td>{'No of Nuerons': 55, 'acivation': ['sigmoid',...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 RMSE        R2  \\\n",
       "Ridge                      506.638072  0.782245   \n",
       "Lasso                      506.565917  0.782307   \n",
       "RandomForestRegressor      561.657658  0.732382   \n",
       "GradientBoostingRegressor  410.441842  0.857086   \n",
       "SVR                        525.606852  0.765634   \n",
       "MLP                        136.461928  0.985062   \n",
       "Logistic_and_MLP           133.835709  0.985631   \n",
       "\n",
       "                                                                   Parameter  \n",
       "Ridge                                                        {'alpha': 10.0}  \n",
       "Lasso                                                         {'alpha': 1.0}  \n",
       "RandomForestRegressor      {'max_depth': 10, 'max_features': 'sqrt', 'n_e...  \n",
       "GradientBoostingRegressor  {'max_depth': 4, 'max_features': 'sqrt', 'n_es...  \n",
       "SVR                           {'C': 200, 'gamma': 1e-07, 'kernel': 'linear'}  \n",
       "MLP                        {'No of Nuerons': 55, 'acivation': ['sigmoid',...  \n",
       "Logistic_and_MLP           {'No of Nuerons': 55, 'acivation': ['sigmoid',...  "
      ]
     },
     "execution_count": 1301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE=mean_squared_error(y_test,mlp_pred)\n",
    "RMSE=math.sqrt(MSE)\n",
    "r2=r2_score(y_test,mlp_pred)\n",
    "print('MSE=',MSE)\n",
    "print('R2=',r2)\n",
    "print('-'*50)\n",
    "\n",
    "model='Logistic_and_MLP'\n",
    "algo.loc[model]=[RMSE,r2,{'No of Nuerons':55,'acivation':['sigmoid','linear']}]\n",
    "algo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So our Final Modal is Logistic+MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets again train these 2 modals on whole train data then use the final modal for predicting test split created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 12)"
      ]
     },
     "execution_count": 1225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(r\"C:\\Users\\Deepak Tripathi\\Desktop\\assignment machine learning\\part3\\CE802_P3_Data.csv\")\n",
    "\n",
    "df['F15']=df['F15'].map({'Very low':0.0,'Low':1.0,'Medium':2.0,'High':3.0,'Very high':4.0})\n",
    "\n",
    "df=df[['F2','F4','F6','F7','F9','F11','F12','F14','F15','Target']]\n",
    "\n",
    "df1=df.drop(['Target'],axis=1)\n",
    "y=df['Target']\n",
    "\n",
    "\n",
    "cat=df1.dtypes==object\n",
    "\n",
    "ct = ColumnTransformer(\n",
    "     [(\"ohe\", OneHotEncoder(handle_unknown='ignore'), cat),\n",
    "      (\"norm\", StandardScaler(), ~cat)])\n",
    "\n",
    "ct.fit(df1)\n",
    "\n",
    "x=ct.transform(df1)\n",
    "y_c=y!=0\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 1226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_c=LogisticRegression()\n",
    "clf_c.fit(x,y_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_73\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_140 (Dense)            (None, 55)                715       \n",
      "_________________________________________________________________\n",
      "dense_141 (Dense)            (None, 1)                 56        \n",
      "=================================================================\n",
      "Total params: 771\n",
      "Trainable params: 771\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/500000\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2057156.1250 - val_loss: 1515982.6250\n",
      "Epoch 2/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1618186.6250 - val_loss: 843844.1875\n",
      "Epoch 3/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 826781.8750 - val_loss: 602670.5000\n",
      "Epoch 4/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 542441.6875 - val_loss: 523019.9375\n",
      "Epoch 5/500000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 444485.2500 - val_loss: 407145.5938\n",
      "Epoch 6/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 333299.5938 - val_loss: 283508.3438\n",
      "Epoch 7/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 241193.9375 - val_loss: 226108.1406\n",
      "Epoch 8/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 198905.6250 - val_loss: 185173.5156\n",
      "Epoch 9/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 170613.7188 - val_loss: 164610.6562\n",
      "Epoch 10/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 141384.5156 - val_loss: 157180.2344\n",
      "Epoch 11/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 115702.3203 - val_loss: 160376.0312\n",
      "Epoch 12/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 107793.3516 - val_loss: 153251.8594\n",
      "Epoch 13/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 104678.9922 - val_loss: 128924.9609\n",
      "Epoch 14/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 93547.0000 - val_loss: 100617.7422\n",
      "Epoch 15/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 77439.3984 - val_loss: 88180.1719\n",
      "Epoch 16/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 68901.7422 - val_loss: 87451.1016\n",
      "Epoch 17/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 67518.1484 - val_loss: 85686.0469\n",
      "Epoch 18/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 64526.1328 - val_loss: 74220.8906\n",
      "Epoch 19/500000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 56578.7305 - val_loss: 64129.3086\n",
      "Epoch 20/500000\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 51597.4219 - val_loss: 61715.9062\n",
      "Epoch 21/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 50978.2578 - val_loss: 56102.8516\n",
      "Epoch 22/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 50331.3320 - val_loss: 48495.5664\n",
      "Epoch 23/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 45787.3789 - val_loss: 39744.9492\n",
      "Epoch 24/500000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 39787.9062 - val_loss: 36700.3359\n",
      "Epoch 25/500000\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 35773.5703 - val_loss: 33211.5078\n",
      "Epoch 26/500000\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 33056.9844 - val_loss: 28907.8008\n",
      "Epoch 27/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 29805.3340 - val_loss: 23489.9824\n",
      "Epoch 28/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 25759.0137 - val_loss: 21295.0859\n",
      "Epoch 29/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 23304.3398 - val_loss: 21401.7969\n",
      "Epoch 30/500000\n",
      "1/1 [==============================] - ETA: 0s - loss: 22464.882 - 0s 21ms/step - loss: 22464.8828 - val_loss: 20750.0078\n",
      "Epoch 31/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 21771.1250 - val_loss: 19498.6738\n",
      "Epoch 32/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 20136.6602 - val_loss: 19514.4590\n",
      "Epoch 33/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 18421.9160 - val_loss: 19228.2441\n",
      "Epoch 34/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 17732.3574 - val_loss: 18843.1152\n",
      "Epoch 35/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 17511.7441 - val_loss: 18975.0195\n",
      "Epoch 36/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 16552.4941 - val_loss: 18750.4316\n",
      "Epoch 37/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 15352.4766 - val_loss: 18542.8535\n",
      "Epoch 38/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 14707.2041 - val_loss: 18181.7852\n",
      "Epoch 39/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 14425.2129 - val_loss: 17288.5527\n",
      "Epoch 40/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 13876.9219 - val_loss: 16527.1113\n",
      "Epoch 41/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 13060.1670 - val_loss: 16365.8584\n",
      "Epoch 42/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 12450.2373 - val_loss: 16491.9609\n",
      "Epoch 43/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 12054.0000 - val_loss: 16563.1895\n",
      "Epoch 44/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 11538.8789 - val_loss: 16351.5752\n",
      "Epoch 45/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 11218.1250 - val_loss: 16329.8281\n",
      "Epoch 46/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 10864.1250 - val_loss: 16402.4375\n",
      "Epoch 47/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 10581.3965 - val_loss: 16595.9512\n",
      "Epoch 48/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 10488.1885 - val_loss: 16420.9492\n",
      "Epoch 49/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 10207.3877 - val_loss: 15786.1768\n",
      "Epoch 50/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 9807.3662 - val_loss: 15388.6650\n",
      "Epoch 51/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 9489.5068 - val_loss: 15216.3604\n",
      "Epoch 52/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 9323.0381 - val_loss: 15230.5967\n",
      "Epoch 53/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 9162.7588 - val_loss: 15428.5400\n",
      "Epoch 54/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 8935.2988 - val_loss: 15845.5479\n",
      "Epoch 55/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 8698.7461 - val_loss: 16263.7969\n",
      "Epoch 56/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 8509.3379 - val_loss: 16269.9316\n",
      "Epoch 57/500000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 8347.3906 - val_loss: 15912.8379\n",
      "Epoch 58/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 8215.5811 - val_loss: 15430.3750\n",
      "Epoch 59/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 8138.5527 - val_loss: 15098.2998\n",
      "Epoch 60/500000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 8062.0605 - val_loss: 14914.1367\n",
      "Epoch 61/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 7913.1562 - val_loss: 14735.4336\n",
      "Epoch 62/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 7779.5439 - val_loss: 14633.0254\n",
      "Epoch 63/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 7715.0864 - val_loss: 14625.6768\n",
      "Epoch 64/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 7618.1528 - val_loss: 14690.7803\n",
      "Epoch 65/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 7453.1289 - val_loss: 14757.5303\n",
      "Epoch 66/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 7357.2295 - val_loss: 14792.7803\n",
      "Epoch 67/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 7261.4673 - val_loss: 14820.2734\n",
      "Epoch 68/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 7147.1099 - val_loss: 14877.4785\n",
      "Epoch 69/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 7039.2837 - val_loss: 14940.7471\n",
      "Epoch 70/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 6931.0444 - val_loss: 14911.1836\n",
      "Epoch 71/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 6822.8735 - val_loss: 14620.1602\n",
      "Epoch 72/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 6743.8696 - val_loss: 14140.0771\n",
      "Epoch 73/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 6713.1787 - val_loss: 13862.6914\n",
      "Epoch 74/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 6674.4702 - val_loss: 13701.2529\n",
      "Epoch 75/500000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 6617.5532 - val_loss: 13626.6221\n",
      "Epoch 76/500000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 6559.7031 - val_loss: 13632.1445\n",
      "Epoch 77/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 6503.7021 - val_loss: 13641.1270\n",
      "Epoch 78/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 6444.1616 - val_loss: 13616.3604\n",
      "Epoch 79/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 6368.7231 - val_loss: 13571.6787\n",
      "Epoch 80/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 6307.0142 - val_loss: 13507.7061\n",
      "Epoch 81/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 6244.4482 - val_loss: 13424.7988\n",
      "Epoch 82/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 6205.6953 - val_loss: 13330.7900\n",
      "Epoch 83/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 6172.4883 - val_loss: 13238.5234\n",
      "Epoch 84/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 6139.0498 - val_loss: 13171.4688\n",
      "Epoch 85/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 6108.8198 - val_loss: 13118.4385\n",
      "Epoch 86/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 6074.8970 - val_loss: 13058.4229\n",
      "Epoch 87/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 6044.3550 - val_loss: 12988.1416\n",
      "Epoch 88/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 6002.7539 - val_loss: 12922.6689\n",
      "Epoch 89/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 5940.7075 - val_loss: 12888.9570\n",
      "Epoch 90/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5875.0029 - val_loss: 12867.3467\n",
      "Epoch 91/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5845.4595 - val_loss: 12813.3838\n",
      "Epoch 92/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5813.0503 - val_loss: 12721.0439\n",
      "Epoch 93/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 5764.6123 - val_loss: 12638.9639\n",
      "Epoch 94/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5721.7871 - val_loss: 12592.1592\n",
      "Epoch 95/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5684.5239 - val_loss: 12527.6699\n",
      "Epoch 96/500000\n",
      "1/1 [==============================] - ETA: 0s - loss: 5645.81 - 0s 19ms/step - loss: 5645.8193 - val_loss: 12445.2021\n",
      "Epoch 97/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5589.1572 - val_loss: 12340.4033\n",
      "Epoch 98/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5526.9688 - val_loss: 12234.2793\n",
      "Epoch 99/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 5489.6606 - val_loss: 12143.2764\n",
      "Epoch 100/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5463.5459 - val_loss: 12047.3896\n",
      "Epoch 101/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5432.0942 - val_loss: 11949.9131\n",
      "Epoch 102/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5408.7139 - val_loss: 11872.1865\n",
      "Epoch 103/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5387.6621 - val_loss: 11826.1592\n",
      "Epoch 104/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5364.4146 - val_loss: 11811.6104\n",
      "Epoch 105/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5342.9258 - val_loss: 11823.5010\n",
      "Epoch 106/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5317.0298 - val_loss: 11839.5273\n",
      "Epoch 107/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5281.4258 - val_loss: 11834.4805\n",
      "Epoch 108/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5238.9497 - val_loss: 11867.9277\n",
      "Epoch 109/500000\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 5197.9287 - val_loss: 11923.0029\n",
      "Epoch 110/500000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 5162.5127 - val_loss: 11959.0928\n",
      "Epoch 111/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 5141.6050 - val_loss: 11977.3721\n",
      "Epoch 112/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5127.5972 - val_loss: 11986.6953\n",
      "Epoch 113/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5115.1406 - val_loss: 11991.0479\n",
      "Epoch 114/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 5106.9121 - val_loss: 11997.1396\n",
      "Epoch 115/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5096.0039 - val_loss: 12001.8682\n",
      "Epoch 116/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 5083.8843 - val_loss: 12000.2334\n",
      "Epoch 117/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5069.8779 - val_loss: 11991.7188\n",
      "Epoch 118/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5049.6558 - val_loss: 11983.3936\n",
      "Epoch 119/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5021.9473 - val_loss: 11987.9912\n",
      "Epoch 120/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4997.8628 - val_loss: 11980.8369\n",
      "Epoch 121/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4976.1099 - val_loss: 11946.3818\n",
      "Epoch 122/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 4959.8784 - val_loss: 11919.4814\n",
      "Epoch 123/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4950.6973 - val_loss: 11908.0615\n",
      "Epoch 124/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4939.2617 - val_loss: 11897.0732\n",
      "Epoch 125/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4925.5273 - val_loss: 11879.7988\n",
      "Epoch 126/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4911.1338 - val_loss: 11854.7490\n",
      "Epoch 127/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4895.8984 - val_loss: 11819.8438\n",
      "Epoch 128/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4876.1934 - val_loss: 11778.8018\n",
      "Epoch 129/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4862.6172 - val_loss: 11735.6445\n",
      "Epoch 130/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4851.0239 - val_loss: 11692.9199\n",
      "Epoch 131/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4840.0469 - val_loss: 11656.9180\n",
      "Epoch 132/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4829.5298 - val_loss: 11634.8789\n",
      "Epoch 133/500000\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 4819.6816 - val_loss: 11632.1074\n",
      "Epoch 134/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 4810.6924 - val_loss: 11651.2520\n",
      "Epoch 135/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4800.9580 - val_loss: 11690.2285\n",
      "Epoch 136/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4789.3037 - val_loss: 11744.4756\n",
      "Epoch 137/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4775.0176 - val_loss: 11807.8496\n",
      "Epoch 138/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4756.7720 - val_loss: 11871.4688\n",
      "Epoch 139/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4737.5430 - val_loss: 11919.2471\n",
      "Epoch 140/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4721.9937 - val_loss: 11938.4629\n",
      "Epoch 141/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4709.1235 - val_loss: 11938.1689\n",
      "Epoch 142/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4696.1973 - val_loss: 11930.0400\n",
      "Epoch 143/500000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step - loss: 4682.9775 - val_loss: 11913.6426\n",
      "Epoch 144/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4672.8438 - val_loss: 11890.2812\n",
      "Epoch 145/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 4663.0215 - val_loss: 11862.6387\n",
      "Epoch 146/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4653.8413 - val_loss: 11832.6562\n",
      "Epoch 147/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 4646.0889 - val_loss: 11800.7422\n",
      "Epoch 148/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4639.1406 - val_loss: 11765.8184\n",
      "Epoch 149/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4632.4902 - val_loss: 11727.1289\n",
      "Epoch 150/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4625.2969 - val_loss: 11686.7529\n",
      "Epoch 151/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4617.2061 - val_loss: 11648.3203\n",
      "Epoch 152/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4608.7114 - val_loss: 11612.9033\n",
      "Epoch 153/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 4600.2002 - val_loss: 11578.1777\n",
      "Epoch 154/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4592.5986 - val_loss: 11541.0996\n",
      "Epoch 155/500000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 4586.5757 - val_loss: 11499.6582\n",
      "Epoch 156/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4581.0654 - val_loss: 11453.7637\n",
      "Epoch 157/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4575.5547 - val_loss: 11405.8936\n",
      "Epoch 158/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4569.8149 - val_loss: 11360.2168\n",
      "Epoch 159/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4563.3569 - val_loss: 11320.0947\n",
      "Epoch 160/500000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 4555.8110 - val_loss: 11286.0498\n",
      "Epoch 161/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4546.6006 - val_loss: 11254.4463\n",
      "Epoch 162/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4533.5796 - val_loss: 11217.9141\n",
      "Epoch 163/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4520.4409 - val_loss: 11176.7979\n",
      "Epoch 164/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4514.4746 - val_loss: 11134.1221\n",
      "Epoch 165/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4509.4717 - val_loss: 11094.5674\n",
      "Epoch 166/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4502.9209 - val_loss: 11062.9258\n",
      "Epoch 167/500000\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 4494.1587 - val_loss: 11041.5020\n",
      "Epoch 168/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4484.0107 - val_loss: 11030.8945\n",
      "Epoch 169/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4472.8857 - val_loss: 11030.5723\n",
      "Epoch 170/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4458.4287 - val_loss: 11038.6592\n",
      "Epoch 171/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4442.0063 - val_loss: 11051.0703\n",
      "Epoch 172/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4428.3696 - val_loss: 11061.6572\n",
      "Epoch 173/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4418.2222 - val_loss: 11064.4805\n",
      "Epoch 174/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4409.3789 - val_loss: 11056.4746\n",
      "Epoch 175/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4400.4824 - val_loss: 11038.5156\n",
      "Epoch 176/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4391.3818 - val_loss: 11014.7314\n",
      "Epoch 177/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4382.6855 - val_loss: 10990.0049\n",
      "Epoch 178/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 4375.0928 - val_loss: 10967.7910\n",
      "Epoch 179/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4368.5576 - val_loss: 10949.8467\n",
      "Epoch 180/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4361.3794 - val_loss: 10936.2129\n",
      "Epoch 181/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4351.7153 - val_loss: 10925.6279\n",
      "Epoch 182/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4341.6787 - val_loss: 10916.0781\n",
      "Epoch 183/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4335.2476 - val_loss: 10905.5459\n",
      "Epoch 184/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4329.3062 - val_loss: 10892.8379\n",
      "Epoch 185/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4319.6421 - val_loss: 10878.6191\n",
      "Epoch 186/500000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 4303.3892 - val_loss: 10865.6719\n",
      "Epoch 187/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4281.2349 - val_loss: 10854.8252\n",
      "Epoch 188/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4271.0347 - val_loss: 10843.6865\n",
      "Epoch 189/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4266.2949 - val_loss: 10829.7383\n",
      "Epoch 190/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 4259.6147 - val_loss: 10810.7266\n",
      "Epoch 191/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4251.0283 - val_loss: 10785.5449\n",
      "Epoch 192/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 4242.3149 - val_loss: 10754.6963\n",
      "Epoch 193/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4234.2617 - val_loss: 10720.0264\n",
      "Epoch 194/500000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 4226.5664 - val_loss: 10684.4561\n",
      "Epoch 195/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4218.3843 - val_loss: 10651.3271\n",
      "Epoch 196/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4209.6011 - val_loss: 10622.7822\n",
      "Epoch 197/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4200.6284 - val_loss: 10599.1318\n",
      "Epoch 198/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4193.0864 - val_loss: 10579.2480\n",
      "Epoch 199/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4188.8706 - val_loss: 10561.5088\n",
      "Epoch 200/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 4185.2920 - val_loss: 10544.8125\n",
      "Epoch 201/500000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 4180.2866 - val_loss: 10529.1152\n",
      "Epoch 202/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4174.2271 - val_loss: 10514.5234\n",
      "Epoch 203/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4168.3066 - val_loss: 10500.5049\n",
      "Epoch 204/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4163.6953 - val_loss: 10485.9834\n",
      "Epoch 205/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4160.5894 - val_loss: 10469.2412\n",
      "Epoch 206/500000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 4157.2568 - val_loss: 10449.0791\n",
      "Epoch 207/500000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 4152.5806 - val_loss: 10426.4209\n",
      "Epoch 208/500000\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 4147.1143 - val_loss: 10403.6904\n",
      "Epoch 209/500000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 4141.8779 - val_loss: 10382.9912\n",
      "Epoch 210/500000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 4137.3228 - val_loss: 10365.4902\n",
      "Epoch 211/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4133.4351 - val_loss: 10351.4248\n",
      "Epoch 212/500000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 4129.8037 - val_loss: 10340.1191\n",
      "Epoch 213/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4125.5752 - val_loss: 10330.7598\n",
      "Epoch 214/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 4119.8818 - val_loss: 10323.0352\n",
      "Epoch 215/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4112.5146 - val_loss: 10317.2305\n",
      "Epoch 216/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4102.1436 - val_loss: 10314.6768\n",
      "Epoch 217/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4086.0403 - val_loss: 10316.0430\n",
      "Epoch 218/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4067.2280 - val_loss: 10317.2871\n",
      "Epoch 219/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4054.9863 - val_loss: 10306.5244\n",
      "Epoch 220/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4049.7864 - val_loss: 10282.9482\n",
      "Epoch 221/500000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 4044.5854 - val_loss: 10257.4746\n",
      "Epoch 222/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4037.1055 - val_loss: 10240.5771\n",
      "Epoch 223/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4027.9805 - val_loss: 10234.6543\n",
      "Epoch 224/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4019.6431 - val_loss: 10237.0234\n",
      "Epoch 225/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4012.5879 - val_loss: 10243.9404\n",
      "Epoch 226/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 4005.4341 - val_loss: 10252.1133\n",
      "Epoch 227/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3997.7544 - val_loss: 10259.9248\n",
      "Epoch 228/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3990.6907 - val_loss: 10266.7930\n",
      "Epoch 229/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3984.6467 - val_loss: 10271.9346\n",
      "Epoch 230/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3978.9392 - val_loss: 10273.7500\n",
      "Epoch 231/500000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3973.3552 - val_loss: 10271.1816\n",
      "Epoch 232/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3968.2529 - val_loss: 10265.0459\n",
      "Epoch 233/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3963.2205 - val_loss: 10257.6816\n",
      "Epoch 234/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3957.0464 - val_loss: 10252.2568\n",
      "Epoch 235/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3949.5273 - val_loss: 10252.1191\n",
      "Epoch 236/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3941.8708 - val_loss: 10260.1357\n",
      "Epoch 237/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3934.8962 - val_loss: 10277.8701\n",
      "Epoch 238/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3928.6882 - val_loss: 10304.6025\n",
      "Epoch 239/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3923.4771 - val_loss: 10336.5098\n",
      "Epoch 240/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3919.3352 - val_loss: 10367.7041\n",
      "Epoch 241/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3916.1746 - val_loss: 10393.4199\n",
      "Epoch 242/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3913.4949 - val_loss: 10412.1094\n",
      "Epoch 243/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3910.7085 - val_loss: 10424.8506\n",
      "Epoch 244/500000\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 3907.5359 - val_loss: 10433.5723\n",
      "Epoch 245/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3904.0574 - val_loss: 10439.9580\n",
      "Epoch 246/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3900.7727 - val_loss: 10445.1572\n",
      "Epoch 247/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3897.9512 - val_loss: 10449.8701\n",
      "Epoch 248/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3895.1990 - val_loss: 10454.2871\n",
      "Epoch 249/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3892.1182 - val_loss: 10457.8896\n",
      "Epoch 250/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3888.6140 - val_loss: 10459.6270\n",
      "Epoch 251/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3885.1670 - val_loss: 10458.3721\n",
      "Epoch 252/500000\n",
      "1/1 [==============================] - ETA: 0s - loss: 3882.36 - 0s 18ms/step - loss: 3882.3677 - val_loss: 10453.5957\n",
      "Epoch 253/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3879.6025 - val_loss: 10445.5879\n",
      "Epoch 254/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3876.3066 - val_loss: 10435.4023\n",
      "Epoch 255/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3872.6919 - val_loss: 10424.6104\n",
      "Epoch 256/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3869.3215 - val_loss: 10414.9707\n",
      "Epoch 257/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3866.4922 - val_loss: 10408.0518\n",
      "Epoch 258/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3864.0637 - val_loss: 10404.5254\n",
      "Epoch 259/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3861.7815 - val_loss: 10403.8096\n",
      "Epoch 260/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3859.4392 - val_loss: 10404.2197\n",
      "Epoch 261/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3856.9692 - val_loss: 10403.8164\n",
      "Epoch 262/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3854.4329 - val_loss: 10401.2373\n",
      "Epoch 263/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3851.9907 - val_loss: 10396.2451\n",
      "Epoch 264/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3849.7107 - val_loss: 10389.4541\n",
      "Epoch 265/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3847.4241 - val_loss: 10381.7822\n",
      "Epoch 266/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3844.9607 - val_loss: 10373.9502\n",
      "Epoch 267/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3842.3315 - val_loss: 10366.2764\n",
      "Epoch 268/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3839.6597 - val_loss: 10358.7275\n",
      "Epoch 269/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3837.0156 - val_loss: 10351.0322\n",
      "Epoch 270/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3834.3545 - val_loss: 10342.9736\n",
      "Epoch 271/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3831.6423 - val_loss: 10334.5254\n",
      "Epoch 272/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3828.9246 - val_loss: 10325.9805\n",
      "Epoch 273/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3826.2380 - val_loss: 10317.7773\n",
      "Epoch 274/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3823.5278 - val_loss: 10310.3945\n",
      "Epoch 275/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3820.5867 - val_loss: 10304.2109\n",
      "Epoch 276/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3817.0134 - val_loss: 10299.5947\n",
      "Epoch 277/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3812.3071 - val_loss: 10297.0703\n",
      "Epoch 278/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3806.6677 - val_loss: 10296.8242\n",
      "Epoch 279/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3801.8396 - val_loss: 10297.8369\n",
      "Epoch 280/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3798.6067 - val_loss: 10298.4336\n",
      "Epoch 281/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3796.0657 - val_loss: 10297.4346\n",
      "Epoch 282/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3793.5012 - val_loss: 10294.4922\n",
      "Epoch 283/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3790.7285 - val_loss: 10289.8105\n",
      "Epoch 284/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3787.8604 - val_loss: 10283.9131\n",
      "Epoch 285/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3785.0195 - val_loss: 10277.3174\n",
      "Epoch 286/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3782.1633 - val_loss: 10270.4521\n",
      "Epoch 287/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3779.0510 - val_loss: 10263.5127\n",
      "Epoch 288/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3775.2893 - val_loss: 10256.6318\n",
      "Epoch 289/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3770.3745 - val_loss: 10250.0234\n",
      "Epoch 290/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3764.3103 - val_loss: 10243.8994\n",
      "Epoch 291/500000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step - loss: 3758.7434 - val_loss: 10238.4502\n",
      "Epoch 292/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3754.7205 - val_loss: 10233.6699\n",
      "Epoch 293/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3751.7412 - val_loss: 10229.4854\n",
      "Epoch 294/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3749.2593 - val_loss: 10225.8535\n",
      "Epoch 295/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3746.8662 - val_loss: 10222.7432\n",
      "Epoch 296/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3744.3354 - val_loss: 10220.1699\n",
      "Epoch 297/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3741.6370 - val_loss: 10218.1084\n",
      "Epoch 298/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3738.8389 - val_loss: 10216.4971\n",
      "Epoch 299/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3735.9656 - val_loss: 10215.3506\n",
      "Epoch 300/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3732.9749 - val_loss: 10214.6729\n",
      "Epoch 301/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3729.7427 - val_loss: 10214.4619\n",
      "Epoch 302/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3725.8640 - val_loss: 10214.6357\n",
      "Epoch 303/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3720.6223 - val_loss: 10215.0566\n",
      "Epoch 304/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3716.1238 - val_loss: 10215.5664\n",
      "Epoch 305/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3715.1208 - val_loss: 10215.8271\n",
      "Epoch 306/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3714.6914 - val_loss: 10215.4453\n",
      "Epoch 307/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3713.2170 - val_loss: 10214.1689\n",
      "Epoch 308/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3710.7419 - val_loss: 10211.9854\n",
      "Epoch 309/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3707.9197 - val_loss: 10209.1670\n",
      "Epoch 310/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3705.2878 - val_loss: 10206.1504\n",
      "Epoch 311/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3703.0244 - val_loss: 10203.4092\n",
      "Epoch 312/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3701.0608 - val_loss: 10201.4023\n",
      "Epoch 313/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3699.1545 - val_loss: 10200.4668\n",
      "Epoch 314/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3697.1284 - val_loss: 10200.7529\n",
      "Epoch 315/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3695.0969 - val_loss: 10202.1084\n",
      "Epoch 316/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3693.2703 - val_loss: 10204.0850\n",
      "Epoch 317/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3691.7456 - val_loss: 10206.0459\n",
      "Epoch 318/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3690.4941 - val_loss: 10207.3809\n",
      "Epoch 319/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3689.3408 - val_loss: 10207.7246\n",
      "Epoch 320/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3688.0488 - val_loss: 10206.9111\n",
      "Epoch 321/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3686.4766 - val_loss: 10204.9404\n",
      "Epoch 322/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3684.6511 - val_loss: 10201.8535\n",
      "Epoch 323/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3682.7212 - val_loss: 10197.8135\n",
      "Epoch 324/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3680.8308 - val_loss: 10193.2100\n",
      "Epoch 325/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3679.0178 - val_loss: 10188.5801\n",
      "Epoch 326/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3677.1982 - val_loss: 10184.4473\n",
      "Epoch 327/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3675.2507 - val_loss: 10181.0967\n",
      "Epoch 328/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3673.1016 - val_loss: 10178.4688\n",
      "Epoch 329/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3670.8286 - val_loss: 10176.1855\n",
      "Epoch 330/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3668.5378 - val_loss: 10173.7617\n",
      "Epoch 331/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3666.2449 - val_loss: 10170.8037\n",
      "Epoch 332/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3663.8809 - val_loss: 10167.1875\n",
      "Epoch 333/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3661.3931 - val_loss: 10163.1543\n",
      "Epoch 334/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3658.7993 - val_loss: 10159.2334\n",
      "Epoch 335/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3656.2141 - val_loss: 10156.0332\n",
      "Epoch 336/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3653.7527 - val_loss: 10154.0332\n",
      "Epoch 337/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3651.4053 - val_loss: 10153.4600\n",
      "Epoch 338/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3649.0667 - val_loss: 10154.4688\n",
      "Epoch 339/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3646.6548 - val_loss: 10157.2441\n",
      "Epoch 340/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3644.1318 - val_loss: 10161.9180\n",
      "Epoch 341/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3641.5095 - val_loss: 10168.3662\n",
      "Epoch 342/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3638.8662 - val_loss: 10176.0879\n",
      "Epoch 343/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3636.2544 - val_loss: 10184.1826\n",
      "Epoch 344/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3633.2993 - val_loss: 10191.5879\n",
      "Epoch 345/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3628.1899 - val_loss: 10197.5557\n",
      "Epoch 346/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3616.8933 - val_loss: 10201.4795\n",
      "Epoch 347/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3609.8118 - val_loss: 10202.2617\n",
      "Epoch 348/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3609.6692 - val_loss: 10199.4463\n",
      "Epoch 349/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3609.3948 - val_loss: 10193.2080\n",
      "Epoch 350/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3608.0112 - val_loss: 10184.7852\n",
      "Epoch 351/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3605.5466 - val_loss: 10176.3379\n",
      "Epoch 352/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3602.5200 - val_loss: 10169.9912\n",
      "Epoch 353/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3599.4929 - val_loss: 10167.1094\n",
      "Epoch 354/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3596.6125 - val_loss: 10168.0703\n",
      "Epoch 355/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3593.8066 - val_loss: 10172.6797\n",
      "Epoch 356/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3591.0295 - val_loss: 10180.5039\n",
      "Epoch 357/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3588.2107 - val_loss: 10190.8701\n",
      "Epoch 358/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3585.2144 - val_loss: 10202.6602\n",
      "Epoch 359/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3581.9529 - val_loss: 10214.1738\n",
      "Epoch 360/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3578.5740 - val_loss: 10223.6104\n",
      "Epoch 361/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3575.5237 - val_loss: 10229.9570\n",
      "Epoch 362/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3573.0908 - val_loss: 10233.4053\n",
      "Epoch 363/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3570.9697 - val_loss: 10234.7686\n",
      "Epoch 364/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3568.7493 - val_loss: 10234.7939\n",
      "Epoch 365/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3566.3999 - val_loss: 10233.8584\n",
      "Epoch 366/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3564.0823 - val_loss: 10231.9502\n",
      "Epoch 367/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3561.7795 - val_loss: 10228.7217\n",
      "Epoch 368/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3559.2422 - val_loss: 10223.3730\n",
      "Epoch 369/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3556.2151 - val_loss: 10214.5400\n",
      "Epoch 370/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3552.5737 - val_loss: 10200.8281\n",
      "Epoch 371/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3548.6277 - val_loss: 10183.5762\n",
      "Epoch 372/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3545.3853 - val_loss: 10168.8828\n",
      "Epoch 373/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3543.2429 - val_loss: 10160.9922\n",
      "Epoch 374/500000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 3541.5181 - val_loss: 10158.4297\n",
      "Epoch 375/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3539.7429 - val_loss: 10158.5391\n",
      "Epoch 376/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3537.8916 - val_loss: 10159.6162\n",
      "Epoch 377/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3535.9888 - val_loss: 10160.7832\n",
      "Epoch 378/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3534.0737 - val_loss: 10161.6221\n",
      "Epoch 379/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3532.2212 - val_loss: 10161.9570\n",
      "Epoch 380/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3530.4585 - val_loss: 10161.8125\n",
      "Epoch 381/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3528.6951 - val_loss: 10161.3965\n",
      "Epoch 382/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3526.8071 - val_loss: 10161.0156\n",
      "Epoch 383/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3524.7556 - val_loss: 10160.9717\n",
      "Epoch 384/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3522.6990 - val_loss: 10161.4033\n",
      "Epoch 385/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3520.8782 - val_loss: 10162.2803\n",
      "Epoch 386/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3519.3469 - val_loss: 10163.4336\n",
      "Epoch 387/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3517.9451 - val_loss: 10164.6914\n",
      "Epoch 388/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3516.4155 - val_loss: 10166.0088\n",
      "Epoch 389/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3514.4922 - val_loss: 10167.4189\n",
      "Epoch 390/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3512.0142 - val_loss: 10168.9102\n",
      "Epoch 391/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3509.1833 - val_loss: 10170.2891\n",
      "Epoch 392/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3506.6260 - val_loss: 10171.2910\n",
      "Epoch 393/500000\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3504.7378 - val_loss: 10171.7119\n",
      "Epoch 394/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3503.3203 - val_loss: 10171.5664\n",
      "Epoch 395/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3501.9822 - val_loss: 10170.8721\n",
      "Epoch 396/500000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3500.3953 - val_loss: 10169.6523\n",
      "Epoch 397/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3498.2815 - val_loss: 10167.8125\n",
      "Epoch 398/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3495.3953 - val_loss: 10165.1445\n",
      "Epoch 399/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3491.7415 - val_loss: 10161.2412\n",
      "Epoch 400/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3488.0220 - val_loss: 10155.3486\n",
      "Epoch 401/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3485.1316 - val_loss: 10146.4834\n",
      "Epoch 402/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3482.7615 - val_loss: 10134.2070\n",
      "Epoch 403/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3480.1692 - val_loss: 10120.9219\n",
      "Epoch 404/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3477.3904 - val_loss: 10112.0088\n",
      "Epoch 405/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3475.0486 - val_loss: 10108.7031\n",
      "Epoch 406/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3473.3918 - val_loss: 10107.0156\n",
      "Epoch 407/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3471.9463 - val_loss: 10104.6074\n",
      "Epoch 408/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3470.0066 - val_loss: 10102.0107\n",
      "Epoch 409/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3466.7659 - val_loss: 10101.6689\n",
      "Epoch 410/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3461.7183 - val_loss: 10105.3604\n",
      "Epoch 411/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3457.8831 - val_loss: 10109.2412\n",
      "Epoch 412/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3456.9900 - val_loss: 10110.6592\n",
      "Epoch 413/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3456.1348 - val_loss: 10110.4102\n",
      "Epoch 414/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3454.4333 - val_loss: 10109.6270\n",
      "Epoch 415/500000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3452.2400 - val_loss: 10108.9326\n",
      "Epoch 416/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3449.9558 - val_loss: 10108.4043\n",
      "Epoch 417/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3447.1841 - val_loss: 10107.7158\n",
      "Epoch 418/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3442.8181 - val_loss: 10106.3145\n",
      "Epoch 419/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3434.4097 - val_loss: 10103.5547\n",
      "Epoch 420/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3421.8225 - val_loss: 10099.2314\n",
      "Epoch 421/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3417.8215 - val_loss: 10093.9639\n",
      "Epoch 422/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3416.7405 - val_loss: 10088.7910\n",
      "Epoch 423/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3414.2144 - val_loss: 10084.7002\n",
      "Epoch 424/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3408.6885 - val_loss: 10082.5166\n",
      "Epoch 425/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3402.8503 - val_loss: 10082.6914\n",
      "Epoch 426/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3401.0142 - val_loss: 10084.9316\n",
      "Epoch 427/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3400.2595 - val_loss: 10088.1289\n",
      "Epoch 428/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3398.7371 - val_loss: 10091.0020\n",
      "Epoch 429/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3396.6785 - val_loss: 10092.6035\n",
      "Epoch 430/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3394.5496 - val_loss: 10092.5537\n",
      "Epoch 431/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3393.3438 - val_loss: 10091.0010\n",
      "Epoch 432/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3392.5186 - val_loss: 10088.6387\n",
      "Epoch 433/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3390.9705 - val_loss: 10086.3701\n",
      "Epoch 434/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3388.6985 - val_loss: 10084.9902\n",
      "Epoch 435/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3385.8826 - val_loss: 10084.9180\n",
      "Epoch 436/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3382.6277 - val_loss: 10086.0195\n",
      "Epoch 437/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3378.9497 - val_loss: 10087.8818\n",
      "Epoch 438/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3375.0088 - val_loss: 10090.6523\n",
      "Epoch 439/500000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step - loss: 3371.5745 - val_loss: 10095.8311\n",
      "Epoch 440/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3369.5251 - val_loss: 10105.7344\n",
      "Epoch 441/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3368.2227 - val_loss: 10120.8672\n",
      "Epoch 442/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3366.3669 - val_loss: 10136.8779\n",
      "Epoch 443/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3363.8691 - val_loss: 10145.9141\n",
      "Epoch 444/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3361.1555 - val_loss: 10144.9062\n",
      "Epoch 445/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3358.5056 - val_loss: 10138.8916\n",
      "Epoch 446/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3356.5552 - val_loss: 10136.5957\n",
      "Epoch 447/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3355.1992 - val_loss: 10142.2725\n",
      "Epoch 448/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3353.1016 - val_loss: 10151.8848\n",
      "Epoch 449/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3350.1370 - val_loss: 10157.6279\n",
      "Epoch 450/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3347.0051 - val_loss: 10156.3760\n",
      "Epoch 451/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3344.0925 - val_loss: 10152.3691\n",
      "Epoch 452/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3341.5396 - val_loss: 10150.7949\n",
      "Epoch 453/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3338.8696 - val_loss: 10152.5781\n",
      "Epoch 454/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3335.9885 - val_loss: 10155.6396\n",
      "Epoch 455/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3333.2419 - val_loss: 10157.5977\n",
      "Epoch 456/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3330.7971 - val_loss: 10157.9844\n",
      "Epoch 457/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3328.7156 - val_loss: 10159.0039\n",
      "Epoch 458/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3327.0444 - val_loss: 10163.6162\n",
      "Epoch 459/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3325.6748 - val_loss: 10171.8525\n",
      "Epoch 460/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3324.5674 - val_loss: 10179.1855\n",
      "Epoch 461/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3323.7925 - val_loss: 10180.3213\n",
      "Epoch 462/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3323.0801 - val_loss: 10174.0254\n",
      "Epoch 463/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3322.1348 - val_loss: 10163.1289\n",
      "Epoch 464/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3320.9397 - val_loss: 10151.6221\n",
      "Epoch 465/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3319.3955 - val_loss: 10142.3418\n",
      "Epoch 466/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3317.4529 - val_loss: 10136.2480\n",
      "Epoch 467/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3315.1853 - val_loss: 10133.2881\n",
      "Epoch 468/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3312.5051 - val_loss: 10133.4277\n",
      "Epoch 469/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3309.8938 - val_loss: 10136.7070\n",
      "Epoch 470/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3308.1707 - val_loss: 10142.6748\n",
      "Epoch 471/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3306.8652 - val_loss: 10150.1787\n",
      "Epoch 472/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3305.6318 - val_loss: 10157.6074\n",
      "Epoch 473/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3304.5710 - val_loss: 10163.6855\n",
      "Epoch 474/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3303.5173 - val_loss: 10168.4189\n",
      "Epoch 475/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3302.1855 - val_loss: 10172.6094\n",
      "Epoch 476/500000\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3300.4360 - val_loss: 10176.3301\n",
      "Epoch 477/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3298.1968 - val_loss: 10178.5947\n",
      "Epoch 478/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3295.2708 - val_loss: 10179.2119\n",
      "Epoch 479/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3291.0215 - val_loss: 10180.3594\n",
      "Epoch 480/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3285.8630 - val_loss: 10183.2832\n",
      "Epoch 481/500000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3282.6760 - val_loss: 10185.4189\n",
      "Epoch 482/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3281.2581 - val_loss: 10184.5596\n",
      "Epoch 483/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3280.0696 - val_loss: 10180.5176\n",
      "Epoch 484/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3278.6775 - val_loss: 10175.1318\n",
      "Epoch 485/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3277.0933 - val_loss: 10170.5137\n",
      "Epoch 486/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3275.3252 - val_loss: 10166.6357\n",
      "Epoch 487/500000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3273.3870 - val_loss: 10161.3984\n",
      "Epoch 488/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3271.4514 - val_loss: 10152.8555\n",
      "Epoch 489/500000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3269.6904 - val_loss: 10140.9404\n",
      "Epoch 490/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3268.0889 - val_loss: 10126.8096\n",
      "Epoch 491/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3266.5012 - val_loss: 10111.2930\n",
      "Epoch 492/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3264.8853 - val_loss: 10094.7871\n",
      "Epoch 493/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3263.2979 - val_loss: 10078.2129\n",
      "Epoch 494/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3261.7144 - val_loss: 10063.1299\n",
      "Epoch 495/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3260.0657 - val_loss: 10050.6445\n",
      "Epoch 496/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3258.3337 - val_loss: 10040.6973\n",
      "Epoch 497/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3256.6182 - val_loss: 10032.5420\n",
      "Epoch 498/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3255.0042 - val_loss: 10025.5986\n",
      "Epoch 499/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3253.4504 - val_loss: 10019.6982\n",
      "Epoch 500/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3251.8760 - val_loss: 10014.7725\n",
      "Epoch 501/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3250.2258 - val_loss: 10010.6562\n",
      "Epoch 502/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3248.4529 - val_loss: 10007.1123\n",
      "Epoch 503/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3246.4551 - val_loss: 10003.8838\n",
      "Epoch 504/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3244.0952 - val_loss: 10000.7051\n",
      "Epoch 505/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3241.1477 - val_loss: 9997.3184\n",
      "Epoch 506/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3237.2290 - val_loss: 9993.5488\n",
      "Epoch 507/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3232.2415 - val_loss: 9989.4355\n",
      "Epoch 508/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3227.3538 - val_loss: 9985.2334\n",
      "Epoch 509/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3223.6375 - val_loss: 9981.1631\n",
      "Epoch 510/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3221.1367 - val_loss: 9977.4678\n",
      "Epoch 511/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3219.7981 - val_loss: 9974.6367\n",
      "Epoch 512/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3219.0823 - val_loss: 9973.2461\n",
      "Epoch 513/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3218.0994 - val_loss: 9973.3486\n",
      "Epoch 514/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3216.4622 - val_loss: 9974.4834\n",
      "Epoch 515/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3214.5037 - val_loss: 9975.9404\n",
      "Epoch 516/500000\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3212.7566 - val_loss: 9977.1660\n",
      "Epoch 517/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3211.4456 - val_loss: 9977.9170\n",
      "Epoch 518/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3210.4534 - val_loss: 9978.2334\n",
      "Epoch 519/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3209.5537 - val_loss: 9978.2490\n",
      "Epoch 520/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3208.5881 - val_loss: 9978.1104\n",
      "Epoch 521/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3207.5059 - val_loss: 9977.8740\n",
      "Epoch 522/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3206.3567 - val_loss: 9977.5430\n",
      "Epoch 523/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3205.2271 - val_loss: 9977.1250\n",
      "Epoch 524/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3204.1638 - val_loss: 9976.6426\n",
      "Epoch 525/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3203.1389 - val_loss: 9976.1709\n",
      "Epoch 526/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3202.0967 - val_loss: 9975.7363\n",
      "Epoch 527/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3201.0115 - val_loss: 9975.3535\n",
      "Epoch 528/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3199.9128 - val_loss: 9975.0068\n",
      "Epoch 529/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3198.8547 - val_loss: 9974.6934\n",
      "Epoch 530/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3197.8877 - val_loss: 9974.4053\n",
      "Epoch 531/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3197.0227 - val_loss: 9974.1729\n",
      "Epoch 532/500000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 3196.2351 - val_loss: 9974.0430\n",
      "Epoch 533/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3195.4778 - val_loss: 9974.0996\n",
      "Epoch 534/500000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 3194.7104 - val_loss: 9974.4072\n",
      "Epoch 535/500000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 3193.9170 - val_loss: 9975.0166\n",
      "Epoch 536/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3193.1138 - val_loss: 9975.9121\n",
      "Epoch 537/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3192.3181 - val_loss: 9977.0654\n",
      "Epoch 538/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3191.5464 - val_loss: 9978.4209\n",
      "Epoch 539/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3190.7944 - val_loss: 9979.9180\n",
      "Epoch 540/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3190.0510 - val_loss: 9981.4902\n",
      "Epoch 541/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3189.3086 - val_loss: 9983.0850\n",
      "Epoch 542/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3188.5615 - val_loss: 9984.7021\n",
      "Epoch 543/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3187.8140 - val_loss: 9986.3232\n",
      "Epoch 544/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3187.0730 - val_loss: 9987.9365\n",
      "Epoch 545/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3186.3394 - val_loss: 9989.5254\n",
      "Epoch 546/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3185.6052 - val_loss: 9991.0537\n",
      "Epoch 547/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3184.8618 - val_loss: 9992.5283\n",
      "Epoch 548/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3184.0996 - val_loss: 9993.9346\n",
      "Epoch 549/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3183.3167 - val_loss: 9995.2725\n",
      "Epoch 550/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3182.5195 - val_loss: 9996.5430\n",
      "Epoch 551/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3181.7300 - val_loss: 9997.7783\n",
      "Epoch 552/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3180.9827 - val_loss: 9998.9990\n",
      "Epoch 553/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3180.3103 - val_loss: 10000.1885\n",
      "Epoch 554/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3179.7249 - val_loss: 10001.3271\n",
      "Epoch 555/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3179.2092 - val_loss: 10002.3555\n",
      "Epoch 556/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3178.7285 - val_loss: 10003.2441\n",
      "Epoch 557/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3178.2585 - val_loss: 10003.9932\n",
      "Epoch 558/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3177.7842 - val_loss: 10004.6104\n",
      "Epoch 559/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3177.2959 - val_loss: 10005.1123\n",
      "Epoch 560/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3176.7903 - val_loss: 10005.5293\n",
      "Epoch 561/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3176.2634 - val_loss: 10005.8721\n",
      "Epoch 562/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3175.7144 - val_loss: 10006.1504\n",
      "Epoch 563/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3175.1489 - val_loss: 10006.3555\n",
      "Epoch 564/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3174.5645 - val_loss: 10006.4883\n",
      "Epoch 565/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3173.9612 - val_loss: 10006.5439\n",
      "Epoch 566/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3173.3308 - val_loss: 10006.5371\n",
      "Epoch 567/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3172.6599 - val_loss: 10006.4609\n",
      "Epoch 568/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3171.9426 - val_loss: 10006.3242\n",
      "Epoch 569/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3171.1736 - val_loss: 10006.1338\n",
      "Epoch 570/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3170.3569 - val_loss: 10005.8652\n",
      "Epoch 571/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3169.5115 - val_loss: 10005.4971\n",
      "Epoch 572/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3168.6653 - val_loss: 10004.9932\n",
      "Epoch 573/500000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3167.8486 - val_loss: 10004.3213\n",
      "Epoch 574/500000\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3167.0818 - val_loss: 10003.4551\n",
      "Epoch 575/500000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3166.3704 - val_loss: 10002.3652\n",
      "Epoch 576/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3165.7078 - val_loss: 10001.0479\n",
      "Epoch 577/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3165.1001 - val_loss: 9999.5039\n",
      "Epoch 578/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3164.5664 - val_loss: 9997.7246\n",
      "Epoch 579/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3164.1133 - val_loss: 9995.6816\n",
      "Epoch 580/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3163.6848 - val_loss: 9993.3311\n",
      "Epoch 581/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3163.1897 - val_loss: 9990.6328\n",
      "Epoch 582/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3162.6233 - val_loss: 9987.5713\n",
      "Epoch 583/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3162.0479 - val_loss: 9984.2119\n",
      "Epoch 584/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3161.4819 - val_loss: 9980.6572\n",
      "Epoch 585/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3160.8911 - val_loss: 9977.0410\n",
      "Epoch 586/500000\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3160.2356 - val_loss: 9973.4385\n",
      "Epoch 587/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3159.4873 - val_loss: 9969.8496\n",
      "Epoch 588/500000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step - loss: 3158.6345 - val_loss: 9966.2314\n",
      "Epoch 589/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3157.6746 - val_loss: 9962.4805\n",
      "Epoch 590/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3156.5889 - val_loss: 9958.5283\n",
      "Epoch 591/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3155.3455 - val_loss: 9954.3193\n",
      "Epoch 592/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3153.9041 - val_loss: 9949.9443\n",
      "Epoch 593/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3152.2551 - val_loss: 9945.7852\n",
      "Epoch 594/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3150.4863 - val_loss: 9942.4658\n",
      "Epoch 595/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3148.8115 - val_loss: 9940.5869\n",
      "Epoch 596/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3147.4470 - val_loss: 9940.1934\n",
      "Epoch 597/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3146.3330 - val_loss: 9940.7334\n",
      "Epoch 598/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3145.2329 - val_loss: 9941.4082\n",
      "Epoch 599/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3144.0793 - val_loss: 9941.6221\n",
      "Epoch 600/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3142.9507 - val_loss: 9941.0967\n",
      "Epoch 601/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3141.9077 - val_loss: 9939.8379\n",
      "Epoch 602/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3140.9390 - val_loss: 9938.0166\n",
      "Epoch 603/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3140.0210 - val_loss: 9935.8018\n",
      "Epoch 604/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3139.1370 - val_loss: 9933.3057\n",
      "Epoch 605/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3138.2600 - val_loss: 9930.5312\n",
      "Epoch 606/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3137.3503 - val_loss: 9927.4492\n",
      "Epoch 607/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3136.4082 - val_loss: 9924.1357\n",
      "Epoch 608/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3135.4722 - val_loss: 9920.7480\n",
      "Epoch 609/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3134.5789 - val_loss: 9917.5400\n",
      "Epoch 610/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3133.7329 - val_loss: 9914.8252\n",
      "Epoch 611/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3132.9099 - val_loss: 9912.8721\n",
      "Epoch 612/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3132.0657 - val_loss: 9911.8438\n",
      "Epoch 613/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3131.1912 - val_loss: 9911.7246\n",
      "Epoch 614/500000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3130.3157 - val_loss: 9912.2705\n",
      "Epoch 615/500000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3129.4919 - val_loss: 9913.0557\n",
      "Epoch 616/500000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3128.7444 - val_loss: 9913.5947\n",
      "Epoch 617/500000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3128.0537 - val_loss: 9913.5029\n",
      "Epoch 618/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3127.3638 - val_loss: 9912.6357\n",
      "Epoch 619/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3126.6289 - val_loss: 9911.0791\n",
      "Epoch 620/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3125.8123 - val_loss: 9909.0977\n",
      "Epoch 621/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3124.8447 - val_loss: 9907.0605\n",
      "Epoch 622/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3123.5352 - val_loss: 9905.4160\n",
      "Epoch 623/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3121.3674 - val_loss: 9904.9756\n",
      "Epoch 624/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3117.4529 - val_loss: 9907.2773\n",
      "Epoch 625/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3113.1892 - val_loss: 9912.5732\n",
      "Epoch 626/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3111.7673 - val_loss: 9918.7881\n",
      "Epoch 627/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3111.5056 - val_loss: 9923.1123\n",
      "Epoch 628/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3111.0181 - val_loss: 9923.5000\n",
      "Epoch 629/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3109.9995 - val_loss: 9919.4570\n",
      "Epoch 630/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3108.5608 - val_loss: 9911.8896\n",
      "Epoch 631/500000\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3106.8389 - val_loss: 9902.1592\n",
      "Epoch 632/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3104.9956 - val_loss: 9891.6875\n",
      "Epoch 633/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3103.6123 - val_loss: 9882.0205\n",
      "Epoch 634/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3102.9226 - val_loss: 9874.7500\n",
      "Epoch 635/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3102.2964 - val_loss: 9871.2334\n",
      "Epoch 636/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3101.3733 - val_loss: 9872.2637\n",
      "Epoch 637/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3100.2896 - val_loss: 9877.7461\n",
      "Epoch 638/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3099.3362 - val_loss: 9886.7744\n",
      "Epoch 639/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3098.6536 - val_loss: 9898.0684\n",
      "Epoch 640/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3098.1011 - val_loss: 9910.4229\n",
      "Epoch 641/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3097.4353 - val_loss: 9922.6445\n",
      "Epoch 642/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3096.6040 - val_loss: 9933.4316\n",
      "Epoch 643/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3095.8049 - val_loss: 9941.5498\n",
      "Epoch 644/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3095.1865 - val_loss: 9946.1680\n",
      "Epoch 645/500000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3094.6714 - val_loss: 9947.0762\n",
      "Epoch 646/500000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3094.1108 - val_loss: 9944.7451\n",
      "Epoch 647/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3093.4685 - val_loss: 9940.1436\n",
      "Epoch 648/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3092.8335 - val_loss: 9934.4736\n",
      "Epoch 649/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3092.2727 - val_loss: 9928.7920\n",
      "Epoch 650/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3091.7546 - val_loss: 9923.7881\n",
      "Epoch 651/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3091.2188 - val_loss: 9919.7031\n",
      "Epoch 652/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3090.6638 - val_loss: 9916.5205\n",
      "Epoch 653/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3090.1331 - val_loss: 9914.0928\n",
      "Epoch 654/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3089.6348 - val_loss: 9912.2705\n",
      "Epoch 655/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3089.1465 - val_loss: 9910.9531\n",
      "Epoch 656/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3088.6438 - val_loss: 9910.0762\n",
      "Epoch 657/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3088.1194 - val_loss: 9909.5957\n",
      "Epoch 658/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3087.5854 - val_loss: 9909.4229\n",
      "Epoch 659/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3087.0588 - val_loss: 9909.4648\n",
      "Epoch 660/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3086.5586 - val_loss: 9909.5225\n",
      "Epoch 661/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3086.0876 - val_loss: 9909.3799\n",
      "Epoch 662/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3085.6338 - val_loss: 9908.7314\n",
      "Epoch 663/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3085.1897 - val_loss: 9907.2979\n",
      "Epoch 664/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3084.7480 - val_loss: 9904.9082\n",
      "Epoch 665/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3084.3123 - val_loss: 9901.6152\n",
      "Epoch 666/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3083.8789 - val_loss: 9897.6426\n",
      "Epoch 667/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3083.4453 - val_loss: 9893.3662\n",
      "Epoch 668/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3083.0103 - val_loss: 9889.1562\n",
      "Epoch 669/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3082.5781 - val_loss: 9885.3125\n",
      "Epoch 670/500000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3082.1501 - val_loss: 9882.0186\n",
      "Epoch 671/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3081.7268 - val_loss: 9879.3604\n",
      "Epoch 672/500000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3081.3052 - val_loss: 9877.2939\n",
      "Epoch 673/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3080.8762 - val_loss: 9875.7539\n",
      "Epoch 674/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3080.4429 - val_loss: 9874.6670\n",
      "Epoch 675/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3080.0049 - val_loss: 9873.9688\n",
      "Epoch 676/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3079.5691 - val_loss: 9873.6123\n",
      "Epoch 677/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3079.1316 - val_loss: 9873.5576\n",
      "Epoch 678/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3078.6951 - val_loss: 9873.7344\n",
      "Epoch 679/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3078.2605 - val_loss: 9874.0732\n",
      "Epoch 680/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3077.8330 - val_loss: 9874.5205\n",
      "Epoch 681/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3077.4163 - val_loss: 9875.0000\n",
      "Epoch 682/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3077.0122 - val_loss: 9875.4932\n",
      "Epoch 683/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3076.6189 - val_loss: 9875.9971\n",
      "Epoch 684/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3076.2412 - val_loss: 9876.5234\n",
      "Epoch 685/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3075.8755 - val_loss: 9877.1436\n",
      "Epoch 686/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3075.5188 - val_loss: 9877.9414\n",
      "Epoch 687/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3075.1667 - val_loss: 9879.0088\n",
      "Epoch 688/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3074.8110 - val_loss: 9880.3936\n",
      "Epoch 689/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3074.4509 - val_loss: 9882.1104\n",
      "Epoch 690/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3074.0852 - val_loss: 9884.1260\n",
      "Epoch 691/500000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3073.7158 - val_loss: 9886.3662\n",
      "Epoch 692/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3073.3447 - val_loss: 9888.7246\n",
      "Epoch 693/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3072.9705 - val_loss: 9891.1016\n",
      "Epoch 694/500000\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3072.5957 - val_loss: 9893.3496\n",
      "Epoch 695/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3072.2197 - val_loss: 9895.3896\n",
      "Epoch 696/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3071.8408 - val_loss: 9897.1338\n",
      "Epoch 697/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3071.4587 - val_loss: 9898.5537\n",
      "Epoch 698/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3071.0698 - val_loss: 9899.6396\n",
      "Epoch 699/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3070.6729 - val_loss: 9900.4043\n",
      "Epoch 700/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3070.2673 - val_loss: 9900.8555\n",
      "Epoch 701/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3069.8501 - val_loss: 9901.0400\n",
      "Epoch 702/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3069.4246 - val_loss: 9900.9990\n",
      "Epoch 703/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3068.9875 - val_loss: 9900.7871\n",
      "Epoch 704/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3068.5400 - val_loss: 9900.4434\n",
      "Epoch 705/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3068.0835 - val_loss: 9900.0088\n",
      "Epoch 706/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3067.6218 - val_loss: 9899.5205\n",
      "Epoch 707/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3067.1589 - val_loss: 9898.9971\n",
      "Epoch 708/500000\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3066.6997 - val_loss: 9898.4570\n",
      "Epoch 709/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3066.2522 - val_loss: 9897.9521\n",
      "Epoch 710/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3065.8171 - val_loss: 9897.5566\n",
      "Epoch 711/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3065.3940 - val_loss: 9897.3604\n",
      "Epoch 712/500000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3064.9780 - val_loss: 9897.4414\n",
      "Epoch 713/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3064.5579 - val_loss: 9897.8379\n",
      "Epoch 714/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3064.1245 - val_loss: 9898.5830\n",
      "Epoch 715/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3063.6716 - val_loss: 9899.6514\n",
      "Epoch 716/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3063.1926 - val_loss: 9901.0244\n",
      "Epoch 717/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3062.6887 - val_loss: 9902.6455\n",
      "Epoch 718/500000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3062.1560 - val_loss: 9904.4717\n",
      "Epoch 719/500000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3061.5989 - val_loss: 9906.4795\n",
      "Epoch 720/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3061.0159 - val_loss: 9908.6445\n",
      "Epoch 721/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3060.4092 - val_loss: 9910.9951\n",
      "Epoch 722/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3059.7764 - val_loss: 9913.5410\n",
      "Epoch 723/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3059.1206 - val_loss: 9916.3506\n",
      "Epoch 724/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3058.4329 - val_loss: 9919.4424\n",
      "Epoch 725/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3057.6973 - val_loss: 9922.7959\n",
      "Epoch 726/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3056.8855 - val_loss: 9926.2988\n",
      "Epoch 727/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3055.9771 - val_loss: 9929.7881\n",
      "Epoch 728/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3054.9690 - val_loss: 9933.1074\n",
      "Epoch 729/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3053.9014 - val_loss: 9936.1162\n",
      "Epoch 730/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3052.8640 - val_loss: 9938.7832\n",
      "Epoch 731/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3051.9590 - val_loss: 9941.0996\n",
      "Epoch 732/500000\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3051.2434 - val_loss: 9943.1240\n",
      "Epoch 733/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3050.6936 - val_loss: 9944.9004\n",
      "Epoch 734/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3050.2356 - val_loss: 9946.4570\n",
      "Epoch 735/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3049.7944 - val_loss: 9947.8340\n",
      "Epoch 736/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3049.3206 - val_loss: 9949.0635\n",
      "Epoch 737/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3048.7876 - val_loss: 9950.2139\n",
      "Epoch 738/500000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step - loss: 3048.1868 - val_loss: 9951.3438\n",
      "Epoch 739/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3047.5317 - val_loss: 9952.5361\n",
      "Epoch 740/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3046.8484 - val_loss: 9953.8174\n",
      "Epoch 741/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3046.1753 - val_loss: 9955.1963\n",
      "Epoch 742/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3045.5464 - val_loss: 9956.6133\n",
      "Epoch 743/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3044.9788 - val_loss: 9957.9570\n",
      "Epoch 744/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3044.4768 - val_loss: 9959.1729\n",
      "Epoch 745/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3044.0325 - val_loss: 9960.1953\n",
      "Epoch 746/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3043.6284 - val_loss: 9961.0605\n",
      "Epoch 747/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3043.2490 - val_loss: 9961.8418\n",
      "Epoch 748/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3042.8823 - val_loss: 9962.6611\n",
      "Epoch 749/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3042.5225 - val_loss: 9963.5986\n",
      "Epoch 750/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3042.1621 - val_loss: 9964.7021\n",
      "Epoch 751/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3041.7983 - val_loss: 9965.9717\n",
      "Epoch 752/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3041.4314 - val_loss: 9967.3359\n",
      "Epoch 753/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3041.0623 - val_loss: 9968.7129\n",
      "Epoch 754/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3040.6924 - val_loss: 9970.0488\n",
      "Epoch 755/500000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3040.3228 - val_loss: 9971.3223\n",
      "Epoch 756/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3039.9539 - val_loss: 9972.5312\n",
      "Epoch 757/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3039.5862 - val_loss: 9973.7207\n",
      "Epoch 758/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3039.2168 - val_loss: 9974.9238\n",
      "Epoch 759/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3038.8496 - val_loss: 9976.1631\n",
      "Epoch 760/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3038.4824 - val_loss: 9977.4805\n",
      "Epoch 761/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3038.1138 - val_loss: 9978.8662\n",
      "Epoch 762/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3037.7454 - val_loss: 9980.3232\n",
      "Epoch 763/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3037.3774 - val_loss: 9981.8359\n",
      "Epoch 764/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3037.0100 - val_loss: 9983.3887\n",
      "Epoch 765/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3036.6431 - val_loss: 9984.9365\n",
      "Epoch 766/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3036.2783 - val_loss: 9986.4453\n",
      "Epoch 767/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3035.9160 - val_loss: 9987.8779\n",
      "Epoch 768/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3035.5557 - val_loss: 9989.2305\n",
      "Epoch 769/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3035.1982 - val_loss: 9990.4863\n",
      "Epoch 770/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3034.8398 - val_loss: 9991.6797\n",
      "Epoch 771/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3034.4790 - val_loss: 9992.8516\n",
      "Epoch 772/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3034.1064 - val_loss: 9994.0537\n",
      "Epoch 773/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3033.7151 - val_loss: 9995.3506\n",
      "Epoch 774/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3033.2903 - val_loss: 9996.7930\n",
      "Epoch 775/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3032.8169 - val_loss: 9998.4180\n",
      "Epoch 776/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3032.2749 - val_loss: 10000.2529\n",
      "Epoch 777/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3031.6567 - val_loss: 10002.3271\n",
      "Epoch 778/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3030.9915 - val_loss: 10004.6865\n",
      "Epoch 779/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3030.3518 - val_loss: 10007.3555\n",
      "Epoch 780/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3029.8081 - val_loss: 10010.3613\n",
      "Epoch 781/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3029.3643 - val_loss: 10013.6816\n",
      "Epoch 782/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3028.9954 - val_loss: 10017.2383\n",
      "Epoch 783/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3028.6475 - val_loss: 10020.9229\n",
      "Epoch 784/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3028.2268 - val_loss: 10024.5908\n",
      "Epoch 785/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3027.6965 - val_loss: 10028.1826\n",
      "Epoch 786/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3027.1453 - val_loss: 10031.7939\n",
      "Epoch 787/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3026.6807 - val_loss: 10035.6602\n",
      "Epoch 788/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3026.3235 - val_loss: 10040.0762\n",
      "Epoch 789/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3026.0015 - val_loss: 10045.3223\n",
      "Epoch 790/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3025.6484 - val_loss: 10051.4580\n",
      "Epoch 791/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3025.2712 - val_loss: 10058.4150\n",
      "Epoch 792/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3024.9077 - val_loss: 10065.9521\n",
      "Epoch 793/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3024.5793 - val_loss: 10073.8047\n",
      "Epoch 794/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3024.2795 - val_loss: 10081.7051\n",
      "Epoch 795/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3023.9807 - val_loss: 10089.4092\n",
      "Epoch 796/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3023.6621 - val_loss: 10096.7070\n",
      "Epoch 797/500000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3023.3171 - val_loss: 10103.4688\n",
      "Epoch 798/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3022.9475 - val_loss: 10109.6416\n",
      "Epoch 799/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3022.5598 - val_loss: 10115.2451\n",
      "Epoch 800/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3022.1675 - val_loss: 10120.3311\n",
      "Epoch 801/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3021.7788 - val_loss: 10124.9229\n",
      "Epoch 802/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3021.3870 - val_loss: 10128.9883\n",
      "Epoch 803/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3020.9802 - val_loss: 10132.4092\n",
      "Epoch 804/500000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3020.5518 - val_loss: 10135.0791\n",
      "Epoch 805/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3020.1028 - val_loss: 10136.9629\n",
      "Epoch 806/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3019.6436 - val_loss: 10138.1338\n",
      "Epoch 807/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3019.1824 - val_loss: 10138.7881\n",
      "Epoch 808/500000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3018.7329 - val_loss: 10139.1992\n",
      "Epoch 809/500000\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3018.2974 - val_loss: 10139.5615\n",
      "Epoch 810/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3017.8777 - val_loss: 10139.9932\n",
      "Epoch 811/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3017.4675 - val_loss: 10140.4053\n",
      "Epoch 812/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3017.0620 - val_loss: 10140.6455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 813/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3016.6577 - val_loss: 10140.4932\n",
      "Epoch 814/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3016.2458 - val_loss: 10139.8291\n",
      "Epoch 815/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3015.8176 - val_loss: 10138.6602\n",
      "Epoch 816/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3015.3555 - val_loss: 10137.1328\n",
      "Epoch 817/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3014.8394 - val_loss: 10135.4609\n",
      "Epoch 818/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3014.2598 - val_loss: 10133.9385\n",
      "Epoch 819/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3013.6252 - val_loss: 10132.8057\n",
      "Epoch 820/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3012.9609 - val_loss: 10132.2119\n",
      "Epoch 821/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3012.3059 - val_loss: 10132.1787\n",
      "Epoch 822/500000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3011.6956 - val_loss: 10132.5752\n",
      "Epoch 823/500000\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3011.1350 - val_loss: 10133.1973\n",
      "Epoch 824/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3010.6155 - val_loss: 10133.8213\n",
      "Epoch 825/500000\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3010.1184 - val_loss: 10134.2666\n",
      "Epoch 826/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3009.6287 - val_loss: 10134.3916\n",
      "Epoch 827/500000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3009.1306 - val_loss: 10134.1953\n",
      "Epoch 828/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3008.6113 - val_loss: 10133.6719\n",
      "Epoch 829/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3008.0549 - val_loss: 10132.9082\n",
      "Epoch 830/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3007.4392 - val_loss: 10132.0029\n",
      "Epoch 831/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3006.7327 - val_loss: 10131.0264\n",
      "Epoch 832/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3005.8745 - val_loss: 10130.0879\n",
      "Epoch 833/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3004.7605 - val_loss: 10129.2998\n",
      "Epoch 834/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3003.2231 - val_loss: 10128.8350\n",
      "Epoch 835/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3001.0310 - val_loss: 10128.9668\n",
      "Epoch 836/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2998.1184 - val_loss: 10129.8662\n",
      "Epoch 837/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 2995.1467 - val_loss: 10131.1309\n",
      "Epoch 838/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2993.1150 - val_loss: 10131.6846\n",
      "Epoch 839/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2991.9099 - val_loss: 10130.5020\n",
      "Epoch 840/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2990.8950 - val_loss: 10127.1348\n",
      "Epoch 841/500000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2989.7764 - val_loss: 10121.9365\n",
      "Epoch 842/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2988.6321 - val_loss: 10115.6562\n",
      "Epoch 843/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2987.6628 - val_loss: 10108.8232\n",
      "Epoch 844/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2986.8660 - val_loss: 10101.7285\n",
      "Epoch 845/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2986.0156 - val_loss: 10094.8936\n",
      "Epoch 846/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2984.9114 - val_loss: 10088.9629\n",
      "Epoch 847/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2983.6472 - val_loss: 10084.0918\n",
      "Epoch 848/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2982.5127 - val_loss: 10079.7832\n",
      "Epoch 849/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2981.5044 - val_loss: 10075.5146\n",
      "Epoch 850/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2980.5066 - val_loss: 10071.1152\n",
      "Epoch 851/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2979.6641 - val_loss: 10066.9102\n",
      "Epoch 852/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2978.9360 - val_loss: 10063.8379\n",
      "Epoch 853/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2977.9670 - val_loss: 10063.0635\n",
      "Epoch 854/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2976.7412 - val_loss: 10065.2900\n",
      "Epoch 855/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2975.5298 - val_loss: 10070.2139\n",
      "Epoch 856/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2974.4390 - val_loss: 10076.5410\n",
      "Epoch 857/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2973.3445 - val_loss: 10082.6562\n",
      "Epoch 858/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2972.1133 - val_loss: 10087.5430\n",
      "Epoch 859/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2970.8186 - val_loss: 10091.0889\n",
      "Epoch 860/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2969.7036 - val_loss: 10093.5586\n",
      "Epoch 861/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2968.9180 - val_loss: 10094.8662\n",
      "Epoch 862/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2968.3730 - val_loss: 10094.4248\n",
      "Epoch 863/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2967.8835 - val_loss: 10091.6611\n",
      "Epoch 864/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2967.3381 - val_loss: 10086.6562\n",
      "Epoch 865/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2966.7334 - val_loss: 10080.3789\n",
      "Epoch 866/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2966.1296 - val_loss: 10074.3682\n",
      "Epoch 867/500000\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2965.5510 - val_loss: 10070.0400\n",
      "Epoch 868/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 2964.9497 - val_loss: 10068.0791\n",
      "Epoch 869/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2964.2805 - val_loss: 10068.2803\n",
      "Epoch 870/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2963.5693 - val_loss: 10069.7295\n",
      "Epoch 871/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2962.8665 - val_loss: 10071.3389\n",
      "Epoch 872/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2962.1802 - val_loss: 10072.2812\n",
      "Epoch 873/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2961.4827 - val_loss: 10072.2363\n",
      "Epoch 874/500000\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2960.7581 - val_loss: 10071.2588\n",
      "Epoch 875/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2960.0188 - val_loss: 10069.5039\n",
      "Epoch 876/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2959.2581 - val_loss: 10067.1084\n",
      "Epoch 877/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2958.4619 - val_loss: 10064.1602\n",
      "Epoch 878/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 2957.6169 - val_loss: 10060.7871\n",
      "Epoch 879/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2956.7412 - val_loss: 10057.2764\n",
      "Epoch 880/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2955.8875 - val_loss: 10054.0010\n",
      "Epoch 881/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2955.1084 - val_loss: 10051.3154\n",
      "Epoch 882/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2954.4207 - val_loss: 10049.4395\n",
      "Epoch 883/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2953.7944 - val_loss: 10048.4004\n",
      "Epoch 884/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 2953.1687 - val_loss: 10048.0000\n",
      "Epoch 885/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2952.5029 - val_loss: 10047.9727\n",
      "Epoch 886/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2951.8179 - val_loss: 10048.0088\n",
      "Epoch 887/500000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2951.1614 - val_loss: 10047.8154\n",
      "Epoch 888/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2950.5435 - val_loss: 10047.0986\n",
      "Epoch 889/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2949.9385 - val_loss: 10045.6680\n",
      "Epoch 890/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2949.3118 - val_loss: 10043.4326\n",
      "Epoch 891/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2948.6416 - val_loss: 10040.5117\n",
      "Epoch 892/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2947.9275 - val_loss: 10037.1533\n",
      "Epoch 893/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2947.1838 - val_loss: 10033.6777\n",
      "Epoch 894/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2946.4419 - val_loss: 10030.3193\n",
      "Epoch 895/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2945.7417 - val_loss: 10027.1650\n",
      "Epoch 896/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2945.1138 - val_loss: 10024.2100\n",
      "Epoch 897/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2944.5667 - val_loss: 10021.3828\n",
      "Epoch 898/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2944.0879 - val_loss: 10018.6621\n",
      "Epoch 899/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 2943.6643 - val_loss: 10016.1143\n",
      "Epoch 900/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2943.2842 - val_loss: 10013.7988\n",
      "Epoch 901/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2942.9421 - val_loss: 10011.7607\n",
      "Epoch 902/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2942.6296 - val_loss: 10010.0088\n",
      "Epoch 903/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2942.3381 - val_loss: 10008.5703\n",
      "Epoch 904/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2942.0498 - val_loss: 10007.5098\n",
      "Epoch 905/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2941.7458 - val_loss: 10006.9189\n",
      "Epoch 906/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2941.4209 - val_loss: 10006.9111\n",
      "Epoch 907/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2941.0808 - val_loss: 10007.5215\n",
      "Epoch 908/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 2940.7358 - val_loss: 10008.6963\n",
      "Epoch 909/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2940.3867 - val_loss: 10010.2754\n",
      "Epoch 910/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 2940.0334 - val_loss: 10012.0596\n",
      "Epoch 911/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2939.6707 - val_loss: 10013.9307\n",
      "Epoch 912/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2939.2971 - val_loss: 10015.7969\n",
      "Epoch 913/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2938.9170 - val_loss: 10017.6982\n",
      "Epoch 914/500000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2938.5298 - val_loss: 10019.6475\n",
      "Epoch 915/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2938.1392 - val_loss: 10021.6816\n",
      "Epoch 916/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2937.7397 - val_loss: 10023.7959\n",
      "Epoch 917/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2937.3264 - val_loss: 10025.9463\n",
      "Epoch 918/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2936.8965 - val_loss: 10028.0947\n",
      "Epoch 919/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2936.4490 - val_loss: 10030.1797\n",
      "Epoch 920/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2935.9841 - val_loss: 10032.1230\n",
      "Epoch 921/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2935.4995 - val_loss: 10033.8535\n",
      "Epoch 922/500000\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2934.9929 - val_loss: 10035.2910\n",
      "Epoch 923/500000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2934.4612 - val_loss: 10036.3945\n",
      "Epoch 924/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2933.9031 - val_loss: 10037.1396\n",
      "Epoch 925/500000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 2933.3149 - val_loss: 10037.5449\n",
      "Epoch 926/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2932.6980 - val_loss: 10037.6523\n",
      "Epoch 927/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2932.0542 - val_loss: 10037.4844\n",
      "Epoch 928/500000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2931.3845 - val_loss: 10037.0801\n",
      "Epoch 929/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2930.6926 - val_loss: 10036.4951\n",
      "Epoch 930/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2929.9839 - val_loss: 10035.7754\n",
      "Epoch 931/500000\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2929.2656 - val_loss: 10034.9717\n",
      "Epoch 932/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2928.5442 - val_loss: 10034.1279\n",
      "Epoch 933/500000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2927.8179 - val_loss: 10033.2451\n",
      "Epoch 934/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2927.0747 - val_loss: 10032.3828\n",
      "Epoch 935/500000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2926.2871 - val_loss: 10031.5283\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(x, y, test_size=0.10, random_state=41)\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "model_mlp = Sequential()\n",
    "model_mlp.add(Dense(55, input_dim=X_train.shape[1], activation='sigmoid',kernel_initializer='random_uniform'))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.5))\n",
    "model_mlp.add(Dense(1, activation='linear'))\n",
    "opt = SGD(learning_rate=0.01,momentum=0.9)\n",
    "model_mlp.compile(optimizer=opt, loss='mse')\n",
    "model_mlp.summary()\n",
    "early_stopping_cb = EarlyStopping(patience=300,restore_best_weights=True)\n",
    "run_logdir=r\"C:\\Users\\Deepak Tripathi\\Desktop\\rossman\\log\"\n",
    "history = model_mlp.fit(np.array(X_train), y_train, batch_size=len(X_train),validation_data=(X_val, y_val) ,epochs=500000, verbose=1,callbacks=[early_stopping_cb])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Test Data provided by company with final modal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1239,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r\"C:\\Users\\Deepak Tripathi\\Desktop\\assignment machine learning\\part3\\CE802_P3_Test.csv\")\n",
    "df['F15']=df['F15'].map({'Very low':0.0,'Low':1.0,'Medium':2.0,'High':3.0,'Very high':4.0})\n",
    "df=df[['F2','F4','F6','F7','F9','F11','F12','F14','F15','Target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 10)"
      ]
     },
     "execution_count": 1240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F2</th>\n",
       "      <th>F4</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F9</th>\n",
       "      <th>F11</th>\n",
       "      <th>F12</th>\n",
       "      <th>F14</th>\n",
       "      <th>F15</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>USA</td>\n",
       "      <td>1286.94</td>\n",
       "      <td>1913.38</td>\n",
       "      <td>6.66</td>\n",
       "      <td>-440.10</td>\n",
       "      <td>12.51</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>USA</td>\n",
       "      <td>1522.99</td>\n",
       "      <td>1458.10</td>\n",
       "      <td>4.96</td>\n",
       "      <td>-328.74</td>\n",
       "      <td>21.03</td>\n",
       "      <td>12</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>UK</td>\n",
       "      <td>979.23</td>\n",
       "      <td>1427.52</td>\n",
       "      <td>4.74</td>\n",
       "      <td>-404.07</td>\n",
       "      <td>1.17</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>Rest</td>\n",
       "      <td>1052.18</td>\n",
       "      <td>605.80</td>\n",
       "      <td>11.46</td>\n",
       "      <td>-506.25</td>\n",
       "      <td>3.99</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>Europe</td>\n",
       "      <td>1235.64</td>\n",
       "      <td>-208.92</td>\n",
       "      <td>12.76</td>\n",
       "      <td>-196.89</td>\n",
       "      <td>25.35</td>\n",
       "      <td>8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   F2      F4       F6       F7     F9     F11    F12  F14  F15  Target\n",
       "0   9     USA  1286.94  1913.38   6.66 -440.10  12.51    4  1.0     NaN\n",
       "1   6     USA  1522.99  1458.10   4.96 -328.74  21.03   12  3.0     NaN\n",
       "2   6      UK   979.23  1427.52   4.74 -404.07   1.17    6  4.0     NaN\n",
       "3   9    Rest  1052.18   605.80  11.46 -506.25   3.99    4  3.0     NaN\n",
       "4   6  Europe  1235.64  -208.92  12.76 -196.89  25.35    8  2.0     NaN"
      ]
     },
     "execution_count": 1241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1242,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=df.drop(['Target'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    x=np.array(x)\n",
    "    x=x.reshape(-1,len(x))\n",
    "    x=ct.transform(x)\n",
    "    pred=clf_c.predict(x)\n",
    "    amount=np.array(0.0)\n",
    "    if(pred[0]==True):\n",
    "        pred_amount=model_mlp.predict(x)\n",
    "        if(pred_amount[0][0]>0.0):\n",
    "            amount=pred_amount\n",
    "        \n",
    "    return pred,amount.flatten() \n",
    "\n",
    "\n",
    "amount_list=[]\n",
    "status_list=[]\n",
    "for ex in np.array(x_test):\n",
    "    status,amount=predict(ex)\n",
    "    amount_list.append(amount)\n",
    "    status_list.append(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>F9</th>\n",
       "      <th>F10</th>\n",
       "      <th>F11</th>\n",
       "      <th>F12</th>\n",
       "      <th>F13</th>\n",
       "      <th>F14</th>\n",
       "      <th>F15</th>\n",
       "      <th>F16</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>154.97</td>\n",
       "      <td>9</td>\n",
       "      <td>0.57</td>\n",
       "      <td>USA</td>\n",
       "      <td>-14.34</td>\n",
       "      <td>1286.94</td>\n",
       "      <td>1913.38</td>\n",
       "      <td>-10.54</td>\n",
       "      <td>6.66</td>\n",
       "      <td>232.40</td>\n",
       "      <td>-440.10</td>\n",
       "      <td>12.51</td>\n",
       "      <td>22.99</td>\n",
       "      <td>4</td>\n",
       "      <td>Low</td>\n",
       "      <td>22482.82</td>\n",
       "      <td>392.273010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>78.99</td>\n",
       "      <td>6</td>\n",
       "      <td>2.31</td>\n",
       "      <td>USA</td>\n",
       "      <td>-16.17</td>\n",
       "      <td>1522.99</td>\n",
       "      <td>1458.10</td>\n",
       "      <td>-12.17</td>\n",
       "      <td>4.96</td>\n",
       "      <td>268.26</td>\n",
       "      <td>-328.74</td>\n",
       "      <td>21.03</td>\n",
       "      <td>20.80</td>\n",
       "      <td>12</td>\n",
       "      <td>High</td>\n",
       "      <td>17183.76</td>\n",
       "      <td>2295.324951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>115.81</td>\n",
       "      <td>6</td>\n",
       "      <td>0.24</td>\n",
       "      <td>UK</td>\n",
       "      <td>6.84</td>\n",
       "      <td>979.23</td>\n",
       "      <td>1427.52</td>\n",
       "      <td>-11.22</td>\n",
       "      <td>4.74</td>\n",
       "      <td>233.43</td>\n",
       "      <td>-404.07</td>\n",
       "      <td>1.17</td>\n",
       "      <td>21.42</td>\n",
       "      <td>6</td>\n",
       "      <td>Very high</td>\n",
       "      <td>17585.36</td>\n",
       "      <td>895.833069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48.99</td>\n",
       "      <td>9</td>\n",
       "      <td>1023.63</td>\n",
       "      <td>Rest</td>\n",
       "      <td>-12.75</td>\n",
       "      <td>1052.18</td>\n",
       "      <td>605.80</td>\n",
       "      <td>-9.75</td>\n",
       "      <td>11.46</td>\n",
       "      <td>261.27</td>\n",
       "      <td>-506.25</td>\n",
       "      <td>3.99</td>\n",
       "      <td>19.64</td>\n",
       "      <td>4</td>\n",
       "      <td>High</td>\n",
       "      <td>14621.10</td>\n",
       "      <td>1391.932007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71.39</td>\n",
       "      <td>6</td>\n",
       "      <td>1.68</td>\n",
       "      <td>Europe</td>\n",
       "      <td>-10.98</td>\n",
       "      <td>1235.64</td>\n",
       "      <td>-208.92</td>\n",
       "      <td>-11.45</td>\n",
       "      <td>12.76</td>\n",
       "      <td>332.18</td>\n",
       "      <td>-196.89</td>\n",
       "      <td>25.35</td>\n",
       "      <td>19.50</td>\n",
       "      <td>8</td>\n",
       "      <td>Medium</td>\n",
       "      <td>14624.56</td>\n",
       "      <td>2731.161865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       F1  F2       F3      F4     F5       F6       F7     F8     F9     F10  \\\n",
       "0  154.97   9     0.57     USA -14.34  1286.94  1913.38 -10.54   6.66  232.40   \n",
       "1   78.99   6     2.31     USA -16.17  1522.99  1458.10 -12.17   4.96  268.26   \n",
       "2  115.81   6     0.24      UK   6.84   979.23  1427.52 -11.22   4.74  233.43   \n",
       "3   48.99   9  1023.63    Rest -12.75  1052.18   605.80  -9.75  11.46  261.27   \n",
       "4   71.39   6     1.68  Europe -10.98  1235.64  -208.92 -11.45  12.76  332.18   \n",
       "\n",
       "      F11    F12    F13  F14        F15       F16       Target  \n",
       "0 -440.10  12.51  22.99    4        Low  22482.82   392.273010  \n",
       "1 -328.74  21.03  20.80   12       High  17183.76  2295.324951  \n",
       "2 -404.07   1.17  21.42    6  Very high  17585.36   895.833069  \n",
       "3 -506.25   3.99  19.64    4       High  14621.10  1391.932007  \n",
       "4 -196.89  25.35  19.50    8     Medium  14624.56  2731.161865  "
      ]
     },
     "execution_count": 1244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(r\"C:\\Users\\Deepak Tripathi\\Desktop\\assignment machine learning\\part3\\CE802_P3_Test.csv\")\n",
    "df['Target']=np.array(amount_list)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1245,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"CE802_P3_Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
